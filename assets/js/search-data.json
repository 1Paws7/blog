{
  
    
        "post0": {
            "title": "Random Forest Classifier",
            "content": "Goal . The goal is to predict whether a passenger on the titanic survived or not. The applications for binary classification are endless and could be applied to many real world problems. Does this patient have this disease? Will this customer Churn? Will price go up? These are just a few examples. . The purpose is to give a general guide to classification. If you get trhrough this and want more detail, I highly recomend checking out the Tabular Chapter of Deep Learning for Coders with fastai &amp; Pytorch by Jeremy Howard and Sylvain Gugger. The book primarily focuses on deep learning, though decision trees are covered for tabular data. All of the material in this guide and more is covered in much greater detail in that book. . https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527 . Setup . We are going to start with loading libraries and datasets that are needed. I am going to skip over this as they are pretty self explanatory, but feel free to look close if you would like. . I am going to use Seaborn to load the titanic dataset. . #collapse-hide from sklearn.ensemble import RandomForestClassifier import seaborn as sns import pandas as pd import numpy as np from fastai2.tabular.all import * from fastai2 import * from sklearn.model_selection import GridSearchCV from dtreeviz.trees import * from scipy.cluster import hierarchy as hc df = sns.load_dataset(&#39;titanic&#39;) df.head() . . survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone . 0 | 0 | 3 | male | 22.0 | 1 | 0 | 7.2500 | S | Third | man | True | NaN | Southampton | no | False | . 1 | 1 | 1 | female | 38.0 | 1 | 0 | 71.2833 | C | First | woman | False | C | Cherbourg | yes | False | . 2 | 1 | 3 | female | 26.0 | 0 | 0 | 7.9250 | S | Third | woman | False | NaN | Southampton | yes | True | . 3 | 1 | 1 | female | 35.0 | 1 | 0 | 53.1000 | S | First | woman | False | C | Southampton | yes | False | . 4 | 0 | 3 | male | 35.0 | 0 | 0 | 8.0500 | S | Third | man | True | NaN | Southampton | no | True | . Data Setup . Dependent Variable . We want to define what we are predicting, or the dependent variable. We also see that &#39;Survived&#39; and &#39;alive&#39; columns are the same thing with different names. We need to get rid of one and predict the other. . # survived is a duplicate of alive, so get rid of it df.drop(&#39;survived&#39;,axis = 1, inplace=True) dep_var = &#39;alive&#39; . Training and Validation Set Split . Best practice is to minimally have a training and validation set. Those are the 2 that we will use for this tutorial. . Training Set: This is what the model actually trains on | Validation Set: This is used to gauge success of the Training | Test Set: This is a held out of the total process to be an additional safeguard against overfitting | . cond = np.random.rand(len(df))&gt;.2 train = np.where(cond)[0] valid = np.where(~cond)[0] splits = (list(train),list(valid)) . Dates . We don&#39;t have any dates to deal with, but if we did we would do the following: . df = add_datepart(df,&#39;date&#39;) . This would replace that date with a ton of different columns, such as the year, the day number, the day of the week, is it month end, is it month start, and more. . Categorical Variables . Ordinal Categorical Variables . Some categorical variables have a natural heirarchy. By telling pandas the order it tends to mean trees don&#39;t have to split as many times, which speeds up training times. . df[&#39;class&#39;].unique() . [Third, First, Second] Categories (3, object): [Third, First, Second] . classes = &#39;First&#39;,&#39;Second&#39;,&#39;Third&#39; . df[&#39;class&#39;] = df[&#39;class&#39;].astype(&#39;category&#39;) df[&#39;class&#39;].cat.set_categories(classes, ordered=True, inplace=True) . Categorical Variables Final . We are not going to do some data cleaning. The Categorify and FillMissing functions in the fastai2 library make this easy. . procs = [Categorify, FillMissing] . cont,cat = cont_cat_split(df, 1, dep_var=dep_var) . to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) . Let&#39;s take a look at the traiing and validation sets and make sure we have a good split of each. . len(to.train),len(to.valid) . (710, 181) . We can now take a look and see that while we see all the same data, behind the scenes it is all numeric. This is exactly what we need for our random forest. . to.show(3) . sex embarked class who adult_male deck embark_town alone age_na pclass age sibsp parch fare alive . 0 male | S | Third | man | True | #na# | Southampton | False | False | 3.0 | 22.0 | 1.0 | 0.0 | 7.250000 | no | . 1 female | C | First | woman | False | C | Cherbourg | False | False | 1.0 | 38.0 | 1.0 | 0.0 | 71.283302 | yes | . 2 female | S | Third | woman | False | #na# | Southampton | True | False | 3.0 | 26.0 | 0.0 | 0.0 | 7.925000 | yes | . to.items.head(3) . pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone age_na . 0 | 3.0 | 2 | 22.0 | 1.0 | 0.0 | 7.250000 | 3 | 3 | 2 | 2 | 0 | 3 | 0 | 1 | 1 | . 1 | 1.0 | 1 | 38.0 | 1.0 | 0.0 | 71.283302 | 1 | 1 | 3 | 1 | 3 | 1 | 1 | 1 | 1 | . 2 | 3.0 | 1 | 26.0 | 0.0 | 0.0 | 7.925000 | 3 | 3 | 3 | 1 | 0 | 3 | 1 | 2 | 1 | . Final Change . Finally, we will put just the data in xs and ys so they are in easy format to pass to models. . xs,y = to.train.xs,to.train.y valid_xs,valid_y = to.valid.xs,to.valid.y . Random Forest Model . Initial Model . Let&#39;s start by creating a model without tuning and see how it does . m = RandomForestClassifier(n_estimators=100) m = m.fit(xs,y) . from sklearn.metrics import confusion_matrix . confusion_matrix(y,m.predict(xs)) . array([[441, 1], [ 5, 263]]) . Looking pretty good! Only 7 wrong. Let&#39;s see how it did on the validation set. . confusion_matrix(valid_y,m.predict(valid_xs)) . array([[94, 13], [24, 50]]) . Still way better than 50/50, but not quite as good. This is because the model did not train based on this validation data so it doesn&#39;t perform nearly as well. . Model Tuning - Grid Search . We made our first model, and it doesn&#39;t seem to predict as well as we would like. Let&#39;s do something about that. . We are going to do a grid search. There are many more sophisticated ways to find parameters (maybe a future post), but the grid search is easy to understand. Basically you pick some ranges, and you try them all to see what works best. . We will use the built in gridsearch. All we need to do is define the range of parameters, and let it find the best model. . parameters = {&#39;n_estimators&#39;:range(10,20,20), &#39;max_depth&#39;:range(10,20,20), &#39;min_samples_split&#39;:range(2,20,1), &#39;max_features&#39;:[&#39;auto&#39;,&#39;log2&#39;]} . clf = GridSearchCV(RandomForestClassifier(), parameters, n_jobs=-1) . clf.fit(xs,y) . /Users/isaac.flath/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning. warnings.warn(CV_WARNING, FutureWarning) . GridSearchCV(cv=&#39;warn&#39;, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=&#39;warn&#39;, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;max_depth&#39;: range(10, 20, 20), &#39;max_features&#39;: [&#39;auto&#39;, &#39;log2&#39;], &#39;min_samples_split&#39;: range(2, 20), &#39;n_estimators&#39;: range(10, 20, 20)}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . Results . We can see below that the best esimator works better for prediciton the validation set than the model above did. Success! . confusion_matrix(y,clf.best_estimator_.predict(xs)) . array([[419, 23], [ 62, 206]]) . confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs)) . array([[100, 7], [ 28, 46]]) . Model Minimizing . Now that we have good results with a tuned model, we may want to simplify the model. If we can simplify the model without significantly impacting accuracy, that&#39;s good for many reasons. . The model is easier to understand | Fewer variables means fewer data quality issues and more focused data quality efforts | It takes less resources and time to run | Feature Importance . There are my ways to measure importance. How often do we use a feature to split? How high up in the tree is it used to split? We are going to use scikit learns feature importance information. . Let&#39;s look at what features are! . #collapse def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) . . fi = rf_feat_importance(m, xs) fi[:5] . cols imp . 13 | fare | 0.209353 | . 10 | age | 0.178731 | . 4 | adult_male | 0.140225 | . 0 | sex | 0.092039 | . 3 | who | 0.079811 | . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:30]); . Remove Low Important Variables . This isn&#39;t strictly neccesarry, but it is nice to simplify models if you can. Simpler models are easier to understand and maintain, and they take less resources to run. It is also interesting to know just how many variables are needed to predict. . to_keep = fi[fi.imp&gt;0.05].cols len(to_keep) . 7 . xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] . clf = GridSearchCV(RandomForestClassifier(), parameters, n_jobs=-1) clf.fit(xs_imp,y) . /Users/isaac.flath/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning. warnings.warn(CV_WARNING, FutureWarning) . GridSearchCV(cv=&#39;warn&#39;, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=&#39;warn&#39;, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;max_depth&#39;: range(10, 20, 20), &#39;max_features&#39;: [&#39;auto&#39;, &#39;log2&#39;], &#39;min_samples_split&#39;: range(2, 20), &#39;n_estimators&#39;: range(10, 20, 20)}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . Results . Now we see with only 6 features we still get pretty good results on on validation set. . Now the question is whether this small loss in accuracy outweighed by a simpler and more efficient model? That&#39;s is a business question more than it is a data science question. . If you are detecting COVID-19, you probably want it to be as accurate as possible. If you are going to predict whether someone is a cat or a dog person based on a survey for marketing purposes, small changes in accuracy probably are not as critical. . confusion_matrix(y,clf.best_estimator_.predict(xs_imp)) . array([[417, 25], [ 55, 213]]) . confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp)) . array([[96, 11], [33, 41]]) . clf.best_estimator_ . RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=10, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=11, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . Redundant columns . Of the 6 remaining variables, we can see that some of them are very related. It makes sense to me that deck and fare are related. Nicer areas probably cost more. It makes sense to me that the perons sex has some redudancy with adult_male - the redundancy is even in the name. . #collapse def cluster_columns(df, figsize=(10,6), font_size=12): corr = np.round(scipy.stats.spearmanr(df).correlation, 4) corr_condensed = hc.distance.squareform(1-corr) z = hc.linkage(corr_condensed, method=&#39;average&#39;) fig = plt.figure(figsize=figsize) hc.dendrogram(z, labels=df.columns, orientation=&#39;left&#39;, leaf_font_size=font_size) plt.show() . . cluster_columns(xs_imp) . Let&#39;s expirament with removing some columns and see what we get. We will use accuracy for our metric. . Here is out baseline: . #collapse print(&quot;accuracy: &quot;) (confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp))[0,0] + confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp))[1,1] )/ confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp)).sum() . . accuracy: . 0.7569060773480663 . #collapse def get_accuracy(x,y,valid_x,valid_y): m = RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=10, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=4, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) m.fit(xs_imp,y) print((confusion_matrix(valid_y,m.predict(valid_xs_imp))[0,0] + confusion_matrix(valid_y,m.predict(valid_xs_imp))[1,1] )/ confusion_matrix(valid_y,m.predict(valid_xs_imp)).sum()) . . We will now loop through each of the remaining variables and train a model and print out the accuracy score. . Judging by the scores below, removing any 1 variable does not significantly reduce the accuracy. This means that we have redundant columns that can likely be trimmed. . From this we can remove variables and iterate through to continue simplifying as much as possible. . #collapse variables = [&#39;sex&#39;,&#39;adult_male&#39;,&#39;pclass&#39;,&#39;who&#39;,&#39;age&#39;,&#39;deck&#39;,&#39;fare&#39;] for variable in variables: print(&#39;drop &#39;+variable+&#39; accuracy:&#39;) get_accuracy(xs_imp.drop(variable, axis=1), y, valid_xs_imp.drop(variable, axis=1), valid_y) . . drop sex accuracy: 0.8232044198895028 drop adult_male accuracy: 0.7900552486187845 drop pclass accuracy: 0.7955801104972375 drop who accuracy: 0.8011049723756906 drop age accuracy: 0.7955801104972375 drop deck accuracy: 0.7955801104972375 drop fare accuracy: 0.7845303867403315 .",
            "url": "https://isaac-flath.github.io/blog/randomforest/classification/2020/05/08/RandomForestClassifier.html",
            "relUrl": "/randomforest/classification/2020/05/08/RandomForestClassifier.html",
            "date": " • May 8, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Jupyter Notebook Tutorial",
            "content": "Top 3 uses: . Exploratory analysis, model creation, data science, any kind of coding that require lots of rapid expiramentation and iteration. | Tutorials, guides, and blogs (like this one). Because you have a great mix of text functionality with code, they work really well for tutorials and guides. Rather than having static images, or code snippits that have to get updated each iteration, the code is part of the guide and it really simplifies the process. Notebooks can be exported directly to html and be opened in any browser to give to people. With the easy conversion to html, naturally it&#39;s easy to post them on a web page. | Technical presentations of results. You can have the actual code analysis done, with text explanations. Excess code can be collapsed so that if someone asks really detailed questions you can expand and have every piece of detail. Changes to the analysis are in the presentation so no need to save and put static images in other documents | Cell Types . A cell can be 3 different types. The most useful are code cells and markdown cells. . Code Cells . - Code cells run code The next few cells are examples of code cells - While the most common application is Python, you can set up environments easily to use R, swift, and other languages within jupyter notebooks . Markdown Cells . - This cell is a markdown cell. It is really nice for adding details and text explanations in where a code comment is not enough - They have all the normal markdown functionality, plus more. For example, I can write any technical or mathy stuff using latex, or create html tables in markdown or html. - I can also make markdown tables. . Latex Formulas . $$ begin{bmatrix}w_1&amp;w_2&amp;w_3&amp;w_4&amp;w_5 x_1&amp;x_2&amp;x_3&amp;x_4&amp;x_5 y_1&amp;y_2&amp;y_3&amp;y_4&amp;y_5 z_1&amp;z_2&amp;z_3&amp;z_4&amp;z_5 end{bmatrix}$$ . $ begin{align} frac{dy}{du} &amp;= f&#39;(u) = e^u = e^{ sin(x^2)}, frac{du}{dv} &amp;= g&#39;(v) = cos v = cos(x^2), frac{dv}{dx} &amp;= h&#39;(x) = 2x. end{align}$ . Markdown Table . This is a table for demos . perc | 55% | 22% | 23% | 12% | 53% | . qty | 23 | 19 | 150 | 9 | 92 | . #collapse-hide import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns pd.options.display.max_columns = None pd.options.display.max_rows = None %matplotlib inline . . Running Code . Naturally you can run code cells and print to the Jupyter Notebook . for x in range(0,5): print(x*10) . 0 10 20 30 40 . DataFrames . iris = sns.load_dataset(&#39;iris&#39;) iris[iris.petal_length &gt; 6] . sepal_length sepal_width petal_length petal_width species . 105 | 7.6 | 3.0 | 6.6 | 2.1 | virginica | . 107 | 7.3 | 2.9 | 6.3 | 1.8 | virginica | . 109 | 7.2 | 3.6 | 6.1 | 2.5 | virginica | . 117 | 7.7 | 3.8 | 6.7 | 2.2 | virginica | . 118 | 7.7 | 2.6 | 6.9 | 2.3 | virginica | . 122 | 7.7 | 2.8 | 6.7 | 2.0 | virginica | . 130 | 7.4 | 2.8 | 6.1 | 1.9 | virginica | . 131 | 7.9 | 3.8 | 6.4 | 2.0 | virginica | . 135 | 7.7 | 3.0 | 6.1 | 2.3 | virginica | . Plotting . Below we are going to make a few graphs to get the point accross. Naturally, each graph can be accompanied with a markdown cell that gives context and explains the value of that graph. . Line Chart . # evenly sampled time at 200ms intervals t = np.arange(0., 5., 0.2) # red dashes, blue squares and green triangles plt.plot(t, t, &#39;r--&#39;, t, t**2, &#39;bs&#39;, t, t**3, &#39;g^&#39;) plt.show() . Scatter Plot . Sometimes we will want to display a graph, but may not want all the code and details to be immediately visable. In these examples we can create a scatter plot like below, but collapse the code cell. . This is great when you want to show a graph and explain it, but the details of how the graph was created aren&#39;t that important. . #collapse-hide data = {&#39;a&#39;: np.arange(50), &#39;c&#39;: np.random.randint(0, 50, 50), &#39;d&#39;: np.random.randn(50)} data[&#39;b&#39;] = data[&#39;a&#39;] + 10 * np.random.randn(50) data[&#39;d&#39;] = np.abs(data[&#39;d&#39;]) * 100 plt.scatter(&#39;a&#39;, &#39;b&#39;, c=&#39;c&#39;, s=&#39;d&#39;, data=data) plt.xlabel(&#39;entry a&#39;) plt.ylabel(&#39;entry b&#39;) plt.show() . . Categorical Plot . We can create subplots to have multiple plots show up. This can be especially helpful when showing lots of the same information, or showing how 2 different metrics are related or need to be analyzed together . #collapse-hide names = [&#39;group_a&#39;, &#39;group_b&#39;, &#39;group_c&#39;] values = [1, 10, 100] plt.figure(figsize=(9, 3)) plt.subplot(131) plt.bar(names, values) plt.subplot(132) plt.scatter(names, values) plt.subplot(133) plt.plot(names, values) plt.suptitle(&#39;Categorical Plotting&#39;) plt.show() . . Stack Traces . When you run into an error, by default jupyter notebooks give you whatever the error message is, but also the entire stack trace. . There is a debug functionality, but I find that these stack traces and jupyter cells work even better than a debugger. I can break my code into as many cells as I want and run things interactively. Here&#39;s a few examples of stack traces. . Matrix Multiplication Good . Now we are going to show an example of errors where the stack trace isn&#39;t as simple. Suppose we are trying to multiply 2 arrays together (matrix multiplication). . a = np.array([ [1,2,4], [3,4,5], [5,6,7] ]) b = np.array([ [11,12,14], [31,14,15], [23,32,23] ]) a@b . array([[165, 168, 136], [272, 252, 217], [402, 368, 321]]) . Matrix Multiplication Bad . Now if it errors because the columns from matrix a don&#39;t match the rows from matrix b, we will get an error as matrix multiplication is impossible with those matrices. We see the same idea s the above for loop, stack trace with error and arrow pointing at the line that failed . # here&#39;s another a = np.array([ [1,2,4], [3,4,5], [5,6,7] ]) b = np.array([ [11,12,14], [31,14,15] ]) a@b . ValueError Traceback (most recent call last) &lt;ipython-input-8-52272ca444fc&gt; in &lt;module&gt; 9 [31,14,15] 10 ]) &gt; 11 a@b ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 3) . Second Layer of Bad . But what if the line we call isn&#39;t what fails? What if what I run works, but the function underneath fails? . In these example, you see the entire trace. It starts with are arrow at what you ran that errored. It then shows an arrow that your code called that caused the error, so you can track all the way back to the source. Here&#39;s how it shows a two step stack trace, but it can be as long as needed. . def matmul(a,b): c = a@b return c . matmul(a,b) . ValueError Traceback (most recent call last) &lt;ipython-input-10-7853c1c27063&gt; in &lt;module&gt; -&gt; 1 matmul(a,b) &lt;ipython-input-9-1c8b6b954779&gt; in matmul(a, b) 1 def matmul(a,b): -&gt; 2 c = a@b 3 return c ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 3) . Magic Commands . Magic commands are special commands for Juptyer Notebooks. They give you incredible functionality and you will likley find the experience very frustrating without them. A few that I use often are: . ? | put a question mark or 2 after a function or method to get the documentation. ?? gives more detail than ?. I can also use it to wild card search modules for functions. | shift tab | when you are writing something holding shift + tab will open a mini popup with the documentation for that thing. It may be a funciton, method, or module. | %who or %whos or %who_ls | These are all variants that list the objects and variables. I prefer %whos most of the time | %history | This allows you to look at the last pieces of code that you ran | $$ | wrapping latex code in dollar signs in a markdown cell renders latex code in markdown cells | ! | putting ! at the beginning of a line makes it run that in terminal. For example !ls | grep .csv | %time | I can use this to time the execution of things | . np.*array*? . np.array np.array2string np.array_equal np.array_equiv np.array_repr np.array_split np.array_str np.asanyarray np.asarray np.asarray_chkfinite np.ascontiguousarray np.asfarray np.asfortranarray np.broadcast_arrays np.chararray np.compare_chararrays np.get_array_wrap np.ndarray np.numarray np.recarray . np.array_equal?? . Signature: np.array_equal(a1, a2) Source: @array_function_dispatch(_array_equal_dispatcher) def array_equal(a1, a2): &#34;&#34;&#34; True if two arrays have the same shape and elements, False otherwise. Parameters - a1, a2 : array_like Input arrays. Returns - b : bool Returns True if the arrays are equal. See Also -- allclose: Returns True if two arrays are element-wise equal within a tolerance. array_equiv: Returns True if input arrays are shape consistent and all elements equal. Examples -- &gt;&gt;&gt; np.array_equal([1, 2], [1, 2]) True &gt;&gt;&gt; np.array_equal(np.array([1, 2]), np.array([1, 2])) True &gt;&gt;&gt; np.array_equal([1, 2], [1, 2, 3]) False &gt;&gt;&gt; np.array_equal([1, 2], [1, 4]) False &#34;&#34;&#34; try: a1, a2 = asarray(a1), asarray(a2) except Exception: return False if a1.shape != a2.shape: return False return bool(asarray(a1 == a2).all()) File: ~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py Type: function . a = np.array(np.random.rand(512,512)) b = np.array(np.random.rand(512,512)) %time for i in range(0,20): a@b . CPU times: user 198 ms, sys: 2.68 ms, total: 200 ms Wall time: 34.8 ms . %whos . Variable Type Data/Info a ndarray 512x512: 262144 elems, type `float64`, 2097152 bytes (2.0 Mb) b ndarray 512x512: 262144 elems, type `float64`, 2097152 bytes (2.0 Mb) data dict n=4 i int 19 iris DataFrame sepal_length sepal_&lt;...&gt; 1.8 virginica matmul function &lt;function matmul at 0x1a169cc680&gt; names list n=3 np module &lt;module &#39;numpy&#39; from &#39;/Us&lt;...&gt;kages/numpy/__init__.py&#39;&gt; pd module &lt;module &#39;pandas&#39; from &#39;/U&lt;...&gt;ages/pandas/__init__.py&#39;&gt; plt module &lt;module &#39;matplotlib.pyplo&lt;...&gt;es/matplotlib/pyplot.py&#39;&gt; sns module &lt;module &#39;seaborn&#39; from &#39;/&lt;...&gt;ges/seaborn/__init__.py&#39;&gt; t ndarray 25: 25 elems, type `float64`, 200 bytes values list n=3 x int 4 . %history -l 5 . matmul(a,b) np.*array*? np.array_equal?? a = np.array(np.random.rand(512,512)) b = np.array(np.random.rand(512,512)) %time for i in range(0,20): a@b %whos . Jupyter Extensions . There are many extensions to Jupyter Notebooks. After all a jupyter notebook is just a JSON file, so you can read the JSON in and manipulate and transform things however you want! There are many features, such as variable explorers, auto code timers, and more - but I find I that most are unneccesary. About half the people I talk to don&#39;t use any, and the other half use several. . NBDEV . NBdev is a jupyter extension/python library that allows you to do full development projects in Jupyter Notebooks. There have been books and libraries written entirely in Jupyter notebooks, including testing frameworks and unit testing that goes with them. A common misconception is that Jupyter notebooks cannot be used for that, though many people already have. . . There are many features NBdev adds. Here&#39;s a few. . Using notebooks written like this, nbdev can create and run any of the following with a single command: . Searchable, hyperlinked documentation; any word you surround in backticks will by automatically hyperlinked to the appropriate documentation | Cells in jupyter notebook marked with #export will be exported automatically to a python module | Python modules, following best practices such as automatically defining all (more details) with your exported functions, classes, and variables | Pip installers (uploaded to pypi for you) | Tests (defined directly in your notebooks, and run in parallel). | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks | . I reccomend checking them out for more detail https://github.com/fastai/nbdev .",
            "url": "https://isaac-flath.github.io/blog/jupyter/2020/05/08/JupyterNotebooks.html",
            "relUrl": "/jupyter/2020/05/08/JupyterNotebooks.html",
            "date": " • May 8, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Geometry of Linear Equations (18.06_L1)",
            "content": "#collapse-hide import matplotlib.pyplot as plt from torch import tensor from torch import solve import numpy as np from mpl_toolkits import mplot3d from mpl_toolkits.mplot3d import Axes3D . . Linear Algebra . One of my goals is to understand more deeply what Neural Networks are doing. Another is to have an easier time understanding and implementing cutting edge academic papers. In order to work toward those goals, I am revisiting the Math behind Neural Networks. This time my goal is to understand intuitively every piece of the material forward and backward - rather than to pass a course on a deadline. . This blog post will be my notes about Lecture 1 from the following course: . Gilbert Strang. 18.06 Linear Algebra. Spring 2010. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. . Goal . The goal is to solve N equations with N unknowns. We will start with 2 equations with 2 unknowns, then go to 3 equations with 3 unknowns. We will use an intuitive approach here to understand a bit about how linear equations work. . How do we multiply these together? . Matrix Multiplication (Ax=b) . $ begin{bmatrix} 2 &amp; 5 1 &amp; 3 end{bmatrix}$ $ begin{bmatrix} 1 2 end{bmatrix}$ $=1$ $ begin{bmatrix} 2 1 end{bmatrix}$ $+2$ $ begin{bmatrix} 5 3 end{bmatrix}$ $=$ $ begin{bmatrix} 12 7 end{bmatrix}$ . Ax is a linear combination of columns . 2 equations 2 unknowns . Ok, Let&#39;s look at a couple equations in a few different ways. The solution to these are any values of x and y that make both equations true. . $2x - y = 0$ . $-x + 2y = 3$ . Row Picture . We can take these 2 linear equations and plot them. . $2x - y = 0$ could be writen as $y = 2x$, which is trivial to graph. If we graph them both, we can see visually where they are both true (the intersection). . def plot_equations_2d(x_range,y_dict): for y in y_dict: plt.plot(x, y_dict[y], label=y) plt.xlabel(&#39;x&#39;, color=&#39;#1C2833&#39;) plt.ylabel(&#39;y&#39;, color=&#39;#1C2833&#39;) plt.legend(loc=&#39;upper left&#39;) plt.grid() plt.show() x = tensor(np.linspace(-4,4,100)) y_dict = {&#39;2x-y=0&#39;:2*x, &#39;-x+2y=3&#39;:(3 + x)/2} plot_equations_2d(x,y_dict) . Column Picture . We can rewrite out equations into a different notation, which gives us a more concise view of what is going on. You can see that the top row is the first equation, and the bottom row is the second. Same thing, written differently. . $x$ $ begin{bmatrix} 2 -1 end{bmatrix}$ $+y$ $ begin{bmatrix} -1 2 end{bmatrix}$ $=$ $ begin{bmatrix} 0 3 end{bmatrix}$ . Graphed as Vectors . Now that we see them it in column form, we can graph the vectors . #collapse-hide strt_pts = tensor([[0,0],[2,-1]]) end_pts = tensor([[2,-1],[1,2]]) diff = end_pts - strt_pts plt.ylim([-3, 3]) plt.xlim([-3, 3]) plt.quiver(strt_pts[:,0], strt_pts[:,1], diff[:,0], diff[:,1], angles=&#39;xy&#39;, scale_units=&#39;xy&#39;, scale=1.) plt.show() . . Summed as Vectors . Now that they are represented as vectors. Let&#39;s add 1X + 2Y vectors and see that we get (0,3). Simply make one vector start where the other ends . #collapse-hide strt_pts = tensor([[0,0],[2,-1],[1,1]]) end_pts = tensor([[2,-1],[1,1],[0,3]]) diff = end_pts - strt_pts plt.ylim([-3, 3]) plt.xlim([-3, 3]) plt.quiver(strt_pts[:,0], strt_pts[:,1], diff[:,0], diff[:,1], angles=&#39;xy&#39;, scale_units=&#39;xy&#39;, scale=1.) plt.show() . . Matrix Form AX = b . We can see the same view in matrix notation. Same as the row and column view, just in a nice compressed format. . $ begin{bmatrix} 2 &amp; -1 -1 &amp; 2 end{bmatrix}$ $ begin{bmatrix} x y end{bmatrix}$ $=$ $ begin{bmatrix} 0 3 end{bmatrix}$ . Equations, 3 equations 3 unknowns . $2x - y = 0$ . $-x + 2y -z = -1$ . $-3y + 4z = 4$ . Matrix Form . $A=$ $ begin{bmatrix} 2 &amp; -1 &amp; 0 -1 &amp; 2 &amp; -1 0 &amp; -3 &amp; 4 end{bmatrix}$ $b=$ $ begin{bmatrix} 0 -1 4 end{bmatrix}$ . Row Picture . #collapse-hide fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) x, y = tensor(np.linspace(-8,8,100)), tensor(np.linspace(-8,8,100)) X, Y = np.meshgrid(x,y) Z1 = 2*X Z2 = (-1 + X + Y) / 2 Z3 = (4 - 4*Y)/3 ax.plot_surface(X,Y,Z1, alpha=0.5, rstride=100, cstride=100) ax.plot_surface(X,Y,Z2, alpha=0.5, rstride=100, cstride=100) ax.plot((1,1),(-8,8),(-9,23), lw=2, c=&#39;b&#39;) ax.plot_surface(X,Y,Z3, alpha=0.5, facecolors=&#39;g&#39;, rstride=100, cstride=100) ax.plot((1,),(-2,),(3,), lw=2, c=&#39;k&#39;, marker=&#39;o&#39;) plt.show() . . Column Picture . We can create the column picture and graph vectors, just like in 2D space. Graphing in 3D is harder to see, but it&#39;s the same concept. . In this example we can clearly see the solution is x = 0, y = 0, z = 1. . $x$ $ begin{bmatrix} 2 -1 0 end{bmatrix}$ $+y$ $ begin{bmatrix} -1 2 4 end{bmatrix}$ $+z$ $ begin{bmatrix} 0 -1 4 end{bmatrix}$ $=$ $ begin{bmatrix} 0 -1 4 end{bmatrix}$ . #collapse-hide strt_pts = tensor([[0,0,0],[0,0,0],[0,0,0]]) end_pts = tensor([[2,-1,0],[-1,2,-1],[0,4,4]]) diff = end_pts - strt_pts fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.set_xlim([-5, 5]) ax.set_ylim([-5, 5]) ax.set_zlim([-5, 5]) plt.quiver(strt_pts[:,0], strt_pts[:,1], strt_pts[:,2], diff[:,0], diff[:,1],diff[:,2]) plt.show() . . Numpy Solver . Here&#39;s how you can solve the equation using Numpy. . a = np.array([[2, -1, 0], [-1, 2, -1], [0, -3, 4]]) b = np.array([0, -1, 4]) x = np.linalg.solve(a, b) print(x) . [ 0. -0. 1.] . Can I solve Ax = b for every b? . Do the linear combinations of the columns fill 3 dimensional space. . If you have some dimensionality in each direction, then you can. 3 equations with 3 unknowns can fill the 3D space as long as they don&#39;t sit on the same line or plane. . # lets expirament length_b = 20 b = np.array([list(np.random.rand(length_b)*10), list(np.random.rand(length_b)*10), list(np.random.rand(length_b)*10)]) for x in range(0,length_b): x = np.linalg.solve(a, b[:,x]) print(x) . [ 8.16805747 10.48024005 9.96809979] [9.06725154 9.2157439 9.14628651] [14.84467827 19.76751284 16.8031129 ] [5.48826464 8.7773201 7.60768914] [ 8.53018759 12.9082786 11.69127729] [ 6.78329217 10.20215186 8.40981525] [ 7.01421253 10.93880412 10.55733849] [ 9.05891997 14.74974623 13.42061373] [10.19869734 16.00490956 14.3459745 ] [9.10954836 9.6864028 9.56966219] [ 9.08434284 10.86306274 9.2531385 ] [12.55797136 18.34256625 15.99588544] [7.09998088 7.1142009 6.85118957] [ 7.3702 11.29534458 10.28193089] [ 6.29293053 10.82184999 8.30130941] [11.5767933 15.66537906 13.26443175] [6.67171881 7.37427105 7.18008504] [3.83815051 4.88854295 4.50169223] [4.79471557 8.08347503 6.72417967] [3.68698941 6.77141712 6.20886562] .",
            "url": "https://isaac-flath.github.io/blog/linearalgebra/math/2020/05/08/.06_1_GeometryOfLinearEquations.html",
            "relUrl": "/linearalgebra/math/2020/05/08/.06_1_GeometryOfLinearEquations.html",
            "date": " • May 8, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a Data Scientist with a passion for teaching. . I really enjoy playing with machine learning algorithms so much of my content will probably relate to that in some way :grinning: . Basically anything I find interesting and/or helpful will be included. .",
          "url": "https://isaac-flath.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://isaac-flath.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}