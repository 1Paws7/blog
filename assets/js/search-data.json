{
  
    
        "post0": {
            "title": "Predictive Customer Analytics (Part 2)",
            "content": "Intro . Customer Analytics is a valuable tool, but often the basics are overlooked. It can have a huge impact, and it doesn’t always mean complex modeling. It always starts with the basics. . This is the 2nd post in a series about customer analytics. The first post discusses descriptive analytics and how to create and use descriptive customer analytics. In this post we will be building on that descriptive framework to create a very basic predictive model. A predictive model allows companies to plan proactively instead of reactively. . Background . In the first post we created three descriptive metrics (Recency, Frequency, and Average Monetary Value). Recency is days since the last purchase. Frequency is the number of cycles that the customer purchased product in. Monetary Value is the average revenue brought in from that customer in cycles where the customer purchased product. How can you determine the predicted customer value and take action based on that information? . We need to group customers based on how they behave. This allows us to quickly and efficiently test different actions and see how that affects our long term revenue projections. A customer who makes many small purchases may be just as valuable as a customer who places huge orders infrequently, but they should be handled differently. RFM is one tool we can use to group customers together. . Normalize the metrics . We start with a basic table with three companies and their 3 KPIs. .   Recency (days since previous purchase) Frequency (number of months with purchases) Monetary Value (avg monthly revenue during months with purchases) . Customer A | 3 | 3 | $79 | . Customer B | 35 | 2 | $91 | . Customer C | 48 | 1 | $9852 | . Now that we have values for each of these three metrics (Recency, Frequency, and Monetary Value), we need to put them into a format where we can easily compare the customers. This is called normalizing the data. Normalization puts all the metrics on a scale of 0 (lowest) to 1 (highest). Here is a formula for normalizing: . (&lt;Value you want to normalize&gt; – &lt;Minimum Value in series&gt;) / (&lt;Maximum value in series&gt; – &lt;Minimum value in series&gt;) . Let’s walk through normalizing the Recency KPI. Based on the formula above we need a couple of pieces of information. Let’s identify these now. . Minimum Value in series = 3 Maximum value in series = 48 . Now let’s plug in the formula for each customer to normalize the values. .   Recency Normalized Recency . Customer A | 3 | =(3-3) / (48-3) = 0 | . Customer B | 35 | =(35-3) / (48-3) = 0.711 | . Customer C | 48 | =(48-3) / (48-3) = 1 | . The next step is to repeat this process for the other metrics. I won’t walk through the other two, but here are the accurate values if you’d like to test yourself. .   Recency Frequency Monetary Value . Customer A | 0 | 1 | 0 | . Customer B | 0.711 | 0.5 | 0.001 | . Customer C | 1 | 0 | 1 | . A low recency score is good. A high Frequency or Monetary Value Score is good. If you’d like to simplify it so you don’t have to remember that feel free to take your normalized recency score and subtract it from 1. That will make low scores bad and high scores good on all metrics. . Group the customers together . Now that we have normalized our KPIs, we need to group our customers together. In the table above we only have 3 customers, so there would be no point in grouping them together. In a more practical example we may have 1000 customers, 10,000 customers, or more. When we have that many customers, we can greatly simplify things by grouping like customers together. . We can group customers using our RFM KPIs, but there may be more information we want to pull into that decision. For example if we are a manufacturing company that sells product to end users as well as distributors we may want to take that into account. A distributor is likely to behave and react differently from an individual person. For example, an individual person may get a coupon in the mail and decide to purchase, but a distributor contact may get hundreds of coupons a week and just get annoyed. . A lot of time should be spent exploring different ways to break up customers. Explaining how you break up the customers into groups to sales people, product management, purchasing, production control, and other functional groups in your company can be a great way to get ideas. Each of these groups sees how customers react in different ways and talking to all of them can help give a more complete picture. . Create probability matrix . The goal of customer analytics is to predict how much money we are going to make next month, and the month after, and the month after that, and so on. Once we can predict that with reasonable accuracy we can determine exactly how much revenue we can expect from each customer if we do not change anything. . We need to put probabilities on customer actions. In simplest terms the customer has 2 choices during each cycle, buy or do not buy. So how do we figure out what the chance of that is for a group? . Suppose we a group of distributors we sell to that we believe act similarly as with the table below. A 1 means that the distributor bought something in that cycle, a 0 means that the distributor did not buy anything in that cycle. .   January February March . Distributor A | 1 | 0 | 0 | . Distributor B | 0 | 1 | 0 | . Distributor C | 0 | 1 | 1 | . Since there are 9 total opportunities to buy, and 4 of those have buys in them, we will assign a 4/9 chance of buying. Really this is just another way to measure frequency. . Now we know how many of the customers in the distributor group will buy in a given month (4 out of 9), so we need to turn that into a revenue figure. To do this we will take the sum of revenue from these customers and divide that by the number of buys. Here we are calculating Monetary Value (M), but instead of looking at it on a customer level, we are looking at it for a group of customers. . Once we multiply those two numbers together we have an estimated monthly revenue. . There’s several areas for future improvement that I won’t cover in this post. Depending on the market or the business model your company uses, they will vary in significance. Here’s a few of them: . Take into account recency. | Take into account customer retention. This makes forecasts accuracy go down farther out you go into the future. Since for this application we aren’t looking out very far in the future it makes less of a difference, but it is definitely high priority on our list of future enhancements. | Changes in buy patterns based on seasonal trends. | . Increase revenue using predicted value of each account . Now that we know roughly what revenue we can expect, we can use that for a variety of things. Here’s a few ideas: . First, we can look at which locations/territories are performing as forcasted. Once we know the high performing locations and the low performing locations, we can start to identify what the differences are. This can also be used to manage performance of employees, set meaningful incentives based on revenue, and set measurable objectives. | We can continue to do A/B testing, but be able to tie this to actual revenue dollars. | We can use these forecasts to determine inventory stances on either components or salable goods. | Now that we have grouped customers that behave similarly, we can start to determine how we should treat each group. We can have limited A/B testing. Instead of taking a random group of customers, we can take a random group of distributors to see how they specifically react. | We can determine who our most valuable customers are. Typically without measures, we remember the big purchases. But a regular purchaser that doesn’t make big purchases may be bringing us more revenue. Understanding which customers are most likely to be the most profitable is crucial and can help us determine pricing tiers for customers or determine customer loyalty programs. | We can identify low performing groups and create a strategy to get more penetration into those customers. Maybe for those customers we can send them surveys. Maybe we can set up a forum and do 6 month surveys where we work and collaborate with them regularly. This can help give insight into why they don’t purchase much, as well as be a good sales opportunity. We can also try to take action to take the low performing customers and get them to increase revenue through promotions or other incentives. | We can take the high performing customers and make sure that they know they are appreciated. Whether it’s a can of cashews over the holidays, or having a sales person reach out to talk once a month, building that relationship can really build customer loyalty. | .",
            "url": "https://isaac-flath.github.io/blog/customer%20analytics/2020/05/12/Customer-Analytics-Part-2.html",
            "relUrl": "/customer%20analytics/2020/05/12/Customer-Analytics-Part-2.html",
            "date": " • May 12, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Factorization into $A=LU$ (18.06_L4)",
            "content": "Inverse of $AB$, $A^T$ . Review . So we know from previous lectures/blog posts that $A$ time the inverse of $A$ gives the Identify Matrix $I$. The Identify matrix is matrix equivilent to multiplying by 1, no change to the matrix. . $AA^{-1}=I=A^{-1}A$ . AB Inverse . So if the Inverse of a Matrix time it&#39;s inverse gives the identity, how do we find the inverse of a product of matrices? It turns out you just multiply the inverses of those matrices in reverse. . Why in reverse? If you put socks and shoes on, you wouldn&#39;t invert that process by taking socks off first. You would invert it by doing the steps in reverse. Shoes off first. . A more technical understanding of this can be seen below: . $ABB^{-1}A^{-1}= I$ . Let&#39;s multiply the middle piece first. $BB^{-1}$ We know that equals the Identify matrix, and a matrix time the identity just gives that matrix. So then what you are left wtih is $AA^{-1}=I$, which we also know is true. . This work the other way as well: . $B^{-1}(A^{-1}A)B = I$ . $A^T$ . Transposes work in a similar way. We can tak transposes in reverse to get the Identity Matrix. . $AA^{-1}=I$ . $(A^{-1})^TA^T=I$ . Converting to $A = LU$ . 2x2 to $A=LU$ . Previously we learned that using Elimination we create these Elimination Matrices. In this example of a 2x2 matrix: . $E_{21}A = U =&gt; $ $ left[ begin{array}{cc} 1 &amp; 0 -4 &amp; 1 end{array} right]$ $ left[ begin{array}{cc} 2 &amp; 1 8 &amp; 7 end{array} right]$ $=$ $ left[ begin{array}{cc} 2 &amp; 1 0 &amp; 3 end{array} right]$ . Now how do we get $A=LU$ from this. We can see matrix $A$ and matrix $U$ are already in the right place, so lets leave those alone. We can see that we need to turn Matrix $E_{21}$ into Matrix $L$ by switching it&#39;s sides. The way we move a matrix from one side to the other is through the inverse of the matrix. . $A=LU=&gt;$ $ left[ begin{array}{cc} 2 &amp; 1 8 &amp; 7 end{array} right]$ $=$ $ left[ begin{array}{cc} 1 &amp; 0 4 &amp; 1 end{array} right]$ $ left[ begin{array}{cc} 2 &amp; 1 0 &amp; 3 end{array} right]$ . Note: $E_{21}A = U$ is just a different format of $A=LU$. In $A=LU$ you can see $L$ is just $E_{21}^{-1}$ . 3x3 to $A = LU$ . As we saw in an earlier blog, when we do elimination of a 3x3 matrix we would get more Elmination matrices. We are still working examples with no row exchanges. . $$E_{32}E_{31}E_{21}A=U$$ . So how do we convert this into $A = LU$? As we learned above, we can invert the product of matrices as well. Let&#39;s apply that here with these matrices. . $$A=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1}U$$ . Well that doesn&#39;t look as nice to work with as $A = LU$. So let&#39;s just simplify it by multiplying all the Elimination inverse matrices together. Meaning L would be a product of these 3 inverse matrices. . $$A=E^{-1}U=LU$$ . $A=LU$ vs $EA=U$ . What&#39;s the point of taking the inverse. It&#39;s the same formula mathamatrically, so why bother with this transformation. Why $E^{-1}$ on the right and not just stay with $E$ on the left? Let&#39;s do an example. . $E_{32}E_{21}=E =&gt;$ $ left[ begin{array}{ccc} 1 &amp; 1 &amp; 0 0 &amp; 1 &amp; 0 0 &amp; -5 &amp; 1 end{array} right]$ $ left[ begin{array}{ccc} 1 &amp; 0 &amp; 0 -2 &amp; 1 &amp; 0 0 &amp; 0 &amp; 1 end{array} right]$ $=$ $ left[ begin{array}{ccc} 1 &amp; 0 &amp; 0 -2 &amp; 1 &amp; 0 10 &amp; -5 &amp; 1 end{array} right]$ . $E_{21}^{-1}E_{32}^{-1}=E^{-1} =&gt;$ $ left[ begin{array}{ccc} 1 &amp; 0 &amp; 0 2 &amp; 1 &amp; 0 0 &amp; 0 &amp; 1 end{array} right]$ $ left[ begin{array}{ccc} 1 &amp; 1 &amp; 0 0 &amp; 1 &amp; 0 0 &amp; 5 &amp; 1 end{array} right]$ $=$ $ left[ begin{array}{ccc} 1 &amp; 0 &amp; 0 2 &amp; 1 &amp; 0 0 &amp; 5 &amp; 1 end{array} right]$ . Note: 2 and 5 were the multipliers that we used in our elinimation steps. We can see that with $E^{-1}$ has 2 and 5 in the matrix. So if there are no row exchanges L is just a record of what the multipliers were, which is very convenient. Worded another way, with no row exchanges the multipliers go directly into L. .",
            "url": "https://isaac-flath.github.io/blog/linear%20algebra/2020/05/12/18.06_4_Factorization-into-a-lu.html",
            "relUrl": "/linear%20algebra/2020/05/12/18.06_4_Factorization-into-a-lu.html",
            "date": " • May 12, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Random Forest Classifier",
            "content": "Goal . The goal is to predict whether a passenger on the Titanic survived or not. The applications for binary classification are endless and could be applied to many real world problems. Does this patient have this disease? Will this customer Churn? Will price go up? These are just a few examples. . The purpose is to give a general guide to classification. If you get through this and want more detail, I highly recommend checking out the Tabular Chapter of Deep Learning for Coders with fastai &amp; Pytorch by Jeremy Howard and Sylvain Gugger. The book primarily focuses on deep learning, though decision trees are covered for tabular data. All of the material in this guide and more is covered in much greater detail in that book. . https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527 . Setup . We are going to start with loading libraries and datasets that are needed. I am going to skip over this as they are pretty self explanatory, but feel free to look close if you would like. . I am going to use Seaborn to load the Titanic dataset. . #collapse-hide from sklearn.ensemble import RandomForestClassifier import seaborn as sns import pandas as pd import numpy as np from fastai2.tabular.all import * from fastai2 import * from sklearn.model_selection import GridSearchCV from dtreeviz.trees import * from scipy.cluster import hierarchy as hc df = sns.load_dataset(&#39;titanic&#39;) df.head() . . survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone . 0 | 0 | 3 | male | 22.0 | 1 | 0 | 7.2500 | S | Third | man | True | NaN | Southampton | no | False | . 1 | 1 | 1 | female | 38.0 | 1 | 0 | 71.2833 | C | First | woman | False | C | Cherbourg | yes | False | . 2 | 1 | 3 | female | 26.0 | 0 | 0 | 7.9250 | S | Third | woman | False | NaN | Southampton | yes | True | . 3 | 1 | 1 | female | 35.0 | 1 | 0 | 53.1000 | S | First | woman | False | C | Southampton | yes | False | . 4 | 0 | 3 | male | 35.0 | 0 | 0 | 8.0500 | S | Third | man | True | NaN | Southampton | no | True | . Data Setup . Dependent Variable . We want to define what we are predicting, or the dependent variable. We also see that &#39;Survived&#39; and &#39;alive&#39; columns are the same thing with different names. We need to get rid of one and predict the other. . # survived is a duplicate of alive, so get rid of it df.drop(&#39;survived&#39;,axis = 1, inplace=True) dep_var = &#39;alive&#39; . Training and Validation Set Split . Best practice is to minimally have a training and validation set. Those are the 2 that we will use for this tutorial. . Training Set: This is what the model actually trains on | Validation Set: This is used to gauge success of the Training | Test Set: This is a held out of the total process to be an additional safeguard against overfitting | . cond = np.random.rand(len(df))&gt;.2 train = np.where(cond)[0] valid = np.where(~cond)[0] splits = (list(train),list(valid)) . Dates . We don&#39;t have any dates to deal with, but if we did we would do the following: . df = add_datepart(df,&#39;date&#39;) . This would replace that date with a ton of different columns, such as the year, the day number, the day of the week, is it month end, is it month start, and more. . Categorical Variables . Ordinal Categorical Variables . Some categorical variables have a natural heirarchy. By telling pandas the order it tends to mean trees don&#39;t have to split as many times, which speeds up training times. . df[&#39;class&#39;].unique() . [Third, First, Second] Categories (3, object): [Third, First, Second] . classes = &#39;First&#39;,&#39;Second&#39;,&#39;Third&#39; . df[&#39;class&#39;] = df[&#39;class&#39;].astype(&#39;category&#39;) df[&#39;class&#39;].cat.set_categories(classes, ordered=True, inplace=True) . Categorical Variables Final . We are not going to do some data cleaning. The Categorify and FillMissing functions in the fastai2 library make this easy. . procs = [Categorify, FillMissing] . cont,cat = cont_cat_split(df, 1, dep_var=dep_var) . to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) . Let&#39;s take a look at the training and validation sets and make sure we have a good split of each. . len(to.train),len(to.valid) . (691, 200) . We can now take a look and see that while we see all the same data, behind the scenes it is all numeric. This is exactly what we need for our random forest. . to.show(3) . sex embarked class who adult_male deck embark_town alone age_na pclass age sibsp parch fare alive . 1 female | C | First | woman | False | C | Cherbourg | False | False | 1.0 | 38.0 | 1.0 | 0.0 | 71.283302 | yes | . 2 female | S | Third | woman | False | #na# | Southampton | True | False | 3.0 | 26.0 | 0.0 | 0.0 | 7.925000 | yes | . 4 male | S | Third | man | True | #na# | Southampton | True | False | 3.0 | 35.0 | 0.0 | 0.0 | 8.050000 | no | . to.items.head(3) . pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone age_na . 1 | 1.0 | 1 | 38.0 | 1.0 | 0.0 | 71.283302 | 1 | 1 | 3 | 1 | 3 | 1 | 1 | 1 | 1 | . 2 | 3.0 | 1 | 26.0 | 0.0 | 0.0 | 7.925000 | 3 | 3 | 3 | 1 | 0 | 3 | 1 | 2 | 1 | . 4 | 3.0 | 2 | 35.0 | 0.0 | 0.0 | 8.050000 | 3 | 3 | 2 | 2 | 0 | 3 | 0 | 2 | 1 | . Final Change . Finally, we will put just the data in xs and ys so they are in easy format to pass to models. . xs,y = to.train.xs,to.train.y valid_xs,valid_y = to.valid.xs,to.valid.y . Random Forest Model . Initial Model . Let&#39;s start by creating a model without tuning and see how it does . m = RandomForestClassifier(n_estimators=100) m = m.fit(xs,y) . from sklearn.metrics import confusion_matrix . confusion_matrix(y,m.predict(xs)) . array([[433, 1], [ 8, 249]]) . Looking pretty good! Only 7 wrong. Let&#39;s see how it did on the validation set. . confusion_matrix(valid_y,m.predict(valid_xs)) . array([[105, 10], [ 23, 62]]) . Still way better than 50/50, but not quite as good. This is because the model did not train based on this validation data so it doesn&#39;t perform nearly as well. . Model Tuning - Grid Search . We made our first model, and it doesn&#39;t seem to predict as well as we would like. Let&#39;s do something about that. . We are going to do a grid search. There are many more sophisticated ways to find parameters (maybe a future post), but the grid search is easy to understand. Basically you pick some ranges, and you try them all to see what works best. . We will use the built in gridsearch. All we need to do is define the range of parameters, and let it find the best model. . parameters = {&#39;n_estimators&#39;:range(10,20,20), &#39;max_depth&#39;:range(10,20,20), &#39;min_samples_split&#39;:range(2,20,1), &#39;max_features&#39;:[&#39;auto&#39;,&#39;log2&#39;]} . clf = GridSearchCV(RandomForestClassifier(), parameters, n_jobs=-1) . clf.fit(xs,y) . /home/isaacflath/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning. warnings.warn(CV_WARNING, FutureWarning) . GridSearchCV(cv=&#39;warn&#39;, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=&#39;warn&#39;, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;max_depth&#39;: range(10, 20, 20), &#39;max_features&#39;: [&#39;auto&#39;, &#39;log2&#39;], &#39;min_samples_split&#39;: range(2, 20), &#39;n_estimators&#39;: range(10, 20, 20)}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . Results . We can see below that the best esimator works better for prediciton the validation set than the model above did. Success! . confusion_matrix(y,clf.best_estimator_.predict(xs)) . array([[407, 27], [ 72, 185]]) . confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs)) . array([[108, 7], [ 23, 62]]) . Model Minimizing . Now that we have good results with a tuned model, we may want to simplify the model. If we can simplify the model without significantly impacting accuracy, that&#39;s good for many reasons. . The model is easier to understand | Fewer variables means fewer data quality issues and more focused data quality efforts | It takes less resources and time to run | Feature Importance . There are many ways to measure importance. How often do we use a feature to split? How high up in the tree is it used to split? We are going to use scikit learns feature importance information. . Let&#39;s look at what features are! . #collapse def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) . . fi = rf_feat_importance(m, xs) fi[:5] . cols imp . 13 | fare | 0.216826 | . 10 | age | 0.210577 | . 4 | adult_male | 0.125894 | . 0 | sex | 0.089614 | . 3 | who | 0.082446 | . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:30]); . Remove Low Important Variables . This isn&#39;t strictly neccesarry, but it is nice to simplify models if you can. Simpler models are easier to understand and maintain, and they take less resources to run. It is also interesting to know just how many variables are needed to predict. . to_keep = fi[fi.imp&gt;0.05].cols len(to_keep) . 6 . xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] . clf = GridSearchCV(RandomForestClassifier(), parameters, n_jobs=-1) clf.fit(xs_imp,y) . /home/isaacflath/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning. warnings.warn(CV_WARNING, FutureWarning) . GridSearchCV(cv=&#39;warn&#39;, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=&#39;warn&#39;, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;max_depth&#39;: range(10, 20, 20), &#39;max_features&#39;: [&#39;auto&#39;, &#39;log2&#39;], &#39;min_samples_split&#39;: range(2, 20), &#39;n_estimators&#39;: range(10, 20, 20)}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . Results . Now we see with only 6 features we still get pretty good results on on validation set. . Now the question is whether this small loss in accuracy outweighed by a simpler and more efficient model? That is a business question more than it is a data science question. . If you are detecting COVID-19, you probably want it to be as accurate as possible. If you are going to predict whether someone is a cat or a dog person based on a survey for marketing purposes, small changes in accuracy probably are not as critical. . confusion_matrix(y,clf.best_estimator_.predict(xs_imp)) . array([[390, 44], [ 59, 198]]) . confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp)) . array([[99, 16], [21, 64]]) . clf.best_estimator_ . RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=10, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=16, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . Redundant columns . Of the 6 remaining variables, we can see that some of them are very related. It makes sense to me that deck and fare are related. Nicer areas probably cost more. It makes sense to me that the person&#39;s sex has some redudancy with adult_male - the redundancy is even in the name. . #collapse def cluster_columns(df, figsize=(10,6), font_size=12): corr = np.round(scipy.stats.spearmanr(df).correlation, 4) corr_condensed = hc.distance.squareform(1-corr) z = hc.linkage(corr_condensed, method=&#39;average&#39;) fig = plt.figure(figsize=figsize) hc.dendrogram(z, labels=df.columns, orientation=&#39;left&#39;, leaf_font_size=font_size) plt.show() . . cluster_columns(xs_imp) . Let&#39;s experiment with removing some columns and see what we get. We will use accuracy for our metric. . Here is out baseline: . #collapse print(&quot;accuracy: &quot;) (confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp))[0,0] + confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp))[1,1] )/ confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp)).sum() . . accuracy: . 0.815 . #collapse def get_accuracy(x,y,valid_x,valid_y): m = RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=10, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=4, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) m.fit(xs_imp,y) print((confusion_matrix(valid_y,m.predict(valid_xs_imp))[0,0] + confusion_matrix(valid_y,m.predict(valid_xs_imp))[1,1] )/ confusion_matrix(valid_y,m.predict(valid_xs_imp)).sum()) . . We will now loop through each of the remaining variables and train a model and print out the accuracy score. . Judging by the scores below, removing any 1 variable does not significantly reduce the accuracy. This means that we have redundant columns that can likely be trimmed. Sex seems to be a column we would definitely keep as removing it have the most impact on accuracy. . From this we can remove variables and iterate through to continue simplifying as much as possible. . #collapse variables = [&#39;sex&#39;,&#39;adult_male&#39;,&#39;who&#39;,&#39;age&#39;,&#39;deck&#39;,&#39;fare&#39;] for variable in variables: print(&#39;drop &#39;+variable+&#39; accuracy:&#39;) get_accuracy(xs_imp.drop(variable, axis=1), y, valid_xs_imp.drop(variable, axis=1), valid_y) . . drop sex accuracy: 0.815 drop adult_male accuracy: 0.825 drop who accuracy: 0.81 drop age accuracy: 0.855 drop deck accuracy: 0.835 drop fare accuracy: 0.825 .",
            "url": "https://isaac-flath.github.io/blog/tree%20ensembles/2020/05/11/RandomForestClassifier.html",
            "relUrl": "/tree%20ensembles/2020/05/11/RandomForestClassifier.html",
            "date": " • May 11, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Multiplication and Inverse Matrices (18.06_L3)",
            "content": "Matrix Multiplication . Rules . A matrix is laid out by row and column. Menaing, a particular cell in matrix $C$ is $C_{ij}$. For example, $C_{34}$ is the 3rd row, 4th column in matrix $C$. . $ begin{bmatrix} 1,1&amp;1,2&amp;1,3&amp;1,4 2,1&amp;2,2&amp;2,3&amp;2,4 3,1&amp;3,2&amp;3,3&amp;3,4 4,1&amp;4,2&amp;4,3&amp;4,4 end{bmatrix}$ . Number of columns in matrix A must match number of rows in matrix B. | The output of matrix multiplication will be dimensions equal to the number of rows in matrix A by the number of columns in matrix B. | You can cut the matrix into blocks and do matrix multiplication in blocks. | $ begin{bmatrix}A_1&amp;A_2 A_3&amp;A_4 end{bmatrix} begin{bmatrix}B_1&amp;B_2 B_3&amp;B_4 end{bmatrix}= begin{bmatrix}A_1B_1+A_2B_3&amp;A_1B_2+A_2B_4 A_3B_1+A_4B_3&amp;A_3B_2+A_4B_4 end{bmatrix}$ . Note: While cutting it into blocks may not seem useful immediately, it is crucial for high speed computation. In Deep Learning where you are multiplying large matrices together can speed up computation speed immensely by breaking them into blocks so you can fit the blocks into CPU Memory. In fact, this is exactly what pytorch does. . 1st Way . Let&#39;s imagine we have a matrix multiplation . $$AB=C$$ . $C_{34}=$(row 3 of A)$ cdot$(column 4 of B) . $C_{34}=a_{31}b_{14}+a_{32}b_{24}+......$ . $C_{34}= sum limits_{k=1}^n a_{3k}b_{k4}$ . 2nd Way . $$AB=C$$ . The second way to think about it is that matrix $A$ times by the first column of matrix $B$ will give you the first column of $C$. . Matrix $A$ times by the second column of matrix $B$ will give you the second column of $C$. . Now really what we are doing is thinking of columns of $C$ as combinations of columns of $A$ . $A cdot B_{n1} =C_{m1}$ . 3rd Way . $$AB=C$$ . The third way to think about it is that a row of $A$ times matrix $B$ will give you a column of $C$. . Now really what we are doing is thinking of rows of $C$ as combinations of rows of $B$. . 4th Way . If we multiply a column by a row, we get a full sized matrix. We also see that the columns are multiples of the column, and the rows and multiples of the row. This is what we expect as we just discussed above with the combinations of each other. . $ begin{bmatrix}2 3 4 end{bmatrix} begin{bmatrix}1&amp;6 end{bmatrix}= begin{bmatrix}2&amp;12 3&amp;18 4&amp;24 end{bmatrix}$ . $AB=$Sum of (Cols of A) $ cdot$ (Rows of B) . $ begin{bmatrix}2&amp;7 3&amp;8 4&amp;9 end{bmatrix} begin{bmatrix}1&amp;6 0&amp;0 end{bmatrix}$ $=$ $ begin{bmatrix}2 3 4 end{bmatrix} begin{bmatrix}1&amp;6 end{bmatrix}+$ $ begin{bmatrix}7 8 9 end{bmatrix} begin{bmatrix}0&amp;0 end{bmatrix}$ $=$ $ begin{bmatrix}2&amp;12 3&amp;18 4&amp;24 end{bmatrix}$ . This matrix all sit on the same line because they are just multiples. . Inverses . Not all Matrices have inverses. Most important question is whether it&#39;s invertable. If $A^-1$ exists. . $A^{-1}A = I = AA^{-1}$ . Invertable, nonsingular are the good case. . Matrices with no inverse . In the singular case there is no inverse. 2x2 matrix that has no inverse. . $ begin{bmatrix}1&amp;3 2&amp;6 end{bmatrix}$ . Cannot get 1,0, because I can find a vector $x$ with $Ax=0$ . $ begin{bmatrix}1&amp;3 2&amp;6 end{bmatrix} begin{bmatrix}3 -1 end{bmatrix}= begin{bmatrix}0 0 end{bmatrix}$ . But why does this matter? Because if I use these values for X anything I multiply by cannot possibly give the identify matrix, because 0 times anything gives 0. . Matrices with an inverse . $AA^{-1}=I =&gt; begin{bmatrix}1&amp;3 2&amp;7 end{bmatrix} begin{bmatrix}a&amp;b c&amp;d end{bmatrix}= begin{bmatrix}1&amp;0 0&amp;1 end{bmatrix}$ . Now from our matrix multiplication work above we know that $A cdot$column $j$ of $A^{-1} = $column $j$ of $I$ . So we really have a system of 2 equations to solve. . $ begin{bmatrix}1&amp;3 2&amp;7 end{bmatrix} begin{bmatrix}a c end{bmatrix}= begin{bmatrix}1 0 end{bmatrix}$ . $ begin{bmatrix}1&amp;3 2&amp;7 end{bmatrix} begin{bmatrix}b d end{bmatrix}= begin{bmatrix}0 1 end{bmatrix}$ . So we are back to solving systems of equations. . Gauss-Jordan / Find $A^{-1}$ . Solve 2 equations at once . $ begin{bmatrix}1&amp;3 2&amp;7 end{bmatrix} begin{bmatrix}a b end{bmatrix}= begin{bmatrix}1 0 end{bmatrix}$ . $ begin{bmatrix}1&amp;3 2&amp;7 end{bmatrix} begin{bmatrix}c d end{bmatrix}= begin{bmatrix}0 1 end{bmatrix}$ . The gauss-Jordan Method solved both equations at once by created an augmented matrix. . We will now do elimination steps to get the identify on the left. This will convert A to A inverse. . Start with $AI$ | Elimination step subtracting 2 of the first row from the second row | Elinination step subtracting 3 of the second row from the first. | End with $IA^{-1}$ | $ left[ begin{array}{cc|cc} 1 &amp; 3 &amp; 1 &amp; 0 2 &amp; 7 &amp; 0 &amp; 1 end{array} right] =&gt; left[ begin{array}{cc|cc} 1 &amp; 3 &amp; 1 &amp; 0 0 &amp; 1 &amp; -2 &amp; 1 end{array} right] =&gt; left[ begin{array}{cc|cc} 1 &amp; 0 &amp; 7 &amp; -3 0 &amp; 1 &amp; -2 &amp; 1 end{array} right] $ . Naturally we get the inverse because we really reverse the sides of the equation. Just like in algebra if you have -5 on one side, you can move it by putting the inverse on the other side (+5). .",
            "url": "https://isaac-flath.github.io/blog/linear%20algebra/2020/05/11/18.06_3_Multiplication-and-Inverse-Matrices.html",
            "relUrl": "/linear%20algebra/2020/05/11/18.06_3_Multiplication-and-Inverse-Matrices.html",
            "date": " • May 11, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Descriptive Customer Analytics (Part 1)",
            "content": "Intro . Customer Analytics is a valuable tool, but often the basics are overlooked. It can have a huge impact, and it doesn’t always mean complex modeling. It always starts with the basics. Because customer analytics can make such a big impact, I am going to break this into several posts. . In this post (Part 1) I am going to explain a basic descriptive customer analytics framework and how use it to identify the current state.  This understanding allows us to conduct A/B testing so we can measure how our actions impact revenue and by how much. There are many ways to do this, this is just one of them. . In Part 2 I’ll talk about how to build on the descriptive framework to create a predictive model.  A predictive model allows companies to plan proactively instead of reactively. . Background: . Many companies rely on experience and basic summarizing of historical statistics to make decisions.  While both of these are useful tools, a powerful way to successfully sustain long term revenue growth is to add customer analytics to their tool belt.  In this post I am discussing the starting point for analytics, descriptive analytics. . An easy and solid starting point for descriptive analytics is a RFM (Recency, Frequency, Monetary Value) model.  A primary advantage to this is the only data you need is purchase history for each customer, which is information that almost every company tracks.  I’ve outlined the three main steps to creating this framework and using it to increase revenue. . Determine length of customer buying cycle: . The first step is to determine what time range is a good time range to use for a buying cycle.  For a grocery store, maybe we say typically people go grocery shopping once a week.  In that scenario we would use 1 week as our time range for our cycle.  However a company that prints business cards may find a much longer time frame as customers tend to buy 1,000 and repurchase when they get low.  For this example we could use a 6 month cycle.  I encourage people to explore their data and pick a time range based on intuition.  There are better and more complicated ways to figure out this time range, but I encourage people to demonstrate results and revenue growth before circling back with more resources and support from executive management. . Calculate descriptive customer metrics: . Let’s say we choose a 1 month cycle.  The next step is to create and fill in a table to get frequency and monetary Value.  In the table below I outline how we calculate both of those KPIs (Key Performance Indicators). .   January February March Frequency Monetary Value . Example | Rev | Rev | Rev | # of slots purchased in | Average Monetary Value | . Customer A | $102 | $60 | $75 | 3 | =($102+$60+$75) / 3 | . Customer B | $97 | $85 | $0 | 2 | =($85+97) / 2 | . Customer C | $0 | $9852 | $0 | 1 | =($9,852) / 1 | . Once this is done add another column for Recency and fill that in with how long since that last purchase.  This can be days, hours, or seconds since last purchase depending on how far apart customer purchases typically are.  For this simplified version we will use days.  We end up with a table like this: .   Recency Frequency Monetary Value . Customer A | 3 | 3 | $79 | . Customer B | 35 | 2 | $91 | . Customer C | 48 | 1 | $9852 | . Conduct A/B testing: . Now that we have three metrics (RFM) that describe customer buying patterns, we can start using them to run experiments to drive revenue growth.  A/B testing is a simple experiment that you can run to compare two things. You split your customers into two group and make no change to what you are doing to group A, but do something different in group B.  By doing this we can compare the revenue of the two groups to see what impact our efforts had.  If B is better, we will apply that action to the entire customer base.  If not, we scrap the idea and move on.  This is a never-ending process of  testing new ideas. . Here’s a few ideas for A/B tests using RFM to get you started: . Call every customer in group B to see how that impacts revenue.  Does this increase Frequency (F) or average Monetary Value (M) of orders?  Is it worth the additional resources? | Send a marketing email blast to everyone in group B.  How does this change group B’s buying patterns? | What impact does sending a one-time-only 5 % off coupon have? | If we raise the price by 5 % in group B, what is the effect on revenue?  This is especially useful because we can run limited experiments and gauge reaction of price hikes without angering every customer. | If we raise the price on a product that is nearing the end of its product life cycle, do sales increase on the newly released product?  Does a price hike on old products encourage customers to buy newer versions? | Send group A customers to the current website and group B customers to a new website layout.  Monitor sales, traffic, and clicks on each site.  This can help determine the better layout. | .",
            "url": "https://isaac-flath.github.io/blog/customer%20analytics/2020/05/10/Customer-Analytics-Part-1.html",
            "relUrl": "/customer%20analytics/2020/05/10/Customer-Analytics-Part-1.html",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Elimination with Matrices (18.06_L2)",
            "content": "Motivation . One of my goals is to understand more deeply what Neural Networks are doing. Another is to have an easier time understanding and implementing cutting edge academic papers. In order to work toward those goals, I am revisiting the Math behind Neural Networks. This time my goal is to understand intuitively every piece of the material forward and backward - rather than to pass a course on a deadline. . This blog post will be my notes about Lecture 2 from the following course: . Gilbert Strang. 18.06 Linear Algebra. Spring 2010. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. . Goal . The goal is to solve N equations with N unknowns, just like it was with lecture 1. In the first Lecture it was about understanding how to solve them intuitively with linear combinations and matrix multiplication. In reality, that is not a scalable choice when you get to higher and higher dimensions. We need to be able to express these concepts in matrices and be more comfortable with that language and operations.. . Matrix Form . As a recap, we can express a system of equations by listing the equations. x . $x+2y+z=2$ . $3x+8y+z=12$ . $4y+z=2$ . We can write the same thing in matrix form using the formula $Ax=b$. The matrix A is from the system of equation above. . $A = begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix}$ . Elimination . The general idea in elimination is to isolate variables so we can solve the equation. For example, if we have 0x + 0y + 1z = 10, it is very easy to solve for z. We can then plug it into another equation in the series that has z and 1 other unknown to get another value, and so on and so forth. . $E_{21}$ . We are going to start by eliminating the first variable in the second row. Row 2, column 1. This would leave the second row with only 2 unknowns. . The way we do this is we subtract row 1 from row 2. If we substract 3 * 1 from 3, we get 0, so 3 will be the multiplier. . $ begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 0&amp;0&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix} = begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;4&amp;1 end{bmatrix}$ . $E_{31}$ . We then move on to row 3, column 1. Lucky for us it&#39;s already 0 so we can skip this step . $E_{32}$ . We now move on to row 3, column 2. If we can get this to 0, then we have 1 unknown in that column. That&#39;s what we want as that&#39;s easily solvable. . $ begin{bmatrix}1&amp;0&amp;0 0&amp;1&amp;0 0&amp;-2&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;4&amp;1 end{bmatrix} = begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix}$ . How can this fail? . A 0 cannot be in one of the pivot spots. The pivot positions are the diagonals. If that happens, elimination will fail. If you think about it, a 0 being in the pivot spot means that the left side of the equation is 0. Either the right side is also 0 and it gives you no information, or the right side is not 0 and there is no solution (ie 2 = 0 is False). . Of course, there are some tricks that can be added to minimize failures. The most common of these are row exchanges. By swapping the order of the rows, you can potentially solve an equation that would have failed in the original order. . For example, if I were to exchange the 2nd and 3rd rows I would multiply by the following matrix. . $ begin{bmatrix}1&amp;0&amp;0 0&amp;0&amp;1 0&amp;1&amp;0 end{bmatrix}$ . Back Substitution . Let&#39;s recap the steps that were taken. We started with A and through elimination in 2 steps we ended with a new matrix. . $A =&gt; begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix} =&gt; begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;4&amp;1 end{bmatrix} =&gt; begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix} =&gt; U$ . Finish the Equation . You may have realized these matrices represented the left side of the equation. You may have wondered how we can modify the left side of the equation while leaving the right side alone? The answer is that we cannot. What is done on the left side must be applied to the right side. Let&#39;s do that now. . We have done 2 transformations. . The first transformation was subtracting 3 of the first row from the second. | The second transformation was subracting 2 of the second row from the third. | Let&#39;s do that to the the right side . b =&gt; $ begin{bmatrix}2 12 2 end{bmatrix} =&gt; begin{bmatrix}2 6 2 end{bmatrix} =&gt; begin{bmatrix}2 6 -10 end{bmatrix} =&gt; c$ . Let&#39;s put the left and right side of the equations together to see what it looks like. . $Ux = C$ . $ begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix} begin{bmatrix}x y z end{bmatrix}= begin{bmatrix}2 6 -10 end{bmatrix}$ . Final Solution . Intuitive Solution . Great! Let&#39;s translate the $Ux=C$ matrix above back to our systems of equations view just to see if we can see how this transformation helps us. . $x + 2y + z = 2$ . $2y - 2z = 6$ . $5z = -10$ . When we look at it here, we can solve them with some simple algebra starting from the bottom equation. . $5z = -10 ; ; ; mathbf{=&gt;} ; ; ; z = -10/5 ; ; ; mathbf{=&gt;} ; ; ; z = -2$ . $2y - 2z = 6 ; ; ; mathbf{=&gt;} ; ; ; 2y - 2(-2) = 6 ; ; ; mathbf{=&gt;} ; ; ; 2y + 4 = 6 ; ; ; mathbf{=&gt;} ; ; ; 2y = 2 ; ; ; mathbf{=&gt;} ; ; ; y = 1$ . $x+2y+z=2 ; ; ; mathbf{=&gt;} ; ; ;x+2-2=2$$x+2y+z=2 ; ; ; mathbf{=&gt;} ; ; ;x=2$ . Matrix Solution . So first we should ask if we have an intuitive solution, why bother with doing the whole thing in matrix format? Isn&#39;t it the same thing? . And yes, we will be doing the same thing. The reason is scalability and ability to transfer to N equation and N unknowns. There&#39;s a limit to what can be done by hand, and matrix form allows for easier scalability. . Recap . We did several steps intuitively. Let&#39;s recap our elimination steps from above . $E_{21}$ step $ begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 0&amp;0&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix} = begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;4&amp;1 end{bmatrix}$ . $E_{32}$ step $ begin{bmatrix}1&amp;0&amp;0 0&amp;1&amp;0 0&amp;-2&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;4&amp;1 end{bmatrix} = begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix}$ . Simplify into 1 step . Great! Now let&#39;s just combine these so we only have 1 equation. . $E_{21}E_{32}A=U$ $ begin{bmatrix}1&amp;0&amp;0 0&amp;1&amp;0 0&amp;-2&amp;1 end{bmatrix} begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 0&amp;0&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix}= begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix} $ . Now let&#39;s simplify this by multiplying and combining my $E_{21}$ and $E_{31}$ matrices together. We will call the result $E$ . $E_{21}E_{32}=E$ $ begin{bmatrix}1&amp;0&amp;0 0&amp;1&amp;0 0&amp;-2&amp;1 end{bmatrix} begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 0&amp;0&amp;1 end{bmatrix} = begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 6&amp;-2&amp;1 end{bmatrix}$ . Great! Now we can use this to simplify our formula. . $EA=U$ $ begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 6&amp;-2&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix}= begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix}$ .",
            "url": "https://isaac-flath.github.io/blog/linear%20algebra/2020/05/10/18.06_2_EliminationWithMatrices.html",
            "relUrl": "/linear%20algebra/2020/05/10/18.06_2_EliminationWithMatrices.html",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Gradient Descent for Linear Regression",
            "content": "The goal of linear regression is to find parameter values that fit a linear function to data points.  The goodness of fit is measured by a cost function.  The lower the cost, the better the model.  Gradient Descent is a method to minimize a cost function.  Gradient descent is a widely used tool and is used frequently in tuning predictive models.  It’s important to understand the method so you can apply it to various models and are not limited to using black box models. . As I just mentioned, gradient descent is a method to reduce a cost function.  To understand how to minimize a cost function, you need to understand how cost is calculated.  For this post I will be using a very simple example; linear regression with one feature, two data points, and two regression coefficients.  I will use the sum of squares cost function to take the predicted line and slowly change the regression coefficients until the line passes through both points. . . In this example we could easily draw a line through the points without using gradient descent, but if we had more data points this would get trickier.  In the table below we can see what the data looks like that we are working with. . . The tree below illustrates how to solve for cost as well as how to improve the values of $ theta$ to minimize cost.  In the illustration above, $J = a^1 + a^2$ is the cost function we want to minimize.  As we can see, if the regression coefficients ($ theta_0+ theta_1$) do not give a good fit, then the difference between our predicted values and observed values will be large and we will have a high cost ($J$).  For low values, we will have a low cost ($J$).  The figure below shows us how to calculate cost from the regression coefficients ($ theta_0$ and $ theta_1$). . . The second thing this chart shows you is how to improve values of theta.  We used the formulas in the boxes to evaluate $J$, so now we will use the values on the edges to improve the parameter values.  Each regression coefficient has a path up to the cost function.  You get a path value for each $ theta$ on the tree by multiplying the edge values along that path.  For example: . $ theta_1 ; path ;value = x^1 (y^1_{pred} - y^1_{obs}) (1) + x^2 (y^2_{pred} - y^2_{obs}) (1)$ . The last step is to improve the value of $ theta$.  In order to improve the value of $ theta$, we need to multiply the path value by $ alpha$, and subtract that from that $ theta$.  $ alpha$ is a value that determines how large the increments will be taken during optimization.  If you pick an $ alpha$ value that is is too large, you risk missing the local optima.  If you choose an $ alpha$ value that is too small you will be very accurate, but it will be more computationally expensive. With more data points there would be more edges originating at $J$, and with more features there would be more thetas originating from the predicted values, but the same concept can be applied to these more complicated examples. .",
            "url": "https://isaac-flath.github.io/blog/gradient%20descent/2020/05/09/GradientDescentforLinearRegression.html",
            "relUrl": "/gradient%20descent/2020/05/09/GradientDescentforLinearRegression.html",
            "date": " • May 9, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Geometry of Linear Equations (18.06_L1)",
            "content": "#collapse-hide import matplotlib.pyplot as plt from torch import tensor from torch import solve import numpy as np from mpl_toolkits import mplot3d from mpl_toolkits.mplot3d import Axes3D . . Motivation . One of my goals is to understand more deeply what Neural Networks are doing. Another is to have an easier time understanding and implementing cutting edge academic papers. In order to work toward those goals, I am revisiting the Math behind Neural Networks. This time my goal is to understand intuitively every piece of the material forward and backward - rather than to pass a course on a deadline. . This blog post will be my notes about Lecture 1 from the following course: . Gilbert Strang. 18.06 Linear Algebra. Spring 2010. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. . Goal . The goal is to solve N equations with N unknowns. We will start with 2 equations with 2 unknowns, then go to 3 equations with 3 unknowns. We will use an intuitive approach here to understand a bit about how linear equations work. . How do we multiply these together? . Matrix Multiplication (Ax=b) . $ begin{bmatrix} 2 &amp; 5 1 &amp; 3 end{bmatrix}$ $ begin{bmatrix} 1 2 end{bmatrix}$ $=1$ $ begin{bmatrix} 2 1 end{bmatrix}$ $+2$ $ begin{bmatrix} 5 3 end{bmatrix}$ $=$ $ begin{bmatrix} 12 7 end{bmatrix}$ . Ax is a linear combination of columns . 2 equations 2 unknowns . Ok, Let&#39;s look at a couple equations in a few different ways. The solution to these are any values of x and y that make both equations true. . $2x - y = 0$ . $-x + 2y = 3$ . Row Picture . We can take these 2 linear equations and plot them. . $2x - y = 0$ could be writen as $y = 2x$, which is trivial to graph. If we graph them both, we can see visually where they are both true (the intersection). . def plot_equations_2d(x_range,y_dict): for y in y_dict: plt.plot(x, y_dict[y], label=y) plt.xlabel(&#39;x&#39;, color=&#39;#1C2833&#39;) plt.ylabel(&#39;y&#39;, color=&#39;#1C2833&#39;) plt.legend(loc=&#39;upper left&#39;) plt.grid() plt.show() x = tensor(np.linspace(-4,4,100)) y_dict = {&#39;2x-y=0&#39;:2*x, &#39;-x+2y=3&#39;:(3 + x)/2} plot_equations_2d(x,y_dict) . Column Picture . We can rewrite out equations into a different notation, which gives us a more concise view of what is going on. You can see that the top row is the first equation, and the bottom row is the second. Same thing, written differently. . $x$ $ begin{bmatrix} 2 -1 end{bmatrix}$ $+y$ $ begin{bmatrix} -1 2 end{bmatrix}$ $=$ $ begin{bmatrix} 0 3 end{bmatrix}$ . Graphed as Vectors . Now that we see them it in column form, we can graph the vectors . #collapse-hide strt_pts = tensor([[0,0],[2,-1]]) end_pts = tensor([[2,-1],[1,2]]) diff = end_pts - strt_pts plt.ylim([-3, 3]) plt.xlim([-3, 3]) plt.quiver(strt_pts[:,0], strt_pts[:,1], diff[:,0], diff[:,1], angles=&#39;xy&#39;, scale_units=&#39;xy&#39;, scale=1.) plt.show() . . Summed as Vectors . Now that they are represented as vectors. Let&#39;s add 1X + 2Y vectors and see that we get (0,3). Simply make one vector start where the other ends . #collapse-hide strt_pts = tensor([[0,0],[2,-1],[1,1]]) end_pts = tensor([[2,-1],[1,1],[0,3]]) diff = end_pts - strt_pts plt.ylim([-3, 3]) plt.xlim([-3, 3]) plt.quiver(strt_pts[:,0], strt_pts[:,1], diff[:,0], diff[:,1], angles=&#39;xy&#39;, scale_units=&#39;xy&#39;, scale=1.) plt.show() . . Matrix Form AX = b . We can see the same view in matrix notation. Same as the row and column view, just in a nice compressed format. . $ begin{bmatrix} 2 &amp; -1 -1 &amp; 2 end{bmatrix}$ $ begin{bmatrix} x y end{bmatrix}$ $=$ $ begin{bmatrix} 0 3 end{bmatrix}$ . Equations, 3 equations 3 unknowns . $2x - y = 0$ . $-x + 2y -z = -1$ . $-3y + 4z = 4$ . Matrix Form . $A=$ $ begin{bmatrix} 2 &amp; -1 &amp; 0 -1 &amp; 2 &amp; -1 0 &amp; -3 &amp; 4 end{bmatrix}$ $b=$ $ begin{bmatrix} 0 -1 4 end{bmatrix}$ . Row Picture . #collapse-hide fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) x, z = tensor(np.linspace(-8,8,100)), tensor(np.linspace(-8,8,100)) X, Z = np.meshgrid(x,z) Y1 = 2*X Y2 = (-1 + X + Z) / 2 Y3 = (4*Z - 4)/3 ax.plot_surface(X,Y1,Z, alpha=0.5, rstride=100, cstride=100) ax.plot_surface(X,Y2,Z, alpha=0.5, rstride=100, cstride=100) ax.plot_surface(X,Y3,Z, alpha=0.5, rstride=100, cstride=100) plt.show() . . Column Picture . We can create the column picture and graph vectors, just like in 2D space. Graphing in 3D is harder to see, but it&#39;s the same concept. . In this example we can clearly see the solution is x = 0, y = 0, z = 1. . $x$ $ begin{bmatrix} 2 -1 0 end{bmatrix}$ $+y$ $ begin{bmatrix} -1 2 -3 end{bmatrix}$ $+z$ $ begin{bmatrix} 0 -1 4 end{bmatrix}$ $=$ $ begin{bmatrix} 0 -1 4 end{bmatrix}$ . #collapse-hide strt_pts = tensor([[0,0,0],[0,0,0],[0,0,0]]) end_pts = tensor([[2,-1,0],[-1,2,-1],[0,-3,4]]) diff = end_pts - strt_pts fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.set_xlim([-5, 5]) ax.set_ylim([-5, 5]) ax.set_zlim([-5, 5]) plt.quiver(strt_pts[0,0], strt_pts[0,1], strt_pts[0,2], end_pts[0,0], end_pts[0,1], end_pts[0,2]) plt.quiver(strt_pts[1,0], strt_pts[1,1], strt_pts[1,2], end_pts[1,0], end_pts[1,1], end_pts[1,2]) plt.quiver(strt_pts[2,0], strt_pts[2,1], strt_pts[2,2], end_pts[2,0], end_pts[2,1], end_pts[2,2]) plt.show() . . Numpy Solver . Here&#39;s how you can solve the equation using Numpy. . a = np.array([[2, -1, 0], [-1, 2, -1], [0, -3, 4]]) b = np.array([0, -1, 4]) x = np.linalg.solve(a, b) print(x) . [ 0. -0. 1.] . Can I solve Ax = b for every b? . Do the linear combinations of the columns fill 3 dimensional space. . If you have some dimensionality in each direction, then you can. 3 equations with 3 unknowns can fill the 3D space as long as they don&#39;t sit on the same line or plane. . # lets expirament length_b = 20 b = np.array([list(np.random.rand(length_b)*10), list(np.random.rand(length_b)*10), list(np.random.rand(length_b)*10)]) for x in range(0,length_b): x = np.linalg.solve(a, b[:,x]) print(x) . [12.34080396 17.25419494 14.45223124] [4.3199813 5.08051595 5.06761607] [11.26347828 12.97487851 12.22512736] [5.70944029 7.59603383 7.26994807] [ 8.14004194 10.62910222 9.74376037] [2.95913308 5.38708946 4.07602836] [ 5.71158581 10.15204127 9.91490065] [ 9.64601458 14.15432693 11.82836017] [14.30704025 20.64995811 17.69860633] [ 7.83852946 14.51438959 12.77361829] [8.10022402 8.2034441 6.89683312] [4.83294145 4.45944243 3.902063 ] [4.97509585 9.38184883 9.47499918] [4.21358292 5.53636436 5.35998439] [ 7.49769118 11.83894164 10.43770683] [11.07197937 17.24535878 15.22695312] [6.41919127 6.39617205 5.86620567] [10.07150973 11.6354783 11.01565376] [10.97611756 12.95884772 10.99723627] [4.40229263 4.48480965 3.50238492] .",
            "url": "https://isaac-flath.github.io/blog/linear%20algebra/2020/05/09/18.06_1_GeometryOfLinearEquations.html",
            "relUrl": "/linear%20algebra/2020/05/09/18.06_1_GeometryOfLinearEquations.html",
            "date": " • May 9, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Jupyter Notebook Tutorial",
            "content": "Top 3 uses: . Exploratory analysis, model creation, data science, any kind of coding that require lots of rapid expiramentation and iteration. | Tutorials, guides, and blogs (like this one). Because you have a great mix of text functionality with code, they work really well for tutorials and guides. Rather than having static images, or code snippits that have to get updated each iteration, the code is part of the guide and it really simplifies the process. Notebooks can be exported directly to html and be opened in any browser to give to people. With the easy conversion to html, naturally it&#39;s easy to post them on a web page. | Technical presentations of results. You can have the actual code analysis done, with text explanations. Excess code can be collapsed so that if someone asks really detailed questions you can expand and have every piece of detail. Changes to the analysis are in the presentation so no need to save and put static images in other documents | Cell Types . A cell can be 3 different types. The most useful are code cells and markdown cells. . Code Cells . - Code cells run code The next few cells are examples of code cells - While the most common application is Python, you can set up environments easily to use R, swift, and other languages within jupyter notebooks . Markdown Cells . - This cell is a markdown cell. It is really nice for adding details and text explanations in where a code comment is not enough - They have all the normal markdown functionality, plus more. For example, I can write any technical or mathy stuff using latex, or create html tables in markdown or html. - I can also make markdown tables. . Latex Formulas . $$ begin{bmatrix}w_1&amp;w_2&amp;w_3&amp;w_4&amp;w_5 x_1&amp;x_2&amp;x_3&amp;x_4&amp;x_5 y_1&amp;y_2&amp;y_3&amp;y_4&amp;y_5 z_1&amp;z_2&amp;z_3&amp;z_4&amp;z_5 end{bmatrix}$$ . $ begin{align} frac{dy}{du} &amp;= f&#39;(u) = e^u = e^{ sin(x^2)}, frac{du}{dv} &amp;= g&#39;(v) = cos v = cos(x^2), frac{dv}{dx} &amp;= h&#39;(x) = 2x. end{align}$ . Markdown Table . This is a table for demos . perc | 55% | 22% | 23% | 12% | 53% | . qty | 23 | 19 | 150 | 9 | 92 | . #collapse-hide import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns pd.options.display.max_columns = None pd.options.display.max_rows = None %matplotlib inline . . Running Code . Naturally you can run code cells and print to the Jupyter Notebook . for x in range(0,5): print(x*10) . 0 10 20 30 40 . DataFrames . iris = sns.load_dataset(&#39;iris&#39;) iris[iris.petal_length &gt; 6] . sepal_length sepal_width petal_length petal_width species . 105 | 7.6 | 3.0 | 6.6 | 2.1 | virginica | . 107 | 7.3 | 2.9 | 6.3 | 1.8 | virginica | . 109 | 7.2 | 3.6 | 6.1 | 2.5 | virginica | . 117 | 7.7 | 3.8 | 6.7 | 2.2 | virginica | . 118 | 7.7 | 2.6 | 6.9 | 2.3 | virginica | . 122 | 7.7 | 2.8 | 6.7 | 2.0 | virginica | . 130 | 7.4 | 2.8 | 6.1 | 1.9 | virginica | . 131 | 7.9 | 3.8 | 6.4 | 2.0 | virginica | . 135 | 7.7 | 3.0 | 6.1 | 2.3 | virginica | . Plotting . Below we are going to make a few graphs to get the point accross. Naturally, each graph can be accompanied with a markdown cell that gives context and explains the value of that graph. . Line Chart . # evenly sampled time at 200ms intervals t = np.arange(0., 5., 0.2) # red dashes, blue squares and green triangles plt.plot(t, t, &#39;r--&#39;, t, t**2, &#39;bs&#39;, t, t**3, &#39;g^&#39;) plt.show() . Scatter Plot . Sometimes we will want to display a graph, but may not want all the code and details to be immediately visable. In these examples we can create a scatter plot like below, but collapse the code cell. . This is great when you want to show a graph and explain it, but the details of how the graph was created aren&#39;t that important. . #collapse-hide data = {&#39;a&#39;: np.arange(50), &#39;c&#39;: np.random.randint(0, 50, 50), &#39;d&#39;: np.random.randn(50)} data[&#39;b&#39;] = data[&#39;a&#39;] + 10 * np.random.randn(50) data[&#39;d&#39;] = np.abs(data[&#39;d&#39;]) * 100 plt.scatter(&#39;a&#39;, &#39;b&#39;, c=&#39;c&#39;, s=&#39;d&#39;, data=data) plt.xlabel(&#39;entry a&#39;) plt.ylabel(&#39;entry b&#39;) plt.show() . . Categorical Plot . We can create subplots to have multiple plots show up. This can be especially helpful when showing lots of the same information, or showing how 2 different metrics are related or need to be analyzed together . #collapse-hide names = [&#39;group_a&#39;, &#39;group_b&#39;, &#39;group_c&#39;] values = [1, 10, 100] plt.figure(figsize=(9, 3)) plt.subplot(131) plt.bar(names, values) plt.subplot(132) plt.scatter(names, values) plt.subplot(133) plt.plot(names, values) plt.suptitle(&#39;Categorical Plotting&#39;) plt.show() . . Stack Traces . When you run into an error, by default jupyter notebooks give you whatever the error message is, but also the entire stack trace. . There is a debug functionality, but I find that these stack traces and jupyter cells work even better than a debugger. I can break my code into as many cells as I want and run things interactively. Here&#39;s a few examples of stack traces. . Matrix Multiplication Good . Now we are going to show an example of errors where the stack trace isn&#39;t as simple. Suppose we are trying to multiply 2 arrays together (matrix multiplication). . a = np.array([ [1,2,4], [3,4,5], [5,6,7] ]) b = np.array([ [11,12,14], [31,14,15], [23,32,23] ]) a@b . array([[165, 168, 136], [272, 252, 217], [402, 368, 321]]) . Matrix Multiplication Bad . Now if it errors because the columns from matrix a don&#39;t match the rows from matrix b, we will get an error as matrix multiplication is impossible with those matrices. We see the same idea s the above for loop, stack trace with error and arrow pointing at the line that failed . # here&#39;s another a = np.array([ [1,2,4], [3,4,5], [5,6,7] ]) b = np.array([ [11,12,14], [31,14,15] ]) a@b . ValueError Traceback (most recent call last) &lt;ipython-input-8-52272ca444fc&gt; in &lt;module&gt; 9 [31,14,15] 10 ]) &gt; 11 a@b ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 3) . Second Layer of Bad . But what if the line we call isn&#39;t what fails? What if what I run works, but the function underneath fails? . In these example, you see the entire trace. It starts with are arrow at what you ran that errored. It then shows an arrow that your code called that caused the error, so you can track all the way back to the source. Here&#39;s how it shows a two step stack trace, but it can be as long as needed. . def matmul(a,b): c = a@b return c . matmul(a,b) . ValueError Traceback (most recent call last) &lt;ipython-input-10-7853c1c27063&gt; in &lt;module&gt; -&gt; 1 matmul(a,b) &lt;ipython-input-9-1c8b6b954779&gt; in matmul(a, b) 1 def matmul(a,b): -&gt; 2 c = a@b 3 return c ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 3) . Magic Commands . Magic commands are special commands for Juptyer Notebooks. They give you incredible functionality and you will likley find the experience very frustrating without them. A few that I use often are: . ? | put a question mark or 2 after a function or method to get the documentation. ?? gives more detail than ?. I can also use it to wild card search modules for functions. | shift tab | when you are writing something holding shift + tab will open a mini popup with the documentation for that thing. It may be a funciton, method, or module. | %who or %whos or %who_ls | These are all variants that list the objects and variables. I prefer %whos most of the time | %history | This allows you to look at the last pieces of code that you ran | $$ | wrapping latex code in dollar signs in a markdown cell renders latex code in markdown cells | ! | putting ! at the beginning of a line makes it run that in terminal. For example !ls | grep .csv | %time | I can use this to time the execution of things | . np.*array*? . np.array np.array2string np.array_equal np.array_equiv np.array_repr np.array_split np.array_str np.asanyarray np.asarray np.asarray_chkfinite np.ascontiguousarray np.asfarray np.asfortranarray np.broadcast_arrays np.chararray np.compare_chararrays np.get_array_wrap np.ndarray np.numarray np.recarray . np.array_equal?? . Signature: np.array_equal(a1, a2) Source: @array_function_dispatch(_array_equal_dispatcher) def array_equal(a1, a2): &#34;&#34;&#34; True if two arrays have the same shape and elements, False otherwise. Parameters - a1, a2 : array_like Input arrays. Returns - b : bool Returns True if the arrays are equal. See Also -- allclose: Returns True if two arrays are element-wise equal within a tolerance. array_equiv: Returns True if input arrays are shape consistent and all elements equal. Examples -- &gt;&gt;&gt; np.array_equal([1, 2], [1, 2]) True &gt;&gt;&gt; np.array_equal(np.array([1, 2]), np.array([1, 2])) True &gt;&gt;&gt; np.array_equal([1, 2], [1, 2, 3]) False &gt;&gt;&gt; np.array_equal([1, 2], [1, 4]) False &#34;&#34;&#34; try: a1, a2 = asarray(a1), asarray(a2) except Exception: return False if a1.shape != a2.shape: return False return bool(asarray(a1 == a2).all()) File: ~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py Type: function . a = np.array(np.random.rand(512,512)) b = np.array(np.random.rand(512,512)) %time for i in range(0,20): a@b . CPU times: user 198 ms, sys: 2.68 ms, total: 200 ms Wall time: 34.8 ms . %whos . Variable Type Data/Info a ndarray 512x512: 262144 elems, type `float64`, 2097152 bytes (2.0 Mb) b ndarray 512x512: 262144 elems, type `float64`, 2097152 bytes (2.0 Mb) data dict n=4 i int 19 iris DataFrame sepal_length sepal_&lt;...&gt; 1.8 virginica matmul function &lt;function matmul at 0x1a169cc680&gt; names list n=3 np module &lt;module &#39;numpy&#39; from &#39;/Us&lt;...&gt;kages/numpy/__init__.py&#39;&gt; pd module &lt;module &#39;pandas&#39; from &#39;/U&lt;...&gt;ages/pandas/__init__.py&#39;&gt; plt module &lt;module &#39;matplotlib.pyplo&lt;...&gt;es/matplotlib/pyplot.py&#39;&gt; sns module &lt;module &#39;seaborn&#39; from &#39;/&lt;...&gt;ges/seaborn/__init__.py&#39;&gt; t ndarray 25: 25 elems, type `float64`, 200 bytes values list n=3 x int 4 . %history -l 5 . matmul(a,b) np.*array*? np.array_equal?? a = np.array(np.random.rand(512,512)) b = np.array(np.random.rand(512,512)) %time for i in range(0,20): a@b %whos . Jupyter Extensions . There are many extensions to Jupyter Notebooks. After all a jupyter notebook is just a JSON file, so you can read the JSON in and manipulate and transform things however you want! There are many features, such as variable explorers, auto code timers, and more - but I find I that most are unneccesary. About half the people I talk to don&#39;t use any, and the other half use several. . NBDEV . NBdev is a jupyter extension/python library that allows you to do full development projects in Jupyter Notebooks. There have been books and libraries written entirely in Jupyter notebooks, including testing frameworks and unit testing that goes with them. A common misconception is that Jupyter notebooks cannot be used for that, though many people already have. . . There are many features NBdev adds. Here&#39;s a few. . Using notebooks written like this, nbdev can create and run any of the following with a single command: . Searchable, hyperlinked documentation; any word you surround in backticks will by automatically hyperlinked to the appropriate documentation | Cells in jupyter notebook marked with #export will be exported automatically to a python module | Python modules, following best practices such as automatically defining all (more details) with your exported functions, classes, and variables | Pip installers (uploaded to pypi for you) | Tests (defined directly in your notebooks, and run in parallel). | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks | . I reccomend checking them out for more detail https://github.com/fastai/nbdev .",
            "url": "https://isaac-flath.github.io/blog/jupyter/2020/05/07/JupyterNotebooks.html",
            "relUrl": "/jupyter/2020/05/07/JupyterNotebooks.html",
            "date": " • May 7, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a Data Scientist with a passion for teaching. . I really enjoy playing with machine learning algorithms so much of my content will probably relate to that in some way :grinning: . Basically anything I find interesting and/or helpful will be included. .",
          "url": "https://isaac-flath.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://isaac-flath.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}