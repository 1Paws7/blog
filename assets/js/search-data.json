{
  
    
        "post0": {
            "title": "Terraform Basics Part 1",
            "content": "Intro . In this blog we are going to take a look at Terraform. Terraform is tool that enables you to manage your cloud infrastructure in an auditable and programmatic way. In this initial post we are going to go through the most basic application, creating and spinning up an EC2 instance. Future posts will build on this for other applications . Setup . You will need to install Terraform in your terminal. I am not going to cover installation in this post, but you can check out terraform.io and install if from there. . What is Terraform . Many people think Terraform is just a bunch of config files, but rather than thinking of it as config files it is more helpful to think of it as a programming language. You can define lists, maps (similar to python dictionaries), strings, and perform actions using values in those variables. . Variables . For example, Here&#39;s how I could define a map, where I may store different ami&#39;s for different region. . variable &quot;AMIS&quot; { type = map(string) default = { us-east-1 = &quot;ami-0ac80df6eff0e70b5&quot;, us-east-2 = &quot;ami-31432143214213435&quot; } } . Then in terrarorm console get that value using: . var.AMIS.us-east-1 . So this is how we can get and store different configuration options. But Terraform is not just variables. You also have ways of running your code to deploy things. For example terraform plan shows you what it would do if you deployed. terraform apply deploys your options and terraform destroy destroys all your infrastructure defined. . But what is it applying? So far all we have done is define a variable. Let&#39;s define an EC2 instance. . Actions . resource &quot;aws_instance&quot; &quot;web&quot; { ami = &quot;${lookup(var.AMIS, var.AWS_REGION)}&quot; instance_type = &quot;t2.micro&quot; } . From the above we can see we have most of what we need to launch an EC2 instance. We define an AMI by looking up the AWS_Region variable from the AMIS variable we defined above. . Full EC2 setup . You may have noticed, there&#39;s a few things missing from the above. For example, how does it connect to AWS? How do you give it the credentials without putting them in github? We will create 4 files that gets a good organized setup for the full thing. . instance.tf: This will be the actual command that defined the EC2 instance we want to create | vars.tf: We can define all the variables we will need here | provider.tf: This will have the command needed to connect to our AWS account | terraform.tfvars: This will store our aws credentials | . vars.tf . First we will define all the variables we need. You may notice I do not put any values in the the credentials. This is on purpose - best practice is to keep access keys out of github. We will define these in a different file. . You can see we also set a default AWS_REGION as well as AMIs to use. I am only setting 1, but you can make as complicated of a map variable as you want with multiple regions, different types of instances (maybe a Deep learning ami, and a random forest ami, and a generally compute ami). . variable &quot;AWS_ACCESS_KEY&quot; {} variable &quot;AWS_SECRET_KEY&quot; {} variable &quot;AWS_REGION&quot; { default = &quot;us-east-1&quot; } variable &quot;AMIS&quot; { type = map(string) default = { us-east-1 = &quot;ami-0ac80df6eff0e70b5&quot; } } . terraform.tfvars . This is a pretty simple file where we define the values for our credentials. The important thing is to add this file to the gitignore so that it does not get put in the github reposity (for security reason). It just holds your credentials, for example it may be. . AWS_ACCESS_KEY = &quot;ABCDE12345FGHI6789&quot; AWS_SECRET_KEY = &quot;abc123def456+789ghi983klm . provider.tf . Next. we will connect to aws using the keys we defined in the terraform.tfvars and vars.tf files. The provider.tf file looks like this: . provider &quot;aws&quot; { access_key = &quot;${var.AWS_ACCESS_KEY}&quot; secret_key = &quot;${var.AWS_SECRET_KEY}&quot; region = &quot;${var.AWS_REGION}&quot; } . instance.tf . Now finally we are ready to launch our instance. We have our region, our credentials, and are connected to aws. Naturally we could have 100 different instances defined in this same way for different regions and instance types and it will automatically create them all, but for now we will just create 1 and stay within the free-tier. . resource &quot;aws_instance&quot; &quot;web&quot; { ami = &quot;${lookup(var.AMIS, var.AWS_REGION)}&quot; instance_type = &quot;t2.micro&quot; } . You will notice I am using the lookup funciton. This is looking up the var.AWS_REGION from the var.AMIS variable. We set the default region to us-east-1 in the vars.tf file. And it uses that to lookup that key in the AMIS variable, which is a map of strings. . Apply the Changes . Now all you need to do is run terraform apply in your terminal and your t2.micro ec2 instance will automatically be created! If it is a new folder for terraform, you will need to run terraform init in the folder before it will work. .",
            "url": "https://isaac-flath.github.io/blog/aws/infrastructure/terraform/devops/2020/07/15/Terraform-Part-1.html",
            "relUrl": "/aws/infrastructure/terraform/devops/2020/07/15/Terraform-Part-1.html",
            "date": " • Jul 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "MultiCat Image Classification",
            "content": "Get Data . from fastai2.vision.all import * import pandas as pd path = untar_data(URLs.PASCAL_2007) . train = pd.read_csv(path/&#39;train.csv&#39;) test = pd.read_csv(path/&#39;test.csv&#39;) . dls = ImageDataLoaders.from_df(train, path, folder=&#39;train&#39;, valid_col=&#39;is_valid&#39;, label_delim=&#39; &#39;, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224)) dls.show_batch() . /home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . learn = cnn_learner(dls,arch = resnet34,metrics = accuracy_multi) learn.fine_tune(3) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.910845 | 0.607924 | 0.686952 | 00:33 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.672910 | 0.484768 | 0.798585 | 00:41 | . 1 | 0.539291 | 0.309211 | 0.930120 | 00:41 | . 2 | 0.432108 | 0.270396 | 0.947709 | 00:41 | .",
            "url": "https://isaac-flath.github.io/blog/neural%20networks/image%20classification/2020/07/05/Milti-Cat-Image-Classifier.html",
            "relUrl": "/neural%20networks/image%20classification/2020/07/05/Milti-Cat-Image-Classifier.html",
            "date": " • Jul 5, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fine Grain Image Classification",
            "content": "Intro . In this blog we are going to do an image classification to take dog pictures and predict the breed. This is considered &#39;fine grain&#39; because the difference between classes is fairly minimal. Classifying between breeds of dogs is fine grain, classifying whether something is a dog or a cup is not. . To do this we are going to use fastaiv2, which is the new version of fastai that will come out in July. The purpose of this post is to introduce a few key concepts that will be useful as we move into harder problems . Setup . Library Import and Dataset Download . from fastai2.vision.all import * seed = 42 # Download and get path for dataseet path = untar_data(URLs.PETS) #Sample dataset from fastai2 path.ls() . (#2) [Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/annotations&#39;)] . img = (path/&#39;images&#39;).ls()[0] img . Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/British_Shorthair_154.jpg&#39;) . Data Setup . Data Blocks and data loaders are convenient ways that fastai helps us manage an load data. There is a lot going on in DataBlock so I am going to break it down piece by piece. . DataBlock . Blocks: What is our data? x is our images (ImageBlock) and y is our categories (CategoryBlock). In this case each image will get a dog breed as the category. . | get_items: How do we get our data (x)? We use the predefined get_image_files for this, though we can give it something custom if needed. . | splitter: How should we create the validation set? The splitter splits our data into test and validation sets. The default 20% validation set is just fine, but we define a seed so it is reproducable. . | get_y: How do we get our dependent variable (y)? In this care we are going to get it from the file name. With using_attr, we can apply a function to an attribute of the file (name). So here we are using regex on the file name to get y. . | item_tfms: What transformations do we need to do before packing things up to be sent to the GPU? In this case resizing it to 460. . | batch_tfms: What transformations do we want to do in batches on the GPU? There are many default transforms, and we are specifying a few ourselves (min_scale and size). . | . pets = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, splitter= RandomSplitter(valid_pct = 0.2, seed=seed), get_y= using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;),&#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(min_scale = 0.9,size=224) ) . dataloader . The dataloader is what we will actually interact with. In the DataBlock we defined lots of things we need to do to get and transform images, but not where to get them from. We define that in the dataloade . dls = pets.dataloaders(path/&quot;images&quot;) . /home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . dls.show_batch() . Training the Model . Get a basic model working . In 2 lines of code I am going to create and train a basic model. There&#39;s a couple things to note: . I am using the dls from the previous step. This is where we defined how to load the data, how to label it, data augmentation, training/validation split, etc. | I can also pass standard architectures. &quot;Resnet&quot; is a common architecture for image classification. &quot;34&quot; signifies that it has 34 layers. If you wish to understand what a layer is, please check out the Image Classifier Basics blog posts that build a simple 1 layer net. | I set a metric, but use the default loss metric. . Note: Loss is what the model uses to train on. Error rate is just for our reference. Accuracy and error rates make very poor choices for loss function because they have either 0 or infinite slope, so calculating the gradient/path value/derivative is not meaningful. This is a prime example of why good functions for computers to understand what&#8217;s going on and good functions for peope to understand what&#8217;s going on can be very different things. | learn = cnn_learner(dls, resnet34, metrics=error_rate) . Next we are going to fine tune the model. . If we were starting from scratch when training a model we will train every layer. Fine tuning is about training the final layer(s) and leaving the rest intact. Previous layers were set using weights via transfer learning. What this means is that a model was trained to be able to detect and classify a bunch of different objects. The earlier layers of the neural network detect things that are common to lots of images (ie circles, edges, corners, etc.). These don&#39;t need to change much generally. The last layer is predicting the specific thing, in our case pet breeds. This is what we need to change. . Note: A fun thought expirament on understanding why transfer learning works is to think about elements that you need to identify basically everything that you take for granted, and try to imagininng the world and objects around you if you were missing some basic concepts. For example, what if you did not have the ability to tell the diference vs a surface and and edge? Or what if you couldn&#8217;t tell the difference between something that is straight and curved? Or what if circular shaped, square shaped objects, and traingular shaped objects all looked the same to you? What if you could not recognize any pattern - what would you think of a tile floor if you had no ability to comprehend that the tiles are a pattern? How could you function if you couldn&#8217;t see corners? . learn.fine_tune(3) . epoch train_loss valid_loss error_rate time . 0 | 1.519997 | 0.311546 | 0.106901 | 01:05 | . epoch train_loss valid_loss error_rate time . 0 | 0.472248 | 0.341488 | 0.105548 | 01:23 | . 1 | 0.374747 | 0.241689 | 0.076455 | 01:22 | . 2 | 0.230108 | 0.202105 | 0.066306 | 01:22 | . learn.recorder.plot_loss() . We see out validation loss improves significantly with our error rate. We will use this error rate at our baseline. . Note: A common misconception is when training loss is lower than validation loss. This is not the case. You cannot be overfitting as unless your validation scores get worse. In a well tuned model, training loss will almost always be lower than validation loss. Let&#39;s take a look at our model, then see if we can improve it! . Look at results . First lets look at some pictures. I think it&#39;s always good to actually loook at some prediciton the model is making. . learn.show_results() . /home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . Many times in classification we look at a confusion matrix. As you can see, when we start having a lot of classes, it is really hard to make anything meaningful out of the confusion matrix. There&#39;s just too many classes. . Next, we will take a look at some more specif data. Let&#39;s start with a high level confusion matrix. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . Instead, we look at top losses to see where our model was most wrong. We also look at the &#39;most confused&#39; function whichprints which classes it gets confused on most often. . interp.plot_top_losses(9, figsize=(15,10)) . interp.most_confused(min_val=3) . [(&#39;staffordshire_bull_terrier&#39;, &#39;american_pit_bull_terrier&#39;, 5), (&#39;British_Shorthair&#39;, &#39;Russian_Blue&#39;, 4), (&#39;Ragdoll&#39;, &#39;Birman&#39;, 4), (&#39;beagle&#39;, &#39;basset_hound&#39;, 4), (&#39;Birman&#39;, &#39;Ragdoll&#39;, 3), (&#39;Egyptian_Mau&#39;, &#39;Bengal&#39;, 3), (&#39;american_pit_bull_terrier&#39;, &#39;american_bulldog&#39;, 3), (&#39;havanese&#39;, &#39;yorkshire_terrier&#39;, 3)] . Make a Better Model . Now that we have a baseline using the defaults, let&#39;s see what we can do to improve it. We will talk about a few main topics. . Freezing and Unfreezing for training | Learning Rate Finder | Discriminate learning rrate | Architecture | . Learning Rate Finder . The Learning Rate Finder is very important because setting a good learning rate can make or break a model. We will use it multiple times, and it will come up in essentially every deep learning project. It is good to spend some time to understand what it is showing and expirament. . The learning rate finder (lr_find) gives us 3 things: . lr_min: This is the learning rate that gives us the minimum loss in our graph. 1 common rule of thumb is to divide this by 10 and use that as your learning rate. | lr_steep: This is the steepest point on our graph. Another rule of thumb is to make this your learning rate. Interestingly enough, these 2 rules of thumb often end up with very similar results. | graph: The graph is really what i use when determining a learning rate. At the beginning of the graph we see very little reduction in loss. At the end of the graph we see loss spiking. Obviously nether of those are good. In reality we want our learning rate to be somewhere in the middle-ish of that steep decline. This is in line with our 2 rule of thumbs. | . learn = cnn_learner(dls, resnet34, metrics=error_rate) lr_min,lr_steep = learn.lr_find() . print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . Minimum/10: 1.00e-02, steepest point: 3.63e-03 . Now that we have our graph, let&#39;s train our model with this learning rate. . What&#39;s the difference between fine tune and fit one cycle? . learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.108581 | 0.351311 | 0.108931 | 01:03 | . 1 | 0.510538 | 0.250211 | 0.078484 | 01:04 | . 2 | 0.329532 | 0.207013 | 0.063599 | 01:04 | . /home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . Unfreezing . Unfreezing a model is about training the full model. We spoke earlier about fine_tune only training that last layers. This is a great start and we want to train the last layers more than the early layers, but we still want to train the early layers. Unfreeze allows us to do this. Now that we unfroze the model and are going to train the model more, we will need to do our learning rate finder to pick a good learning rate again. . learn.unfreeze() learn.lr_find() . SuggestedLRs(lr_min=1.584893179824576e-05, lr_steep=5.754399353463668e-06) . Discriminative Learning Rates . Discriminative Learning Rates means that we are going to use a different learning rate for each layer. We have previously really been training the final layers of the model. We are now ready to udate all the weights and biases, including the ones that were set by our transfer learning. While we do want to train the whole model, we don&#39;t want to train it at the same speed. We want more changes in the end (ie figuring out exactly which breed it is) and less changes in the early layers (ie identifying lines). This isn&#39;t so different than how people work. Learning a new thing (ie a type of dog breed) is much easier than improving my fundamental understanding of the world around me. . We will fit this now for 6 epochs. The first layers will have 1e-6 learning rate. The final layers will have 1e-4. Middle layers will be between those 2 numbers. We can see we get down to just under a 6% error rate. 94% accuracy - not bad! . learn.fit_one_cycle(6, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss error_rate time . 0 | 0.260102 | 0.199174 | 0.064953 | 01:22 | . 1 | 0.243101 | 0.191330 | 0.062246 | 01:23 | . 2 | 0.211765 | 0.187266 | 0.059540 | 01:22 | . 3 | 0.188451 | 0.184359 | 0.063599 | 01:22 | . 4 | 0.181025 | 0.180899 | 0.060217 | 01:22 | . 5 | 0.175231 | 0.181373 | 0.059540 | 01:22 | . /home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . learn.recorder.plot_loss() . Architecture . Another simple lever is to increase the number of layers in the neural network. The more layers, the higher capacity to learn from data. This also means the higher capacity that you overfit, so more layers does not always mean better! . Note: Overfitting is defined as continued training increases your validation loss. Many people feel that overfitting is when your training loss is less than validation loss, but in reality almost every well tuned model will have a lower training loss than validation loss. If the prediction accuracy on things the model haven&#8217;t seen keeps getting better, how can you be overfitting? Lets see what the impact can be from using a different architecture. A few comments: . We were using resnet34 before, and now we are using resnet101. the number represents the number of layers | We added a to_fp16. We are actually decreasing the precision of our calculations a bit for 2 reasons Faster to train | Helps combat overfitting | . | We are using fine_tune with freeze_epochs. All we are doing is training with the earlier layers frozen for 3 epochs, then training unfreezing, then training for 6 epochs. Take a look through the code of the fine_tune method at the end and you will see that I am not summarizing, that&#39;s just exactly what it is doing. (self.freeze -&gt; self.fit_one_cycle -&gt; divide learning rate -&gt; self.unfreeze -&gt; self.fit_one_cycle). | . With the resnet101 we are down to less that 5% error rate. Even better! . learn = cnn_learner(dls, resnet101, metrics=error_rate).to_fp16() learn.fine_tune(6, freeze_epochs=3) . epoch train_loss valid_loss error_rate time . 0 | 1.371693 | 0.247931 | 0.079161 | 02:43 | . 1 | 0.551731 | 0.246441 | 0.077131 | 02:39 | . 2 | 0.371089 | 0.233275 | 0.080514 | 02:39 | . /home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . epoch train_loss valid_loss error_rate time . 0 | 0.258692 | 0.246271 | 0.081191 | 03:36 | . 1 | 0.310040 | 0.321208 | 0.091340 | 03:33 | . 2 | 0.266156 | 0.289912 | 0.081867 | 03:34 | . 3 | 0.149828 | 0.213094 | 0.060893 | 03:34 | . 4 | 0.082449 | 0.183762 | 0.056834 | 03:35 | . 5 | 0.048759 | 0.173572 | 0.049391 | 03:34 | . learn.recorder.plot_loss() . ??learn.fine_tune . Signature: learn.fine_tune( epochs, base_lr=0.002, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, lr_max=None, div_final=100000.0, wd=None, moms=None, cbs=None, reset_opt=False, ) Source: @patch @log_args(but_as=Learner.fit) @delegates(Learner.fit_one_cycle) def fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, **kwargs): &#34;Fine tune with `freeze` for `freeze_epochs` then with `unfreeze` from `epochs` using discriminative LR&#34; self.freeze() self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs) base_lr /= 2 self.unfreeze() self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs) File: ~/anaconda3/lib/python3.7/site-packages/fastai2/callback/schedule.py Type: method .",
            "url": "https://isaac-flath.github.io/blog/neural%20networks/image%20classification/2020/06/27/Fine-Grain-Image-Classifier.html",
            "relUrl": "/neural%20networks/image%20classification/2020/06/27/Fine-Grain-Image-Classifier.html",
            "date": " • Jun 27, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Connecting Jupyter to EC2",
            "content": "Intro . The goal of this is to run a jupyter notebook as if it were locally but have all of it running in the backend. I have tried sagemaker and other packaged products, and I prefer just using Jupyter-lab. This guide is how I connect. . This is mostly a reference guide to refer back to until you memorize the steps. If this is your first time using AWS, EC2, or you aren&#39;t already fairly familiar with the process then I recommend checking out a guide that explains things a bit more. This is meant to be be a good place to use as a reminder of what to do while you do it the first 10 or 20 times, rather than a step-by-step guide on how to do it. . A great place (and what I used initially) for getting instructions to do this for the first time would be: https://fzr72725.github.io/2018/01/14/How-to-Connect-to-Jupyter-Notebook-Server-on-AWS-EC2-from-Your-Local-Machine.html . Launch an EC2 Instance . First, launch an EC2 instance. I use p2.xlarge for almost everything. You will be prompted to create or use an existing key pair. You will need this private key, so download it and store it somewhere. If you lose it, you lose the EC2 instance. I have a copy of mine in S3 . Connect to your EC2 Instance . In the AWS console, you can click connect. If your EC2 instance isn&#39;t started then start it. It will give you instructions for SSHing in. There&#39;s 2 steps that are important here. . chmod 400 &lt;key.pem&gt; . and . ssh -i &quot;key.pem&quot; username@whatever_it_tells_you_in_aws_console.compute-1.amazonaws.com . Launch Jupyter . In the EC2 I just ssh into I run . jupyter-lab --no-browser --port=8889 . I stored my key in the directory ~/.aws, but that should be replaced with wherever. I open a new console and run . ssh -i ~/.aws/key.pem -L 8000:localhost:8889 username@whatever_it_tells_you_in_aws_console.compute-1.amazonaws.com . From there open localhost:8000 in your browser and you are good to go. If it asks for a token you can find it in the terminal output where you ran jupyter-lab! .",
            "url": "https://isaac-flath.github.io/blog/aws/2020/06/25/JupyterEC2.html",
            "relUrl": "/aws/2020/06/25/JupyterEC2.html",
            "date": " • Jun 25, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Neural Network Basics (Part 2)",
            "content": "from fastai2.vision.all import * from fastai2.data.external import * from PIL import Image import math . Intro . Today we will be working with the MNIST dataset. The goal is going to be to take an image of handwritten digits and automatically predict what number it is. We will be building a Neural Network to do this. This is building off of the Image Classifier Basics post where we classified into 3s and 7s. If anything in this post is confusing, I reccomend heading over to part 1. I am assuming that the content covered in Part 1 is understood. . If you get through this and want more detail, I highly recommend checking out Deep Learning for Coders with fastai &amp; Pytorch by Jeremy Howard and Sylvain Gugger. All of the material in this guide and more is covered in much greater detail in that book. . https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527 . Load the Data . Naturally, the first step is to get and load the data. We&#39;ll look at it a bit along tohe way to make sure it was loaded properly as well. We will be using fastai&#39;s built in dataset feature rather than sourcing it ourself. I am skimming over this quickly as this was covered in part 1. . # This command downloads the MNIST_TINY dataset and returns the path where it was downloaded path = untar_data(URLs.MNIST) # This takes that path from above, and get the path for training and validation training = [x.ls() for x in (path/&#39;training&#39;).ls().sorted()] validation = [x.ls() for x in (path/&#39;testing&#39;).ls().sorted()] . Let&#39;s take a look at an image. The first thing I reccomend doing for any dataset is to view something to verify you loaded it right. The second thing is to look at the size of it. This is not just for memory concerns, but you want to generally know some basics about whatever you are working with. . # Let&#39;s view what one of the images looks like im3 = Image.open(training[6][1]) im3 . # Let&#39;s see what shape the underlying matrix is that represents the picture tensor(im3).shape . torch.Size([28, 28]) . Linear Equation . We are looking to do wx + b = y. It seems that a Neural network should use some super fancy equation in it&#39;s layers, but that&#39;s all it is. In a single class classifier, y has 1 column as it is predicting 1 thing. In a multiclass classifier y has &quot;however-many-classes-you-have&quot; columns. . Tensor Setup . The first thing I will do is get my xs and ys in tensors in the right format. . training_t = list() for x in range(0,len(training)): # For each class, stack them together. Divide by 255 so all numbers are between 0 and 1 training_t.append(torch.stack([tensor(Image.open(i)) for i in training[x]]).float()/255) validation_t = list() for x in range(0,len(validation)): # For each class, stack them together. Divide by 255 so all numbers are between 0 and 1 validation_t.append(torch.stack([tensor(Image.open(i)) for i in validation[x]]).float()/255) . # Let&#39;s make sure images are the same size as before (ie we didn&#39;t screw anything up) training_t[1][1].shape . torch.Size([28, 28]) . Let&#39;s do a simple average of one of our images. It&#39;s a nice sanity check to see that we did things ok. We can see that after averaging, we get a recognizable number. That&#39;s a good sign. . show_image(training_t[5].mean(0)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fbf7ea98290&gt; . # combine all our different images into 1 matrix. Convert Rank 3 tensor to rank 2 tensor. x = torch.cat([x for x in training_t]).view(-1, 28*28) valid_x = torch.cat([x for x in validation_t]).view(-1, 28*28) # Defining Y. I am starting with a tensor of all 0. # This tensor has 1 row per image, and 1 column per class y = tensor([[0]*len(training_t)]*len(x)) valid_y = tensor([[0]*len(validation_t)]*len(valid_x)) # Column 0 = 1 when the digit is a 0, 0 when the digit is not a 0 # Column 1 = 1 when the digit is a 1, 0 when the digit is not a 1 # Column 2 = 1 when the digit is a 2, 0 when the digit is not a 2 # etc. j=0 for colnum in range(0,len(training_t)): y[j:j+len(training_t[colnum]):,colnum] = 1 j = j + len(training[colnum]) j=0 for colnum in range(0,len(validation_t)): valid_y[j:j+len(validation_t[colnum]):,colnum] = 1 j = j + len(validation[colnum]) # Combine by xs and ys into 1 dataset for convenience. dset = list(zip(x,y)) valid_dset = list(zip(valid_x,valid_y)) # Inspect the shape of our tensors x.shape,y.shape,valid_x.shape,valid_y.shape . (torch.Size([60000, 784]), torch.Size([60000, 10]), torch.Size([10000, 784]), torch.Size([10000, 10])) . Perfect. We have exactly what we need and defined above. 60,000 images x 784 pixels for my x. A 60,000 images x 10 classes for the predictions. . 10,000 images make up the validation set. . Calculate wx + b . Let&#39;s initialize our weights and biases, then do the matrix multiplication and make sure the output is the expected shape (60,000 images x 10 classes). . # Here is how we will initialize paramaters. This is just giving me random numbers. def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() # Initializze w and b weight tensors w = init_params((28*28,10)) b = init_params(10) . # Lets do our linear equation and see what shape we get. (x@w+b).shape,(valid_x@w+b).shape . (torch.Size([60000, 10]), torch.Size([10000, 10])) . Great, we have the right number of predictions. Obviously the predictions are no good. There a couple things left to do. The first thing we need to do is turn our Linear Equation into a Neural Network. To do that we need to do this twice, with a ReLu inbetween. . Neural Network . . Note: You can check out the first Image Classifier blog post, which explains does this in a simpler problem (single class classifier) and assumes less pre-requisite knowledge. I am assuming that the information in Part 1 is understood. If you understand Part 1, you are ready for Part2! . # Here&#39;s a simple Neural Network. # This can have more layers by duplicating the patten seen below, this is just the fewest layers for demonstration. def simple_net(xb): # Linear Equation from above res = xb@w1 + b1 #Linear # Replace any negative values with 0. This is called a ReLu. res = res.max(tensor(0.0)) #ReLu # Do another Linear Equation res = res@w2 + b2 #Linear # return the predictions return res . # initialize random weights. # The number 30 here can be adjusted for more or less model complexity. multipliers = 30 w1 = init_params((28*28,multipliers)) b1 = init_params(multipliers) w2 = init_params((multipliers,10)) b2 = init_params(10) . simple_net(x).shape # 60,000 images with 10 predictions per class (one per digit) . torch.Size([60000, 10]) . Improving Weights and Biases . We have predictions with random weights and biases. What we need to do is to get these weights and biases to be the right numbers rather than random numbers. To do this we need to use Gradient Descent to improve the weights. Here&#39;s roughly what we need to do: . Create a loss function to measure how close (or far) off we are | Calculate the gradient (slope) so we know which direction to step | Adjust our values in that direction | Repeat many times | . The first thing we need to use gradient descent is we need a loss function. Let&#39;s use something simple, how far off we were. If the correct answer was 1, and we predicted a 0.5 that would be a loss of 0.5. We will do this for every class . The one addition is that we will add something called a sigmoid. All a sigmoid is doing is ensuring that all of our predictions land between 0 and 1. We never want to predict anything outside of these ranges. . Note: If you want more of a background on what is going on here, please take a look at my series on Gradient Descent where I dive deeper on this. We will be calculating a gradient - which are equivilant to the &quot;Path Value&quot; . Loss Function . def mnist_loss(predictions, targets): # make all prediction between 0 and 1 predictions = predictions.sigmoid() # Difference between predictions and target return torch.where(targets==1, 1-predictions, predictions).mean() . # Calculate loss on training and validation sets to make sure the function works mnist_loss(simple_net(x),y),mnist_loss(simple_net(valid_x),valid_y) . (tensor(0.5644, grad_fn=&lt;MeanBackward0&gt;), tensor(0.5678, grad_fn=&lt;MeanBackward0&gt;)) . Calculate Gradient . Now we have a function we need to optimize and a loss function to tell us our error. We are ready for gradient descent. Let&#39;s create a function to change our weights. . First, we will make sure our datasets are in a DataLoader. This is convenience class that helps manage our data and get batches. . . Note: This is the same from part 1 . # Batch size of 256 - feel free to change that based on your memory dl = DataLoader(dset, batch_size=1000, shuffle=True) valid_dl = DataLoader(valid_dset, batch_size=1000) # Example for how to get the first batch xb,yb = first(dl) valid_xb,valid_yb = first(valid_dl) . def calc_grad(xb, yb, model): # calculate predictions preds = model(xb) # calculate loss loss = mnist_loss(preds, yb) # Adjust weights based on gradients loss.backward() . Train the Model . . Note: This is the same from part 1 . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . Measure Accuracy on Batch . def batch_accuracy(xb, yb): # this is checking for each row, which column has the highest score. # p_inds, y_inds gives the index highest score, which is our prediction. p_out, p_inds = torch.max(preds,dim=1) y_out, y_inds = torch.max(yb,dim=1) # Compre predictions with actual correct = p_inds == y_inds # average how often we are right (accuracy) return correct.float().mean() . Measure Accuracy on All . . Note: This is the same from part 1 . def validate_epoch(model): # Calculate accuracy on the entire validation set accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] # Combine accuracy from each batch and round return round(torch.stack(accs).mean().item(), 4) . Initialize weights and biases . # When classifying 3 vs 7 in part one, we just used 30 weights. # With this problem being much harder, I will give it more weights to work with complexity = 500 w1 = init_params((28*28,complexity)) b1 = init_params(complexity) w2 = init_params((complexity,10)) b2 = init_params(10) params = w1,b1,w2,b2 . Train the Model . Below we will actually train our model. . lr = 50 # epoch means # of passes through our data (60,000 images) epochs = 30 loss_old = 9999999 for i in range(iterations): train_epoch(simple_net, lr, params) # Print Accuracy metric every 10 iterations if (i % 10 == 0) or (i == iterations - 1): print(&#39;Accuracy:&#39;+ str(round(validate_epoch(simple_net)*100,2))+&#39;%&#39;) loss_new = mnist_loss(simple_net(x),y) # Print Loss score every iteration print(&#39;Loss: &#39; + str(round(mnist_loss(simple_net(x),y).item(),5)),&quot;Loss decreased&quot; if loss_new &lt; loss_old else &quot;Loss increased&quot;) loss_old = loss_new . Accuracy:10.7% Loss: 0.10003 Loss decreased Loss: 0.10002 Loss decreased Loss: 0.09893 Loss decreased Loss: 0.09408 Loss decreased Loss: 0.09275 Loss decreased Loss: 0.09227 Loss decreased Loss: 0.0924 Loss increased Loss: 0.09199 Loss decreased Loss: 0.09156 Loss decreased Loss: 0.09231 Loss increased Accuracy:19.6% Loss: 0.09141 Loss decreased Loss: 0.09151 Loss increased Loss: 0.09128 Loss decreased Loss: 0.09118 Loss decreased Loss: 0.09135 Loss increased Loss: 0.09108 Loss decreased Loss: 0.09042 Loss decreased Loss: 0.08482 Loss decreased Loss: 0.08329 Loss decreased Loss: 0.08295 Loss decreased Accuracy:28.32% Loss: 0.08366 Loss increased Loss: 0.08272 Loss decreased Loss: 0.08259 Loss decreased Loss: 0.08258 Loss decreased Loss: 0.08245 Loss decreased Loss: 0.08249 Loss increased Loss: 0.08239 Loss decreased Loss: 0.08215 Loss decreased Loss: 0.07953 Loss decreased Accuracy:36.19% Loss: 0.07496 Loss decreased . Results . A few key points: . The Loss is not the same as the metric (Accuracy). Loss is what the models use, Accuracy is more meaningful to us humans. | We see that our loss slowly decreases each epoch. Our accuracy is getting better over time as well. | . We start at about 10% accuracy, which makes sense. With random weights we predict correctly 1/10 times. With 10 digits that sounds like a random guess. Over time, we slowly decrease the loss and after 30 epochs we are around 36% accuracy. Much better! We could keep training more to keep improving accuracy, but I think we see the idea. . This Model vs SOTA . What is different about this model than a best practice model? . This model is only 1 layer. State of the art for image recognitions will use more layers. Resnet 34 and Resnet 50 are common (34 and 50 layers). This would just mean we would alternate between the ReLu and linear layers and duplicate what we are doing with more weights and biases. | More weights and Biases. The Weights and Biases I used are fairly small - I ran this extremely quickly on a CPU. With the appropriate size weight and biases tensors, it would make way more sense to use a GPU. | Matrix Multiplication is replaced with Convolutions for image recognition. A Convolution can be thought of as matrix multiplication if you averaged some of the pixels together. This intuitively makes sense as 1 pixel in itself is meaningless without the context of other pixels. So we tie them together some. | Dropout would make our model less likely to overfit and less dependent on specific pixels. It would do this by randomly ignoring different pixels so it cannot rely on them. It&#39;s very similar to how decision trees randomly ignore variables for their splits. | Discriminate learning rates means that the learning rates are not the same for all levels of the neural network. With only 1 layer, naturally we don&#39;t worry about this. | Gradient Descent - we can adjust our learning rate based on our loss to speed up the process | Transfer learning - we can optimize our weights on a similar task so when we start trying to optimize weights on digits we aren&#39;t starting from random variables. | Keep training for as many epochs as we see our validation loss decrease | . As you can see, these are not completely different models. These are small tweaks to what we have done above that make improvements - the combination of these small tweaks and a few other tricks are what elevate these models. There are many &#39;advanced&#39; variations of Neural Networks, but the concepts are typically along the lines of above. If you boil them down to what they are really doing without all the jargon - they are pretty simple concepts. I&#39;ll cover them in future blog posts. .",
            "url": "https://isaac-flath.github.io/blog/neural%20networks/image%20classification/2020/06/21/Image-Classifier-Basics-MultiClass.html",
            "relUrl": "/neural%20networks/image%20classification/2020/06/21/Image-Classifier-Basics-MultiClass.html",
            "date": " • Jun 21, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Neural Network Basics (Part 1)",
            "content": "from fastai2.vision.all import * from fastai2.data.external import * from PIL import Image import math torch.manual_seed(100) . &lt;torch._C.Generator at 0x7f9fd6fb4d90&gt; . Intro . Today we will be working with a subset of the MNIST dataset. The goal is going to be to take an image of handwritten digits and automatically predict whether it is a 3 or a 7. We will be building a Neural Network to do this. . If you get through this and want more detail, I highly recommend checking out Deep Learning for Coders with fastai &amp; Pytorch by Jeremy Howard and Sylvain Gugger. All of the material in this guide and more is covered in much greater detail in that book. They have some awesome courses on their fast.ai website as well. . https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527 . Load the Data . Naturally, the first step is to get and load the data. We&#39;ll look at it a bit along tohe way to make sure it was loaded properly and we understand it. We will be using fastai&#39;s built in dataset feature rather than sourcing it ourself. . # This command downloads the MNIST_TINY dataset and returns the path where it was downloaded path = untar_data(URLs.MNIST_TINY) # This takes that path from above, and get the path for the threes and the sevens threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() . Let&#39;s take a look at an image. The first thing I reccomend doing for any dataset is to view something to verify you loaded it right. The second thing is to look at the size of it. This is not just for memory concerns, but you want to generally know some basics about whatever you are working with. . # Let&#39;s view what one of the images looks like im3 = Image.open(threes[1]) im3 . # Let&#39;s see what shape the underlying matrix is that represents the picture tensor(im3).shape . torch.Size([28, 28]) . What I am going to do below is put the tensor into a dataframe, and color the pixels based on the value in each place. We can clearly see that this is a 3 just from the values in the tensor. This should give a good idea for the data we are working with and how an image can be worked with. . pd.DataFrame(tensor(im3)).loc[3:24,6:20].style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 77 | 181 | 254 | 255 | 95 | 88 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 3 | 97 | 242 | 253 | 253 | 253 | 253 | 251 | 117 | 15 | 0 | 0 | 0 | 0 | . 6 0 | 20 | 198 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 239 | 59 | 0 | 0 | 0 | . 7 0 | 0 | 108 | 248 | 253 | 244 | 220 | 231 | 253 | 253 | 253 | 138 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 110 | 129 | 176 | 0 | 83 | 253 | 253 | 253 | 194 | 24 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 26 | 0 | 83 | 253 | 253 | 253 | 253 | 48 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 83 | 253 | 253 | 253 | 189 | 22 | 0 | 0 | . 11 0 | 0 | 0 | 0 | 0 | 0 | 0 | 83 | 253 | 253 | 253 | 138 | 0 | 0 | 0 | . 12 0 | 0 | 0 | 0 | 0 | 0 | 0 | 183 | 253 | 253 | 253 | 138 | 0 | 0 | 0 | . 13 0 | 0 | 0 | 0 | 0 | 65 | 246 | 253 | 253 | 253 | 175 | 4 | 0 | 0 | 0 | . 14 0 | 0 | 0 | 0 | 0 | 172 | 253 | 253 | 253 | 253 | 70 | 0 | 0 | 0 | 0 | . 15 0 | 0 | 0 | 0 | 0 | 66 | 253 | 253 | 253 | 253 | 238 | 54 | 0 | 0 | 0 | . 16 0 | 0 | 0 | 0 | 0 | 17 | 65 | 232 | 253 | 253 | 253 | 149 | 5 | 0 | 0 | . 17 0 | 0 | 0 | 0 | 0 | 0 | 0 | 45 | 141 | 253 | 253 | 253 | 123 | 0 | 0 | . 18 0 | 0 | 0 | 41 | 205 | 205 | 205 | 33 | 2 | 128 | 253 | 253 | 245 | 99 | 0 | . 19 0 | 0 | 0 | 50 | 253 | 253 | 253 | 213 | 131 | 179 | 253 | 253 | 231 | 59 | 0 | . 20 0 | 0 | 0 | 50 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 212 | 0 | 0 | . 21 0 | 0 | 0 | 21 | 187 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 212 | 0 | 0 | . 22 0 | 0 | 0 | 0 | 9 | 58 | 179 | 251 | 253 | 253 | 253 | 219 | 44 | 0 | 0 | . 23 0 | 0 | 0 | 0 | 0 | 0 | 0 | 139 | 253 | 253 | 130 | 49 | 0 | 0 | 0 | . 24 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Defining our Linear Equation . We are looking to do xw + b = y. It seems that a Neural network should use some super fancy equation in it&#39;s layers, but that&#39;s all it is. Basically the same equation everyone learns that defines a line (mx+b) . Setup . We will need a wieght matrix with 1 weight per pixel, meaning this will be a 784 row by 1 column matrix. In order to multiply this by our 784 pixels we need to frormat that in 1 row x 784 column matrix. Then we can do matrix multiplication. We are also going to add b, so let&#39;s initialize that as well. Since we haven&#39;t solved the problem yet, we don&#39;t know what good values for w and b are so we will make them random to start with. . Note: When we checked the shape above, we saw our images were 28 x 28 pixels, which is 784 total pixels. . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() w = init_params((28*28,1)) b = init_params(1) . Now we just need x and y. A 784x1 matrix times a 1x784 matrix. We want all values to be between 0 and 1, so we divide by the max pixel value (255) . # open each image and convert them to a tensor threes_t = [tensor(Image.open(i)) for i in threes] sevens_t = [tensor(Image.open(i)) for i in sevens] # Get my list of tensors and &quot;stack&quot; them. Also dividing by 255 so all values are between 0 and 1 threes_s = torch.stack(threes_t).float()/255 sevens_s = torch.stack(sevens_t).float()/255 #Verify max and min pixel values torch.min(threes_s), torch.max(threes_s), torch.min(sevens_s), torch.max(sevens_s) . (tensor(0.), tensor(1.), tensor(0.), tensor(1.)) . Let&#39;s do a simple average of all our threes together and see what we get. It&#39;s a nice sanity check to see that we did things ok. We can see that after averaging, we pretty much get a three! . show_image(threes_s.mean(0)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9fdda2a410&gt; . Perfect, lets finish definining our x. We want x to have both threes and sevens, but right now they are seperate. We will use torch.cat to combine them, and .view to change the format of the matrix to the right format. Y is being defined as a long matrix with 1 row per image (prediction) and 1 column. . # combine our threes and sevens into 1 matrix. Convert Rank 3 matrix to rank 2. x = torch.cat([threes_s, sevens_s]).view(-1, 28*28) # Set my y, or dependent variable matrix. A three will be 1, and seven will be 0. So we will be prediction 0 or 1. y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) # Combine by xs and ys into 1 dataset for convenience. dset = list(zip(x,y)) x_0,y_0 = dset[0] x_0.shape,y_0 . (torch.Size([784]), tensor([1])) . Perfect. We have exactly what we need and defined above. A 784 x 1 matrix times a 1 x 784 matrix + a constanct = our prediction. Let&#39;s take a look to verify things are the right shape, and if we actually multiply these things together we get 1 prediction per image. . w.shape,x_0.shape,b.shape,y_0.shape . (torch.Size([784, 1]), torch.Size([784]), torch.Size([1]), torch.Size([1])) . print((x@w+b).shape) (x@w+b)[1:10] . torch.Size([709, 1]) . tensor([[ 3.3164], [ 5.2035], [-3.7491], [ 1.2665], [ 2.2916], [ 1.3741], [-7.6092], [ 1.3464], [ 2.7644]], grad_fn=&lt;SliceBackward&gt;) . Great, we have the right number of predictions. Obviosly the predictions are no good at predictions 0 or 1. After all, our weights and biases are all random. Let&#39;s do something about that. . We will need to do all this on our validation set as well, so let&#39;s do that here. . #collapse-hide valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . . Loss Function . We need to improve our weights and biases (w and b) and we do that using gradient descent. I have a few posts on gradient descent, feel free to check those out if you want details on how it works. Here we will use the built-in pytorch functionality. . The first thing we need to use gradient descent is we need a loss function. Let&#39;s use something simple, how far off we were. If the correct answer was 1, and we predicted a 0.5 that would be a loss of 0.5. . The one addition is that we will add something called a sigmoid. All a sigmoid is doing is ensuring that all of our predictions land between 0 and 1. We never want to predict anything outside of these ranges as those are our 2 categories. . def mnist_loss(predictions, targets): # make all prediction between 0 and 1 predictions = predictions.sigmoid() # Difference between predictions and target return torch.where(targets==1, 1-predictions, predictions).mean() . Gradient Descent . Background and Setup . predict | calculate loss | calculate gradient | subtract from weights and bias | . Ok, now we have a function we need to optimize and a loss function to tell us our error. We are ready for gradient descent. Let&#39;s create a function to change our weights. . Note: If you want more of a background on what is going on here, please take a look at my series on Gradient Descent where I dive deeper on this. We will be calculating a gradient - which are equivilant to the &quot;Path Value&quot; . # Here is the function to minimize def linear1(xb): return xb@weights + bias # Here is how we will initialize paramaters. This is just giving me random numbers. def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() . First, we will make sure our datasets are in a DataLoader. This is convenience class that helps manage our data and get batches.. . dl = DataLoader(dset, batch_size=256, shuffle=True) valid_dl = DataLoader(valid_dset, batch_size=256) . We are now going to get the first batch out. We&#39;ll use a batch size of 256, but feel free to change that based on your memory. You can see that we can just simply call first dl, and it creates our shuffled batch for us. . xb,yb = first(dl) xb.shape,yb.shape . (torch.Size([256, 784]), torch.Size([256, 1])) . Let&#39;s Initialize our paramaters we will need. . weights = init_params((28*28,1)) bias = init_params(1) . Calculate the Gradient . We now have our batch of x and y, and we have our weights and biases. The next step is to make a prediction. Since our batch size is 256, we see 256x1 tensor. . preds = linear1(xb) preds.shape, preds[:5] . (torch.Size([256, 1]), tensor([[-17.7688], [ -3.9593], [ -4.0014], [ 1.4874], [ 0.5148]], grad_fn=&lt;SliceBackward&gt;)) . Now we calculate our loss to see how we did. Probably not well considering all our weights at this point are random. . loss = mnist_loss(preds, yb) loss . tensor(0.5678, grad_fn=&lt;MeanBackward0&gt;) . Let&#39;s calculate our Gradients . loss.backward() weights.grad.shape,weights.grad.mean(),bias.grad . (torch.Size([784, 1]), tensor(-0.0011), tensor([-0.0071])) . Since we are going to want to calculate gradients every since step, let&#39;s create a function that we can call that does these three steps above. Let&#39;s put all that in a function. From here on out, we will use this function. . Note: It&#8217;s always a good idea to periodically reviewing and trying to simplify re-usable code. I reccomend doing following the above approach, make something that works - then simplify. It often wastes a lot of time trying to write things in the most perfect way from the start. . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . calc_grad(xb, yb, linear1) weights.grad.mean(),bias.grad . (tensor(-0.0022), tensor([-0.0142])) . weights.grad.zero_() bias.grad.zero_(); . Training the Model . Now we are ready to create a function that trains for 1 epoch. . Note: Epoch is just a fancy way of saying 1 pass through all our data. . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . Naturally, we want to be able to measure accuracy. It&#39;s hard for us to gauge how well the model is doing from our loss function. We will create an accuracy metric to look at accuracy for that batch. . Note: A loss function is designed to be good for gradient descent. A Metric is designed to be good for human understanding. This is why they are different sometimes. . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() . batch_accuracy(linear1(xb), yb) . tensor(0.4180) . Looking at accuracy of our batch is great, but we also want to look at our accuracy for the validation set. This is our way to do that using the accuracy funcion we just defined. . def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . validate_epoch(linear1) . 0.4138 . Awesome! Let&#39;s throw this all in a loop and see what we get. . params = weights,bias lr = 1. for i in range(20): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.5249 0.6097 0.7038 0.7628 0.8165 0.8439 0.8685 0.8839 0.8922 0.8984 0.9071 0.9097 0.9136 0.9162 0.9176 0.9202 0.9215 0.9267 0.9311 0.9319 . Linear Recap . Lots of stuff, but let&#39;s recap really what we did: . Make a prediction | Measure how we did | Change our weights so we do slightly better next time | Print out accuracy metrics along the way so we can see how we are doing | . This is a huge start, but what we have is a linear model. Now we need to add non-linearities so that we can have a true Neural Network. . ReLu . What is it? . First, a ReLu is our non-linearity in a neural network. A neural network is just alternating linear and nonlinear layers. We defined the Linear layer above, here we will talk about the non-linear ones. So what exactly does the non-linear layer do? It&#39;s actually much simpler than people like to believe, it&#39;s just a max. . For example, I am going to apply a ReLu to a matrix. . $ begin{bmatrix}-1&amp;1 -1&amp;-1 0&amp;0 0&amp;1 1&amp;-1 1&amp;0 -1&amp;0 -1&amp;-1 1&amp;1 end{bmatrix}$ $=&gt;$ $ begin{bmatrix}0&amp;1 0&amp;0 0&amp;0 0&amp;1 1&amp;0 1&amp;0 0&amp;0 0&amp;0 1&amp;1 end{bmatrix}$ . As you will see I just took max(x,0). Another way of saying that is I replaced any negative values with 0. That&#39;s all a ReLu is. . In a Neural Network . These ReLus go between our linear layers. Here&#39;s what a simple Neural Net looks like. . # initialize random weights. # The number 30 here can be adjusted for more a less model complexity. Future posts will talk more about what that is. w1 = init_params((28*28,30)) b1 = init_params(30) w2 = init_params((30,1)) b2 = init_params(1) . # Here&#39;s a simple Neural Network. # This can have more layers by duplicating the patten seen below, this is just the simplest model. def simple_net(xb): res = xb@w1 + b1 #Linear res = res.max(tensor(0.0)) #ReLu res = res@w2 + b2 #Linear return res . Train the Full Neural Network . So we have our new model with new weights. It&#39;s more than just the linear model, so how do we use gradient descent? We now have 4 weights (w1,w2,b1,b2). . Turns our it&#39;s exactly what we already did. Let&#39;s add the new paramaters to our paramsm, and change out the linear model with the simple_net we just defined. We end up with a pretty decent accuracy! . w1 = init_params((28*28,30)) b1 = init_params(30) w2 = init_params((30,1)) b2 = init_params(1) . params = w1,b1,w2,b2 lr = 1 for i in range(20): train_epoch(simple_net, lr, params) print(validate_epoch(simple_net), end=&#39; &#39;) . 0.6711 0.8028 0.8865 0.9198 0.9245 0.9225 0.9291 0.9223 0.9402 0.9433 0.9343 0.9433 0.9392 0.9392 0.9463 0.9431 0.9476 0.9489 0.9457 0.9528 . Recap of Tensor Shapes . Understanding the shapes of these tensors and how the network works is crucial. Here&#39;s the network we built. You can see how each layer can fit into the next layer. . x.shape,w1.shape,b1.shape,w2.shape,b2.shape . (torch.Size([709, 784]), torch.Size([784, 30]), torch.Size([30]), torch.Size([30, 1]), torch.Size([1])) . . What More? . This is a Neural Network. Now we can do tweaks to enhance performance. I will talk about those in future posts, but here&#39;s a few concepts. . Instead of a Linear Layer, A ReLu, then a linear layer - Can we add more layers to have a deeper net? | What if we average some of the pixels in our image together before dong matrix multiplication (ie a convolutions)? | Can we randomly ignore pixels to prevent overfitting (ie dropout)? | . There are many &#39;advanced&#39; variations of Neural Networks, but the concepts are typically along the lines of above. If you boil them down to what they are really doing - they are pretty simple concepts. .",
            "url": "https://isaac-flath.github.io/blog/neural%20networks/image%20classification/2020/06/19/Image-Classifier-Basics.html",
            "relUrl": "/neural%20networks/image%20classification/2020/06/19/Image-Classifier-Basics.html",
            "date": " • Jun 19, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Gradient Descent for Linear Regression (Part 2)",
            "content": "Why part 2? . I have done a couple blog posts on Gradient Descent for linear regression focusing on the basic algorithm. In this post, I will be covering some more advanced gradient descent algorithms. I will post as I complete a section rather than waiting until I have every variation posted. This is partially to show some popular ones, but the more important thing to understand from this post is that all these advanced algorithms are really just minor tweaks on the basic algorithm. . Goal Recap . The goal of linear regression is to find parameter values that fit a linear function to data points.  The goodness of fit is measured by a cost function.  The lower the cost, the better the model.  Gradient Descent is a method to minimize a cost function.  Gradient descent is a widely used tool and is used frequently in tuning predictive models.  It’s important to understand the method so you can apply it to various models and are not limited to using black box models. This approach will use the sum of squares cost function to take a predicted line and slowly change the regression coefficients until the line is a line of best fit. . This post will cover the algorithms. Part 4 of this series will focus on scaling this up to larger datasets. One of the primary tools of scaling is using stochastic gradient descent, which is just a fancy way to say &quot;just use a subset of the points instead of all of them&quot;. . Background . Our goal is to define the equation $y= theta_0+ theta_1x$. This is the same thing as $y=mx+b$. For this post I will use $y=mx+b$ language with $m$ being the slope and $b$ being the y intercept. . Note: In order to adjust $m$, I will take $m$ - &lt;$m$ PathValue&gt; * &lt;adj $ alpha$&gt;. . . Note: In order to adjust $b$, I will take $b$ - &lt;$b$ PathValue&gt; * &lt;adj $ alpha$&gt;. Each of these advanced algorithms either change the Path Value, or change $ alpha$. I will show what the calculation for each is for each algorthm is, have a written explanation, and python code that illustrates it. . Setup . Here is where I load libraries, define my dataset, and create a graphing function. . #collapse-hide import matplotlib.pyplot as plt import pandas as pd import numpy as np import math from IPython.display import clear_output from time import sleep np.random.seed(44) xs = np.random.randint(-100,100,100) ys = xs * np.random.randint(-10,10) + 100 # + np.random.randint(-200,200,50) cycles = 50 def graph_gradient_descent(values,cycles, figures,step): plt.figure(figsize=(20,10)) cols = 3 rows = math.ceil(figures/3) for x in range(1,figures+1): plt.subplot(rows,cols,x) plt.scatter(values[&#39;x&#39;],values[&#39;y&#39;]) plt.scatter(values[&#39;x&#39;],values[&#39;cycle0&#39;]) plt.scatter(values[&#39;x&#39;],values[&#39;cycle&#39;+str(x*step)]) labels = [&#39;y&#39;,&#39;initial&#39;,&#39;cycle&#39;+str(x*step)] plt.legend(labels) plt.ylim(-1000,1000) plt.xlim(-100,100) . . Basic Gradient Descent . This is the basic gradient descent algorithm that all others are based on. If you are not clear on how gradient descent works, please refer to the background section for a review or Gradient Descent Part 1 Blog Post for more details. I will use this same format below for each algorithm, and change only what is necessary for easier comparison. . Inputs . $ alpha = learning rate$ . n = number of data points . New Formulas . $PathValue_m = PathValue_bx$ . $PathValue_b = y_{pred}-y_{obs}$ . Each variation after this does 1 of 3 things to modify this algorithm: . Adjusts $ alpha$ by some amount | Adjust $PathValue$ by some amount. | Adjust both $ alpha$ and $PathValue$. | . Really logically speaking, what else can you do? These are the only values that are used to adjust our values, so any tweaks must involve those. We can modify number through addition, subtraction, multiplication, and division: If you get stuck, try to get back to those basics. . Python Function . alpha = 0.0005 def gradient_descent(xs,ys,alpha,cycles): n = len(xs) adj_alpha = (1/n)*alpha values = pd.DataFrame({&#39;x&#39;:xs,&#39;y&#39;:ys}) weights = pd.DataFrame({&#39;cycle0&#39;:[1,1,0,0]},index=[&#39;m&#39;,&#39;b&#39;,&#39;pvb&#39;,&#39;pvm&#39;]) values[&#39;cycle0&#39;] = weights[&#39;cycle0&#39;].m*values[&#39;x&#39;] + weights[&#39;cycle0&#39;].b for cycle in range(1,cycles+1): p_cycle_name = &#39;cycle&#39;+str(cycle-1) c_cycle_name = &#39;cycle&#39;+str(cycle) error = values[p_cycle_name]-values[&#39;y&#39;] path_value_b = sum(error) path_value_m = sum(error * values[&#39;x&#39;]) new_m = weights[p_cycle_name].m - path_value_m * adj_alpha new_b = weights[p_cycle_name].b - path_value_b * adj_alpha weights[c_cycle_name] = [new_m, new_b, path_value_m, path_value_b] y_pred = weights[c_cycle_name].m*values[&#39;x&#39;] + weights[c_cycle_name].b values[c_cycle_name] = y_pred return weights,values weights, values = gradient_descent(xs,ys,alpha,cycles) graph_gradient_descent(values,cycles,12,2) . Momentum . Concept . The idea of momentum is to use the previous path values to influence the new path value. It&#39;s taking a weighted average of the previous path value and the new calculation. This is referred to as momentum because it is using the momentum from previous points to change the size of the step to take. To control what kind of weighted average is used, we define $ beta$. . This is useful and effective because we want to have very large steps early on, but the closer we get to the optimal values the lower we want our learning rate to be. This allows us to do that, and if we overshoot then it will average with previous path values and lower the step size. This allows for larger steps while minimizing the risk of our gradient descent going out of control. If you overshoot the optimal weights the weighted average will decrease the step size and keep going, eventually settling on the minimum. A very handy feature! . What is different . What is different: $PathValue$ has changed and is using a new input $ beta$ . If you look at $PathValue_b$ you will notice a change in this formula. $PathValue_m$ multiplies $PathValue_b$ by our x value for that point, so it is effected as well. . New Inputs . $ beta = 0.9$ . New Formulas . $ alpha_{adj} = frac{1}{n} alpha$ . $PathValue_m$ = $PathValue_bx$ . $PathValue_b = ( beta)(PathValue_{b_{previous}}) + (1 - beta)(y_{pred}-y_{obs})$ . Python Function . alpha = 0.0001 beta = 0.9 def gradient_descent_momentum(xs,ys,alpha,cycles,beta): n = len(xs) adj_alpha = (1/n)*alpha values = pd.DataFrame({&#39;x&#39;:xs,&#39;y&#39;:ys}) weights = pd.DataFrame({&#39;cycle0&#39;:[1,1,0,0]},index=[&#39;m&#39;,&#39;b&#39;,&#39;pvm&#39;,&#39;pvb&#39;]) values[&#39;cycle0&#39;] = weights[&#39;cycle0&#39;].m*values[&#39;x&#39;] + weights[&#39;cycle0&#39;].b for cycle in range(1,cycles+1): p_cycle_name = &#39;cycle&#39;+str(cycle-1) c_cycle_name = &#39;cycle&#39;+str(cycle) error = values[p_cycle_name]-values[&#39;y&#39;] path_value_b = sum(error) path_value_m = sum(error * values[&#39;x&#39;]) if cycle &gt; 1: path_value_b = (beta) * weights[p_cycle_name].pvb + (1-beta) * path_value_b path_value_m = (beta) * weights[p_cycle_name].pvm + (1-beta) * path_value_m new_m = weights[p_cycle_name].m - path_value_m * adj_alpha new_b = weights[p_cycle_name].b - path_value_b * adj_alpha weights[c_cycle_name] = [new_m, new_b, path_value_m, path_value_b] y_pred = weights[c_cycle_name].m*values[&#39;x&#39;] + weights[c_cycle_name].b values[c_cycle_name] = y_pred return weights,values weights, values = gradient_descent_momentum(xs,ys,alpha,cycles,beta) graph_gradient_descent(values,cycles,12,3) . RMSProp . Concept . The idea of RMS prop is that we will adjust out learning rate based on how large our error rate it. If we have a very large error, we will take a larger step. With a smaller error, we will take a smaller step. This minimizes the changces that we overshoot the ideal weights. This is accomplished by diving the learning rate by an weighted exponential average of the previous path values. To control what kind of weighted average is used, we define $ beta$. . This is useful and effective because we want to have very large steps early on with a big learning rate, but the closer we get to the optimal values the lower we want our learning rate to be. This is exactly what RMS prop does - adjust out learning rate based on our error rate. This allows for larger steps with a bigger learning rate while minimizing the risk of our gradient descent going out of control. It has similar benefits of momentum, but approaches it by modifying the learning rath rather than the path value. . What is different . What is different: we have an alpha_multiplier for each variable that is calculated each cycle. When calculating the new value, we divide our learning rate $ alpha$ by the square root of this alpha multiplier. The alpha multiplier uses a new input, $ beta$ . Our alpha multiplier is calculated with this formula. . $alphamultiplier_b = ( beta)(alphamultiplier_{b_{previous}}) + (1 - beta)((y_{pred}-y_{obs})^2)$ . $alphamultiplier_m = ( beta)(alphamultiplier_{m_{previous}}) + (1 - beta)(x(y_{pred}-y_{obs})^2)$ . Here&#39;s the path value for our Regular Gradient Descent | . $new_m = m - PathValue_m * frac{ alpha}{n}$ . $new_b = b - PathValue_b * frac{ alpha}{n}$ . Here&#39;s the path value for our RMS Prop | . $new_m = m - PathValue_m * frac{ alpha}{n sqrt{alphamultiplier_m}}$ . $new_b = b - PathValue_b * frac{ alpha}{n sqrt{alphamultiplier_b}}$ . Python Function . beta = 0.9 alpha = 500 def gradient_descent_momentum(xs,ys,alpha,cycles,beta): n = len(xs) adj_alpha = (1/n)*alpha values = pd.DataFrame({&#39;x&#39;:xs,&#39;y&#39;:ys}) weights = pd.DataFrame({&#39;cycle0&#39;:[1,1,0,0,0,0]},index=[&#39;m&#39;,&#39;b&#39;,&#39;pvm&#39;,&#39;pvb&#39;,&#39;am_m&#39;,&#39;am_b&#39;]) values[&#39;cycle0&#39;] = weights[&#39;cycle0&#39;].m*values[&#39;x&#39;] + weights[&#39;cycle0&#39;].b for cycle in range(1,cycles+1): p_cycle_name = &#39;cycle&#39;+str(cycle-1) c_cycle_name = &#39;cycle&#39;+str(cycle) error = values[p_cycle_name]-values[&#39;y&#39;] path_value_b = sum(error) path_value_m = sum(error * values[&#39;x&#39;]) alpha_multiplier_b = abs(path_value_b)**2 alpha_multiplier_m = abs(path_value_m)**2 if cycle &gt; 1: alpha_multiplier_b = (beta) * weights[p_cycle_name].am_b + (1-beta) * alpha_multiplier_b alpha_multiplier_m = (beta) * weights[p_cycle_name].am_m + (1-beta) * alpha_multiplier_m new_m = weights[p_cycle_name].m - path_value_m * adj_alpha / math.sqrt(alpha_multiplier_m) new_b = weights[p_cycle_name].b - path_value_b * adj_alpha / math.sqrt(alpha_multiplier_b) weights[c_cycle_name] = [new_m, new_b, path_value_m, path_value_b, alpha_multiplier_m, alpha_multiplier_b] y_pred = weights[c_cycle_name].m*values[&#39;x&#39;] + weights[c_cycle_name].b values[c_cycle_name] = y_pred return weights,values weights, values = gradient_descent_momentum(xs,ys,alpha,cycles,beta) graph_gradient_descent(values,cycles,15,2) . Adam . Concept . The idea of Adam is that there are really nice properties to RMS Prop as well as momentum, so why not do both at the same time. We will modify our path value using the momentum formula and we will modify our learning rate using RMSProp formula. To control what kind of weighted average is used, we define beta_rmsprop and beta_momentum. We can have a pretty big learning rate without overshooting. . This is useful and effective because we want the ability to pick up speed like momentum does, but also want to minimize overshooting. Basically we pick up momementum when we are far from the minimum, but we slow down as we get close before we overshoot. . beta_rmsprop = 0.9 beta_momentum = 0.7 alpha = 200 def gradient_descent_momentum(xs,ys,alpha,cycles,beta_rmsprop,beta_momentum): n = len(xs) adj_alpha = (1/n)*alpha values = pd.DataFrame({&#39;x&#39;:xs,&#39;y&#39;:ys}) weights = pd.DataFrame({&#39;cycle0&#39;:[1,1,0,0,0,0]},index=[&#39;m&#39;,&#39;b&#39;,&#39;pvm&#39;,&#39;pvb&#39;,&#39;am_m&#39;,&#39;am_b&#39;]) values[&#39;cycle0&#39;] = weights[&#39;cycle0&#39;].m*values[&#39;x&#39;] + weights[&#39;cycle0&#39;].b for cycle in range(1,cycles+1): p_cycle_name = &#39;cycle&#39;+str(cycle-1) c_cycle_name = &#39;cycle&#39;+str(cycle) error = values[p_cycle_name]-values[&#39;y&#39;] path_value_b = sum(error) path_value_m = sum(error * values[&#39;x&#39;]) if cycle &gt; 1: path_value_b = (beta_momentum) * weights[p_cycle_name].pvb + (1-beta_momentum) * path_value_b path_value_m = (beta_momentum) * weights[p_cycle_name].pvm + (1-beta_momentum) * path_value_m alpha_multiplier_b = abs(path_value_b)**2 alpha_multiplier_m = abs(path_value_m)**2 if cycle &gt; 1: alpha_multiplier_b = (beta_rmsprop) * weights[p_cycle_name].am_b + (1-beta_rmsprop) * alpha_multiplier_b alpha_multiplier_m = (beta_rmsprop) * weights[p_cycle_name].am_m + (1-beta_rmsprop) * alpha_multiplier_m new_m = weights[p_cycle_name].m - path_value_m * adj_alpha / math.sqrt(alpha_multiplier_m) new_b = weights[p_cycle_name].b - path_value_b * adj_alpha / math.sqrt(alpha_multiplier_b) weights[c_cycle_name] = [new_m, new_b, path_value_m, path_value_b, alpha_multiplier_m, alpha_multiplier_b] y_pred = weights[c_cycle_name].m*values[&#39;x&#39;] + weights[c_cycle_name].b values[c_cycle_name] = y_pred return weights,values weights, values = gradient_descent_momentum(xs,ys,alpha,cycles,beta_rmsprop,beta_momentum) graph_gradient_descent(values,cycles,15,2) .",
            "url": "https://isaac-flath.github.io/blog/gradient%20descent/2020/06/11/GradientDescentforLinearRegression-P2.html",
            "relUrl": "/gradient%20descent/2020/06/11/GradientDescentforLinearRegression-P2.html",
            "date": " • Jun 11, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Solving $Ax=b$ - The Complete Solution (18.06_L8)",
            "content": "Ax=b . $ left[ begin{array}{cccc|c} 1&amp;2&amp;2&amp;2&amp;b1 2&amp;4&amp;6&amp;8&amp;b2 3&amp;6&amp;8&amp;10&amp;b3 end{array} right]$ . Note: You may notice not all rows and columns are independent Let&#39;s do elimination to start solving. . $ left[ begin{array}{cccc|c} 1&amp;2&amp;2&amp;2&amp;b_1 2&amp;4&amp;6&amp;8&amp;b_2 3&amp;6&amp;8&amp;10&amp;b_3 end{array} right]$ $=&gt;$ $ left[ begin{array}{cccc|c} 1&amp;2&amp;2&amp;2&amp;b_1 0&amp;0&amp;2&amp;4&amp;b_2-2b_1 0&amp;0&amp;2&amp;4&amp;b_3-3b_1 end{array} right]$ $=&gt;$ $ left[ begin{array}{cccc|c} 1&amp;2&amp;2&amp;2&amp;b_1 0&amp;0&amp;2&amp;4&amp;b_2-2b_1 0&amp;0&amp;0&amp;0&amp;b_3-b_2-b_1 end{array} right]$ . Well that makes sense. We can see intuitively we that the last row - the second - the first gives us 0. . Solvability condition on b . Solvable when b is in $C(A)$ . Note: Column space of A $C(A)$ is all linear combinations of columns of A | If a combination of A gives zero row, then the same combinations of entries of b must give 0 | . Find Complete Solution . Find $x_{particular}$ . Set all free variables to 0 | Solve $Ax=b$ for pivot variables | Set all free variables to 0 . Free variable are rows that do not have a pivot column. These free variables can be set to anything, so we set them to the most convenient thing 0 and do back substitution. . $ left[ begin{array}{cccc} 1&amp;2&amp;2&amp;2 0&amp;0&amp;2&amp;4 0&amp;0&amp;0&amp;0 end{array} right]$ $ left[ begin{array}{cccc} x_1 0 x_3 0 end{array} right]$ $=$ $ left[ begin{array}{ccc} 1 3 0 end{array} right]$ . Solve $Ax-b$ for pivot variables. . $x_1 + 2x_3 = 1$ $2x_3 = 3$ . This means . $x_3 = 3/2$ and $x_1 = -2$ . Put those into out x and we have. . $x_p =$ $ left[ begin{array}{cccc} -2 0 3/2 0 end{array} right]$ . Find all Solutions . Find anything in $x_{nullspace}$ | $X=X_p + X_n$ . Note: The nullspace are any values of x that solve for 0. So naturally the x particular solution + any combination of 0 is still the solution. $x_{complete} = $ $ left[ begin{array}{cccc} -2 0 3/2 0 end{array} right]$ $+c_1$ $ left[ begin{array}{cccc} -2 1 0 0 end{array} right]$ $+c_1$ $ left[ begin{array}{cccc} 2 0 -2 1 end{array} right]$ |",
            "url": "https://isaac-flath.github.io/blog/linear%20algebra/2020/06/06/18.06_8_Solving-Ax=b-row-reduced-form-R.html",
            "relUrl": "/linear%20algebra/2020/06/06/18.06_8_Solving-Ax=b-row-reduced-form-R.html",
            "date": " • Jun 6, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Gradient Descent for Linear Regression (Part 1B)",
            "content": "Why part 1B? . I have been getting questions about the initial Gradient Descent Post. The questions boil down to &quot;So with 2 points I can define a line, but I could already do that. What I need is to fit a line where points aren&#39;t perfect and I have lots of them. How do I use gradient descent in a more complicated problem? . This post will quickly recap the initial Gradient Descent for Linear Regression post, show that methodology applied in a google sheet so you can see each calculation, and then scale that methodology to more points. . Goal Recap . The goal of linear regression is to find parameter values that fit a linear function to data points.  The goodness of fit is measured by a cost function.  The lower the cost, the better the model.  Gradient Descent is a method to minimize a cost function.  Gradient descent is a widely used tool and is used frequently in tuning predictive models.  It’s important to understand the method so you can apply it to various models and are not limited to using black box models. . I will use the sum of squares cost function to take a predicted line and slowly change the regression coefficients until the line is a line of best fit. Here&#39;s what it looks like before and after 24 iterations of gradient descent. As you can see, after 24 iterations our predicted points are getting pretty close to a best fit. You will be able to use the method defined here to scale this to as many points as you have. . . In the first blog we showed this table for how we calculate out prediction. Because we are talking about a linear problem, y = mx + b is the equation, or in calculus terms $y = theta_0+ theta_1x$. We could take this table and expand it down to include $x_3$ all the way through $x_n$ to represent our dataset. . . The tree below from the first blog illustrates how to solve for cost as well as how to improve the values of $ theta$ to minimize cost in the 2 point problem defined there. So the question is, how would we modify this tree for more points? Well, with more data points there would be more edges originating at $J$, and with more features there would be more thetas originating from the predicted values, but the same concept can be applied to these more complicated examples. Specifically, here is what I would change for a more complicated example with more features: . First, we have a branch for $x^1$ and a branch for $x^2$. These branches are almost identical, other than it being for the 2nd point vs the 1st point. So the first step is to add a branch off of $J = A^1 + a^2$ for $x^3$ all the way to $x^n$. | The second step is to take our formula $1/2 * (y_{pred} - y_{obs})^2$ and change $1/2$ to 1 over &lt;# of data points&gt;. This isn&#39;t strictly neccesary, but it makes the values of J we see a bit more intiutive and scaled. | The third thing is to multiply our path values by 1 over &lt;# of data points&gt;. Again, this isn&#39;t strictly neccesary but it makes setting a learning rate much more intuitive rather than having to do something more complicated to scale our learning rate based on the number of points we have. As a refresher, the path value for theta 1 was $ theta_1 ; path ;value = x^1 (y^1_{pred} - y^1_{obs}) (1) + x^2 (y^2_{pred} - y^2_{obs}) (1)$, which by multiplying values from the edges in the chart together. The path value for theta 1 will now be $ theta_1 ; path ;value = (x^1 (y^1_{pred} - y^1_{obs}) (1) + x^2 (y^2_{pred} - y^2_{obs}) (1)) * frac{1}{&lt; # features&gt;)}$. We will do that for the path value formula for $ theta_0$ as well. | . Just like with 2 points, we will multiply the path value by $ alpha$, and subtract that from that $ theta$ to improve our predictions . See this actually work . I have created a google sheet that walks through these cauculations. I strongly reccomend walking through each cell calculation and matching it up to the chart above. Star with the 2Points_Linear_Scalable tab. You can then go to the More_Points_Linear tab and see that it&#39;s the exact same formulas and calculations. . Click here for the Google Sheet . For bonus points, you can start to see what a more advanced gradient descent algorithm is on the Momentum tab. If you look through all the formulas, you will see it&#39;s almost the same thing - but instead of using just the new path value we are doing a weighted average of the path value with the previous path value. .",
            "url": "https://isaac-flath.github.io/blog/gradient%20descent/2020/06/01/GradientDescentforLinearRegression-P1B.html",
            "relUrl": "/gradient%20descent/2020/06/01/GradientDescentforLinearRegression-P1B.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Column Space and Null Space (18.06_L7)",
            "content": "Ax=0 . $A =$ $ begin{bmatrix} 1&amp;2&amp;2&amp;2 2&amp;4&amp;6&amp;8 3&amp;6&amp;8&amp;10 end{bmatrix}$ . Note: You may notice not all rows and columns are independent Let&#39;s do elimination to start solving. . $ begin{bmatrix} 1&amp;2&amp;2&amp;2 2&amp;4&amp;6&amp;8 3&amp;6&amp;8&amp;10 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;2&amp;2&amp;2 0&amp;0&amp;2&amp;4 0&amp;0&amp;2&amp;4 end{bmatrix}$ . Uh oh. Now I see a 0 in the pivot position and I cannot do a row exchange. Really that&#39;s just telling me that the now is just a combination of earlier columns. It&#39;s depending on earlier columns. Let&#39;s just go on and use the next column as the pivot for that row. . $A=$ $ begin{bmatrix} 1&amp;2&amp;2&amp;2 2&amp;4&amp;6&amp;8 3&amp;6&amp;8&amp;10 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;2&amp;2&amp;2 0&amp;0&amp;2&amp;4 0&amp;0&amp;2&amp;4 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;2&amp;2&amp;2 0&amp;0&amp;2&amp;4 0&amp;0&amp;0&amp;0 end{bmatrix}$ $=U$ . Note: We have 2 pivots (aka rank 2). Row 1, Column 1 and Row 2, Column 3. . Special Solutions. . Let&#39;s write out $U$ in matrix in equation form to see what we have. . $x_1+2x_2+2x_3+2x_4=0$ . $2x_3+4x_4=0$ . Let&#39;s call the columns with a pivot a pivot column, and ones without a free column. So we have 2 pivot columns and 2 free columns. . Let&#39;s assign some values to the free columns, then solve for the pivot columns using back substitution. We&#39;ll call these special solutions. . $x=$ $ begin{bmatrix} 1 0 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} -2 1 0 0 end{bmatrix}$ $=&gt;$ $C$ $ begin{bmatrix} -2 1 0 0 end{bmatrix}$ . Let&#39;s put 1 in the other free variable. . $x=$ $ begin{bmatrix} 0 1 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 2 0 -2 1 end{bmatrix}$ $=&gt;$ $C$ $ begin{bmatrix} 2 0 -2 1 end{bmatrix}$ . Great. Our entire nullspace is a linear combination of these 2 special solutions. There will be 1 special solution for each free variable. In this case there were 2 (free columns, rank, free variables, etc). . $C$ $ begin{bmatrix} -2 1 0 0 end{bmatrix}$ $+d$ $ begin{bmatrix} 2 0 -2 1 end{bmatrix}$ . Reduced Row Echelon Form ($R$) . Reduced row echelon form has zeros above and below pivots, and the pivots are = 1. Let&#39;s do this using elimination. . $U=$ $ begin{bmatrix} 1&amp;2&amp;2&amp;2 0&amp;0&amp;2&amp;4 0&amp;0&amp;0&amp;0 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;2&amp;0&amp;-2 0&amp;0&amp;2&amp;4 0&amp;0&amp;0&amp;0 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;2&amp;0&amp;-2 0&amp;0&amp;1&amp;2 0&amp;0&amp;0&amp;0 end{bmatrix}$ $=R$ . Note: This row of 0&#8217;s was created because elimination identified that row 3 is just a combination of rows 1 and 2. Why this form? . We see in the pivot columns we see $I$ $ begin{bmatrix}1&amp;0 0&amp;1 end{bmatrix}$ . In the free columns we see the opposite of the nullspace definintions we found above $ begin{bmatrix}2&amp;-2 0&amp;2 end{bmatrix}$ . Solve the transpose . $A=$ $ begin{bmatrix} 1&amp;2&amp;3 2&amp;4&amp;6 2&amp;6&amp;8 2&amp;8&amp;10 end{bmatrix}$ . Elimination from $A$ -&gt; $U$ . $ begin{bmatrix} 1&amp;2&amp;3 2&amp;4&amp;6 2&amp;6&amp;8 2&amp;8&amp;10 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;2&amp;3 0&amp;0&amp;0 0&amp;2&amp;2 0&amp;4&amp;4 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;2&amp;3 0&amp;2&amp;2 0&amp;0&amp;0 0&amp;0&amp;0 end{bmatrix}$ . Note: Notice that I did a row exchange to do this elimination and ended with a rank 2, which was the same as with the transpose. . Solve for Nullspace . There&#39;s 1 free column, so you fill in a 1 in the free column. $x=$ $ begin{bmatrix} 1 end{bmatrix}$ Then with back substution solve for the pivot columns. The solution is any linear combination of this, which is a line in the null space. . $$x=C begin{bmatrix}-1 -1 1 end{bmatrix}$$ . Row Reduction from $U$ -&gt; $R$ . Row Echelon Form . $ begin{bmatrix} 1&amp;2&amp;3 0&amp;2&amp;2 0&amp;0&amp;0 0&amp;0&amp;0 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;0&amp;1 0&amp;2&amp;2 0&amp;0&amp;0 0&amp;0&amp;0 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;0&amp;1 0&amp;1&amp;1 0&amp;0&amp;0 0&amp;0&amp;0 end{bmatrix}$ $=R$ . We see the identity matrix in the pivot columns, and the opposite of our pivot nullspace varibales in the free column. Perfect! .",
            "url": "https://isaac-flath.github.io/blog/linear%20algebra/2020/05/17/18.06_7_pivot_variables_special_solutions.html",
            "relUrl": "/linear%20algebra/2020/05/17/18.06_7_pivot_variables_special_solutions.html",
            "date": " • May 17, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Column Space and Null Space (18.06_L6)",
            "content": "Background . We started on this topic in the last post, so much this section is review - but critical for the upcoming discussion. . Vector Spaces . To have a vector space you need to be able to: . Add any vectors and still be within the space | Multiply by any number and stay within space | Take any linear combination and stay within space | Examples . $R^2$ is a vector space. This is all real number vectors with 2 components. Here&#39;s a few examples. . $ begin{bmatrix}3 2 end{bmatrix}$, $ begin{bmatrix}0 0 end{bmatrix}$, $ begin{bmatrix} pi e end{bmatrix}$ . $R^3$ is another vector space, but it contains all real 3D vectors. Here&#39;s a few examples. . $ begin{bmatrix}3 2 5 end{bmatrix}$, $ begin{bmatrix}3 2 0 end{bmatrix}$, $ begin{bmatrix}0 0 0 end{bmatrix}$, $ begin{bmatrix} pi e 5 end{bmatrix}$ . $R^n$ is another with all column vectors with $n$ real components. . Subspaces . What if we just want a piece of $R^n$? We still need to be able to add vectors and multiply it together. So what are some subspaces? . A line through $R^n$ that goes through the origin. Any multiple of a point on a line is still on the line. We need the origin because you can mutliply any vector by 0, which would give you the origin. | Just the Origin is a subspace. You can add it to itelf, multiply it by anything, and take any combination and you will stil lhave the origin. | In $R^3$ and above, a plane through the origin is also a subspace. | Combinations of Subspaces . If I have 2 subspaces, then all vectors in P or L or both does not make a subspace. I can&#39;t add all together. In $R^3$ I would have to make a cube which would be all of $R^3$. . However, if you all points that are in both subspaces does make a subspace it is a subspace. If you think about it, by definition 2 Vectors in the intersection are for the subspace 1 and they are both in subspace 2. We already know that if you add 2 vectors in subspace 1 you get another vector in subspace 1. We also know the same thing about subspace 2. So really, any linear combination has to be in both subspaces (probably a smaller subspace). . Subspace from a matrix . Column Space . $ begin{bmatrix}1&amp;1&amp;2 2&amp;1&amp;3 3&amp;1&amp;4 4&amp;1&amp;5 end{bmatrix}$ . Columns are in $R^4$ and all their linear combinations are a subspace. So we have 3 subspaces, 1 for each column. So the full subspace for this matrix must have a linear combination of all columns, or all 3 subspaces. With 3 columns do we get all of $R^4$? What exactly do we get? How big is this subspace? Does $Ax=b$ have a solution for every $b$? Which $b$ values are ok? . We know it doesn&#39;t, because we have 4 equations but only 3 unknowns. In the proper form we see. . $Ax=b$ $ begin{bmatrix}1&amp;1&amp;2 2&amp;1&amp;3 3&amp;1&amp;4 4&amp;1&amp;5 end{bmatrix}$ $ begin{bmatrix}x_1 x_2 x_3 end{bmatrix}$ $=$ $ begin{bmatrix}b_1 b_2 b_3 b_4 end{bmatrix}$ . Now, just because we can&#39;t solve for every b doesn&#39;t mean we can&#39;t solve it for any b. So which $b$&#39;s allow this to be solved? Obviously we can solve for the 0 vector. Obviously we can solve for (1,2,3,4) or any other column as we can have $x_1 = 1$ with the other x&#39;s being 0. We could also solve for multiples of any column because $x_2$ could equal 2 or 3 with other uknowns being 0. . Note: To Summarize, Ax=b can be solved with $b$ is in $C(A)$, the column space of A. Now column 3 and column 1 are redundant, so we can throw away column 3. Column 1 + column 2 = column 3. So really linear combinations of the first 2 columns gives the same subspace as linear combinations of all 3 columns. So in all practicality, we have 2 equations with 4 unknowns with 2 of the columns sitting on the same plane. . Nullspace . $ begin{bmatrix}1&amp;1&amp;2 2&amp;1&amp;3 3&amp;1&amp;4 4&amp;1&amp;5 end{bmatrix}$ . Nullspace of $A$ = all solutions of $x$ where $Ax=0$. Because there are 3 unknowns, it&#39;s a subspace in $R^3$. This is opposed to the column space that is in $R^4$. Naturally the 0 vector always satisfies this so is always in the nullspace. Here&#39;s a few others . $ begin{bmatrix}0 0 0 end{bmatrix}$, $ begin{bmatrix}1 1 -1 end{bmatrix}$, $ begin{bmatrix}2 2 -2 end{bmatrix}$, $ begin{bmatrix}-1 -1 1 end{bmatrix}$, $ begin{bmatrix}-2 -2 2 end{bmatrix}$ . To summarize, we really have a line in $R^3$, which is a subspace: $c begin{bmatrix}1 1 -1 end{bmatrix}$ . Solution Spaces . Great! So we have defined the nullspace, which is just a subspace when $b$ is the 0 vector. Why not do the same thing with other $b$&#39;s? For example: . $Ax=b$ $ begin{bmatrix}1&amp;1&amp;2 2&amp;1&amp;3 3&amp;1&amp;4 4&amp;1&amp;5 end{bmatrix}$ $ begin{bmatrix}x_1 x_2 x_3 end{bmatrix}$ $=$ $ begin{bmatrix}1 2 3 4 end{bmatrix}$ . Clearly it isn&#39;t a space because the zero vector is not a solution. . Possible Solutions = $ begin{bmatrix}1 0 0 end{bmatrix}$, $ begin{bmatrix}2 1 -1 end{bmatrix}$, $ begin{bmatrix}3 2 -2 end{bmatrix}$, $ begin{bmatrix}0 -1 1 end{bmatrix}$ .",
            "url": "https://isaac-flath.github.io/blog/linear%20algebra/2020/05/16/18.06_6_ColumnSpace_Nullspace.html",
            "relUrl": "/linear%20algebra/2020/05/16/18.06_6_ColumnSpace_Nullspace.html",
            "date": " • May 16, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Transposes, Permutations, and Spaces (18.06_L5)",
            "content": "Background . In previous posts, we have gone over elimination to solve systems of equations. However, every example workout out perfectly. What I mean by that is we didn&#39;t have to do row exchanges. Every example was in an order that worked on attempt 1. In this post, we are going to talk about how to solve equations when this isn&#39;t the case. . Row Exchanges . The way we do a row exchange in matrix language is that we multiply by a permutation matrix, which we will refer to as $P$. . So how do we account for row exchanges? Our previous equation of $A=LU$ does not account for row exchanges. . Permutations . A Permutation matrix executes our row exhanges. This matrix $P$ is the identify matrix with reording rows. $A=LU$ becomes $PA=LU$. We multipy a permutation matrix $P$ by $A$. . Note: The reason we were able to ignore $P$ in previous posts is because when you have no row exchanges, $P$ is just the identify matrix (no re-ordering) If we were to switch rows 2 and 3 in a 3x3 matrix, $P$ would be . $$ begin{bmatrix}1&amp;0&amp;0 0&amp;0&amp;1 0&amp;1&amp;0 end{bmatrix}$$ . Transposed Matrices . A transposed matrix has rows and columns switched. Naturally, symmetric matrices are unchanged by the transpose. . A Matrix multiplied by it&#39;s transpose always gives a symmetric matrix. . Vector Spaces . To have a vector space you need to be able to: . Add any vectors and still be within the space | Multiply by any number and stay within space | Take any linear combination and stay within space | Examples . $R^2$ is a vector space. This is all real number vectors with 2 components. Here&#39;s a few examples. . $ begin{bmatrix}3 2 end{bmatrix}$, $ begin{bmatrix}0 0 end{bmatrix}$, $ begin{bmatrix} pi e end{bmatrix}$ . $R^3$ is another vector space, but it contains all real 3D vectors. Here&#39;s a few examples. . $ begin{bmatrix}3 2 5 end{bmatrix}$, $ begin{bmatrix}3 2 0 end{bmatrix}$, $ begin{bmatrix}0 0 0 end{bmatrix}$, $ begin{bmatrix} pi e 5 end{bmatrix}$ . $R^n$ is another with all column vectors with $n$ real components. . Subspaces . What if we just want a piece of $R^n$? We still need to be able to add vectors and multiply it together. So what are some subspaces? . A line through $R^n$ that goes through the origin. Any multiple of a point on a line is still on the line. We need the origin because you can mutliply any vector by 0, which would give you the origin. | Just the Origin is a subspace. You can add it to itelf, multiply it by anything, and take any combination and you will stil lhave the origin. | In $R^3$ and above, a plane through the origin is also a subspace. | Subpace from a matrix . $ begin{bmatrix}1&amp;3 2&amp;3 4&amp;1 end{bmatrix}$ . Columns in $R^3$ and all their linear combinations are a subspace. This subspace is a plane going through the origin .",
            "url": "https://isaac-flath.github.io/blog/linear%20algebra/2020/05/13/18.06_5_Transposes,Permutations,Spaces.html",
            "relUrl": "/linear%20algebra/2020/05/13/18.06_5_Transposes,Permutations,Spaces.html",
            "date": " • May 13, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Predictive Customer Analytics (Part 2)",
            "content": "Intro . Customer Analytics is a valuable tool, but often the basics are overlooked. It can have a huge impact, and it doesn’t always mean complex modeling. It always starts with the basics. . This is the 2nd post in a series about customer analytics. The first post discusses descriptive analytics and how to create and use descriptive customer analytics. In this post we will be building on that descriptive framework to create a very basic predictive model. A predictive model allows companies to plan proactively instead of reactively. . Background . In the first post we created three descriptive metrics (Recency, Frequency, and Average Monetary Value). Recency is days since the last purchase. Frequency is the number of cycles that the customer purchased product in. Monetary Value is the average revenue brought in from that customer in cycles where the customer purchased product. How can you determine the predicted customer value and take action based on that information? . We need to group customers based on how they behave. This allows us to quickly and efficiently test different actions and see how that affects our long term revenue projections. A customer who makes many small purchases may be just as valuable as a customer who places huge orders infrequently, but they should be handled differently. RFM is one tool we can use to group customers together. . Normalize the metrics . We start with a basic table with three companies and their 3 KPIs. .   Recency (days since previous purchase) Frequency (number of months with purchases) Monetary Value (avg monthly revenue during months with purchases) . Customer A | 3 | 3 | $79 | . Customer B | 35 | 2 | $91 | . Customer C | 48 | 1 | $9852 | . Now that we have values for each of these three metrics (Recency, Frequency, and Monetary Value), we need to put them into a format where we can easily compare the customers. This is called normalizing the data. Normalization puts all the metrics on a scale of 0 (lowest) to 1 (highest). Here is a formula for normalizing: . (&lt;Value you want to normalize&gt; – &lt;Minimum Value in series&gt;) / (&lt;Maximum value in series&gt; – &lt;Minimum value in series&gt;) . Let’s walk through normalizing the Recency KPI. Based on the formula above we need a couple of pieces of information. Let’s identify these now. . Minimum Value in series = 3 Maximum value in series = 48 . Now let’s plug in the formula for each customer to normalize the values. .   Recency Normalized Recency . Customer A | 3 | =(3-3) / (48-3) = 0 | . Customer B | 35 | =(35-3) / (48-3) = 0.711 | . Customer C | 48 | =(48-3) / (48-3) = 1 | . The next step is to repeat this process for the other metrics. I won’t walk through the other two, but here are the accurate values if you’d like to test yourself. .   Recency Frequency Monetary Value . Customer A | 0 | 1 | 0 | . Customer B | 0.711 | 0.5 | 0.001 | . Customer C | 1 | 0 | 1 | . A low recency score is good. A high Frequency or Monetary Value Score is good. If you’d like to simplify it so you don’t have to remember that feel free to take your normalized recency score and subtract it from 1. That will make low scores bad and high scores good on all metrics. . Group the customers together . Now that we have normalized our KPIs, we need to group our customers together. In the table above we only have 3 customers, so there would be no point in grouping them together. In a more practical example we may have 1000 customers, 10,000 customers, or more. When we have that many customers, we can greatly simplify things by grouping like customers together. . We can group customers using our RFM KPIs, but there may be more information we want to pull into that decision. For example if we are a manufacturing company that sells product to end users as well as distributors we may want to take that into account. A distributor is likely to behave and react differently from an individual person. For example, an individual person may get a coupon in the mail and decide to purchase, but a distributor contact may get hundreds of coupons a week and just get annoyed. . A lot of time should be spent exploring different ways to break up customers. Explaining how you break up the customers into groups to sales people, product management, purchasing, production control, and other functional groups in your company can be a great way to get ideas. Each of these groups sees how customers react in different ways and talking to all of them can help give a more complete picture. . Create probability matrix . The goal of customer analytics is to predict how much money we are going to make next month, and the month after, and the month after that, and so on. Once we can predict that with reasonable accuracy we can determine exactly how much revenue we can expect from each customer if we do not change anything. . We need to put probabilities on customer actions. In simplest terms the customer has 2 choices during each cycle, buy or do not buy. So how do we figure out what the chance of that is for a group? . Suppose we a group of distributors we sell to that we believe act similarly as with the table below. A 1 means that the distributor bought something in that cycle, a 0 means that the distributor did not buy anything in that cycle. .   January February March . Distributor A | 1 | 0 | 0 | . Distributor B | 0 | 1 | 0 | . Distributor C | 0 | 1 | 1 | . Since there are 9 total opportunities to buy, and 4 of those have buys in them, we will assign a 4/9 chance of buying. Really this is just another way to measure frequency. . Now we know how many of the customers in the distributor group will buy in a given month (4 out of 9), so we need to turn that into a revenue figure. To do this we will take the sum of revenue from these customers and divide that by the number of buys. Here we are calculating Monetary Value (M), but instead of looking at it on a customer level, we are looking at it for a group of customers. . Once we multiply those two numbers together we have an estimated monthly revenue. . There’s several areas for future improvement that I won’t cover in this post. Depending on the market or the business model your company uses, they will vary in significance. Here’s a few of them: . Take into account recency. | Take into account customer retention. This makes forecasts accuracy go down farther out you go into the future. Since for this application we aren’t looking out very far in the future it makes less of a difference, but it is definitely high priority on our list of future enhancements. | Changes in buy patterns based on seasonal trends. | . Increase revenue using predicted value of each account . Now that we know roughly what revenue we can expect, we can use that for a variety of things. Here’s a few ideas: . First, we can look at which locations/territories are performing as forcasted. Once we know the high performing locations and the low performing locations, we can start to identify what the differences are. This can also be used to manage performance of employees, set meaningful incentives based on revenue, and set measurable objectives. | We can continue to do A/B testing, but be able to tie this to actual revenue dollars. | We can use these forecasts to determine inventory stances on either components or salable goods. | Now that we have grouped customers that behave similarly, we can start to determine how we should treat each group. We can have limited A/B testing. Instead of taking a random group of customers, we can take a random group of distributors to see how they specifically react. | We can determine who our most valuable customers are. Typically without measures, we remember the big purchases. But a regular purchaser that doesn’t make big purchases may be bringing us more revenue. Understanding which customers are most likely to be the most profitable is crucial and can help us determine pricing tiers for customers or determine customer loyalty programs. | We can identify low performing groups and create a strategy to get more penetration into those customers. Maybe for those customers we can send them surveys. Maybe we can set up a forum and do 6 month surveys where we work and collaborate with them regularly. This can help give insight into why they don’t purchase much, as well as be a good sales opportunity. We can also try to take action to take the low performing customers and get them to increase revenue through promotions or other incentives. | We can take the high performing customers and make sure that they know they are appreciated. Whether it’s a can of cashews over the holidays, or having a sales person reach out to talk once a month, building that relationship can really build customer loyalty. | .",
            "url": "https://isaac-flath.github.io/blog/customer%20analytics/2020/05/12/Customer-Analytics-Part-2.html",
            "relUrl": "/customer%20analytics/2020/05/12/Customer-Analytics-Part-2.html",
            "date": " • May 12, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Factorization into $A=LU$ (18.06_L4)",
            "content": "Inverse of $AB$, $A^T$ . Review . So we know from previous lectures/blog posts that $A$ time the inverse of $A$ gives the Identify Matrix $I$. The Identify matrix is matrix equivilent to multiplying by 1, no change to the matrix. . $AA^{-1}=I=A^{-1}A$ . AB Inverse . So if the Inverse of a Matrix time it&#39;s inverse gives the identity, how do we find the inverse of a product of matrices? It turns out you just multiply the inverses of those matrices in reverse. . Why in reverse? If you put socks and shoes on, you wouldn&#39;t invert that process by taking socks off first. You would invert it by doing the steps in reverse. Shoes off first. . A more technical understanding of this can be seen below: . $ABB^{-1}A^{-1}= I$ . Let&#39;s multiply the middle piece first. $BB^{-1}$ We know that equals the Identify matrix, and a matrix time the identity just gives that matrix. So then what you are left wtih is $AA^{-1}=I$, which we also know is true. . This work the other way as well: . $B^{-1}(A^{-1}A)B = I$ . $A^T$ . Transposes work in a similar way. We can tak transposes in reverse to get the Identity Matrix. . $AA^{-1}=I$ . $(A^{-1})^TA^T=I$ . Converting to $A = LU$ . 2x2 to $A=LU$ . Previously we learned that using Elimination we create these Elimination Matrices. In this example of a 2x2 matrix: . $E_{21}A = U =&gt; $ $ left[ begin{array}{cc} 1 &amp; 0 -4 &amp; 1 end{array} right]$ $ left[ begin{array}{cc} 2 &amp; 1 8 &amp; 7 end{array} right]$ $=$ $ left[ begin{array}{cc} 2 &amp; 1 0 &amp; 3 end{array} right]$ . Now how do we get $A=LU$ from this. We can see matrix $A$ and matrix $U$ are already in the right place, so lets leave those alone. We can see that we need to turn Matrix $E_{21}$ into Matrix $L$ by switching it&#39;s sides. The way we move a matrix from one side to the other is through the inverse of the matrix. . $A=LU=&gt;$ $ left[ begin{array}{cc} 2 &amp; 1 8 &amp; 7 end{array} right]$ $=$ $ left[ begin{array}{cc} 1 &amp; 0 4 &amp; 1 end{array} right]$ $ left[ begin{array}{cc} 2 &amp; 1 0 &amp; 3 end{array} right]$ . Note: $E_{21}A = U$ is just a different format of $A=LU$. In $A=LU$ you can see $L$ is just $E_{21}^{-1}$ . 3x3 to $A = LU$ . As we saw in an earlier blog, when we do elimination of a 3x3 matrix we would get more Elmination matrices. We are still working examples with no row exchanges. . $$E_{32}E_{31}E_{21}A=U$$ . So how do we convert this into $A = LU$? As we learned above, we can invert the product of matrices as well. Let&#39;s apply that here with these matrices. . $$A=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1}U$$ . Well that doesn&#39;t look as nice to work with as $A = LU$. So let&#39;s just simplify it by multiplying all the Elimination inverse matrices together. Meaning L would be a product of these 3 inverse matrices. . $$A=E^{-1}U=LU$$ . $A=LU$ vs $EA=U$ . What&#39;s the point of taking the inverse. It&#39;s the same formula mathamatrically, so why bother with this transformation. Why $E^{-1}$ on the right and not just stay with $E$ on the left? Let&#39;s do an example. . $E_{32}E_{21}=E =&gt;$ $ left[ begin{array}{ccc} 1 &amp; 1 &amp; 0 0 &amp; 1 &amp; 0 0 &amp; -5 &amp; 1 end{array} right]$ $ left[ begin{array}{ccc} 1 &amp; 0 &amp; 0 -2 &amp; 1 &amp; 0 0 &amp; 0 &amp; 1 end{array} right]$ $=$ $ left[ begin{array}{ccc} 1 &amp; 0 &amp; 0 -2 &amp; 1 &amp; 0 10 &amp; -5 &amp; 1 end{array} right]$ . $E_{21}^{-1}E_{32}^{-1}=E^{-1} =&gt;$ $ left[ begin{array}{ccc} 1 &amp; 0 &amp; 0 2 &amp; 1 &amp; 0 0 &amp; 0 &amp; 1 end{array} right]$ $ left[ begin{array}{ccc} 1 &amp; 1 &amp; 0 0 &amp; 1 &amp; 0 0 &amp; 5 &amp; 1 end{array} right]$ $=$ $ left[ begin{array}{ccc} 1 &amp; 0 &amp; 0 2 &amp; 1 &amp; 0 0 &amp; 5 &amp; 1 end{array} right]$ . Note: 2 and 5 were the multipliers that we used in our elinimation steps. We can see that with $E^{-1}$ has 2 and 5 in the matrix. So if there are no row exchanges L is just a record of what the multipliers were, which is very convenient. Worded another way, with no row exchanges the multipliers go directly into L. .",
            "url": "https://isaac-flath.github.io/blog/linear%20algebra/2020/05/12/18.06_4_Factorization-into-a-lu.html",
            "relUrl": "/linear%20algebra/2020/05/12/18.06_4_Factorization-into-a-lu.html",
            "date": " • May 12, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Random Forest Classifier",
            "content": "Goal . The goal is to predict whether a passenger on the Titanic survived or not. The applications for binary classification are endless and could be applied to many real world problems. Does this patient have this disease? Will this customer Churn? Will price go up? These are just a few examples. . The purpose is to give a general guide to classification. If you get through this and want more detail, I highly recommend checking out the Tabular Chapter of Deep Learning for Coders with fastai &amp; Pytorch by Jeremy Howard and Sylvain Gugger. The book primarily focuses on deep learning, though decision trees are covered for tabular data. All of the material in this guide and more is covered in much greater detail in that book. . https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527 . Setup . We are going to start with loading libraries and datasets that are needed. I am going to skip over this as they are pretty self explanatory, but feel free to look close if you would like. . I am going to use Seaborn to load the Titanic dataset. . #collapse-hide from sklearn.ensemble import RandomForestClassifier import seaborn as sns import pandas as pd import numpy as np from fastai2.tabular.all import * from fastai2 import * from sklearn.model_selection import GridSearchCV from dtreeviz.trees import * from scipy.cluster import hierarchy as hc df = sns.load_dataset(&#39;titanic&#39;) df.head() . . survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone . 0 | 0 | 3 | male | 22.0 | 1 | 0 | 7.2500 | S | Third | man | True | NaN | Southampton | no | False | . 1 | 1 | 1 | female | 38.0 | 1 | 0 | 71.2833 | C | First | woman | False | C | Cherbourg | yes | False | . 2 | 1 | 3 | female | 26.0 | 0 | 0 | 7.9250 | S | Third | woman | False | NaN | Southampton | yes | True | . 3 | 1 | 1 | female | 35.0 | 1 | 0 | 53.1000 | S | First | woman | False | C | Southampton | yes | False | . 4 | 0 | 3 | male | 35.0 | 0 | 0 | 8.0500 | S | Third | man | True | NaN | Southampton | no | True | . Data Setup . Dependent Variable . We want to define what we are predicting, or the dependent variable. We also see that &#39;Survived&#39; and &#39;alive&#39; columns are the same thing with different names. We need to get rid of one and predict the other. . # survived is a duplicate of alive, so get rid of it df.drop(&#39;survived&#39;,axis = 1, inplace=True) dep_var = &#39;alive&#39; . Training and Validation Set Split . Best practice is to minimally have a training and validation set. Those are the 2 that we will use for this tutorial. . Training Set: This is what the model actually trains on | Validation Set: This is used to gauge success of the Training | Test Set: This is a held out of the total process to be an additional safeguard against overfitting | . cond = np.random.rand(len(df))&gt;.2 train = np.where(cond)[0] valid = np.where(~cond)[0] splits = (list(train),list(valid)) . Dates . We don&#39;t have any dates to deal with, but if we did we would do the following: . df = add_datepart(df,&#39;date&#39;) . This would replace that date with a ton of different columns, such as the year, the day number, the day of the week, is it month end, is it month start, and more. . Categorical Variables . Ordinal Categorical Variables . Some categorical variables have a natural heirarchy. By telling pandas the order it tends to mean trees don&#39;t have to split as many times, which speeds up training times. . df[&#39;class&#39;].unique() . [Third, First, Second] Categories (3, object): [Third, First, Second] . classes = &#39;First&#39;,&#39;Second&#39;,&#39;Third&#39; . df[&#39;class&#39;] = df[&#39;class&#39;].astype(&#39;category&#39;) df[&#39;class&#39;].cat.set_categories(classes, ordered=True, inplace=True) . Categorical Variables Final . We are now going to do some data cleaning. The Categorify and FillMissing functions in the fastai2 library make this easy. . procs = [Categorify, FillMissing] . cont,cat = cont_cat_split(df, 1, dep_var=dep_var) . to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) . Let&#39;s take a look at the training and validation sets and make sure we have a good split of each. . len(to.train),len(to.valid) . (713, 178) . We can now take a look and see that while we see all the same data, behind the scenes it is all numeric. This is exactly what we need for our random forest. . to.show(3) . sex embarked class who adult_male deck embark_town alone age_na pclass age sibsp parch fare alive . 0 male | S | Third | man | True | #na# | Southampton | False | False | 3.0 | 22.0 | 1.0 | 0.0 | 7.250000 | no | . 3 female | S | First | woman | False | C | Southampton | False | False | 1.0 | 35.0 | 1.0 | 0.0 | 53.099998 | yes | . 4 male | S | Third | man | True | #na# | Southampton | True | False | 3.0 | 35.0 | 0.0 | 0.0 | 8.050000 | no | . to.items.head(3) . pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone age_na . 0 | 3.0 | 2 | 22.0 | 1.0 | 0.0 | 7.250000 | 3 | 3 | 2 | 2 | 0 | 3 | 0 | 1 | 1 | . 3 | 1.0 | 1 | 35.0 | 1.0 | 0.0 | 53.099998 | 3 | 1 | 3 | 1 | 3 | 3 | 1 | 1 | 1 | . 4 | 3.0 | 2 | 35.0 | 0.0 | 0.0 | 8.050000 | 3 | 3 | 2 | 2 | 0 | 3 | 0 | 2 | 1 | . Final Change . Finally, we will put just the data in xs and ys so they are in easy format to pass to models. . xs,y = to.train.xs,to.train.y valid_xs,valid_y = to.valid.xs,to.valid.y . Random Forest Model . Initial Model . Let&#39;s start by creating a model without tuning and see how it does . m = RandomForestClassifier(n_estimators=100) m = m.fit(xs,y) . from sklearn.metrics import confusion_matrix . confusion_matrix(y,m.predict(xs)) . array([[445, 1], [ 5, 262]]) . Looking pretty good! Only 6 wrong. Let&#39;s see how it did on the validation set. . confusion_matrix(valid_y,m.predict(valid_xs)) . array([[89, 14], [26, 49]]) . Still way better than 50/50, but not quite as good. This is because the model did not train based on this validation data so it doesn&#39;t perform nearly as well. . Model Tuning - Grid Search . We made our first model, and it doesn&#39;t seem to predict as well as we would like. Let&#39;s do something about that. . We are going to do a grid search. There are many more sophisticated ways to find parameters (maybe a future post), but the grid search is easy to understand. Basically you pick some ranges, and you try them all to see what works best. . We will use the built in gridsearch. All we need to do is define the range of parameters, and let it find the best model. . parameters = {&#39;n_estimators&#39;:range(10,20,20), &#39;max_depth&#39;:range(10,20,20), &#39;min_samples_split&#39;:range(2,20,1), &#39;max_features&#39;:[&#39;auto&#39;,&#39;log2&#39;]} . clf = GridSearchCV(RandomForestClassifier(), parameters, n_jobs=-1) . clf.fit(xs,y) . GridSearchCV(cv=&#39;warn&#39;, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=&#39;warn&#39;, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;max_depth&#39;: range(10, 20, 20), &#39;max_features&#39;: [&#39;auto&#39;, &#39;log2&#39;], &#39;min_samples_split&#39;: range(2, 20), &#39;n_estimators&#39;: range(10, 20, 20)}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . Results . We can see below that the best esimator works better for prediciton the validation set than the model above did. Success! . confusion_matrix(y,clf.best_estimator_.predict(xs)) . array([[431, 15], [ 43, 224]]) . confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs)) . array([[95, 8], [25, 50]]) . Model Minimizing . Now that we have good results with a tuned model, we may want to simplify the model. If we can simplify the model without significantly impacting accuracy, that&#39;s good for many reasons. . The model is easier to understand | Fewer variables means fewer data quality issues and more focused data quality efforts | It takes less resources and time to run | Feature Importance . There are many ways to measure importance. How often do we use a feature to split? How high up in the tree is it used to split? We are going to use scikit learns feature importance information. . Let&#39;s look at what features are! . #collapse def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) . . fi = rf_feat_importance(m, xs) fi[:5] . cols imp . 13 | fare | 0.205103 | . 10 | age | 0.198317 | . 4 | adult_male | 0.104956 | . 0 | sex | 0.097934 | . 3 | who | 0.094934 | . Alright so we see that the most important variable is how much the passenger paid for their fare. Lovely. . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:30]); . Remove Low Important Variables . This isn&#39;t strictly neccesarry, but it is nice to simplify models if you can. Simpler models are easier to understand and maintain, and they take less resources to run. It is also interesting to know just how many variables are needed to predict. . to_keep = fi[fi.imp&gt;0.045].cols len(to_keep) . 8 . xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] . clf = GridSearchCV(RandomForestClassifier(), parameters, n_jobs=-1) clf.fit(xs_imp,y) . GridSearchCV(cv=&#39;warn&#39;, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=&#39;warn&#39;, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;max_depth&#39;: range(10, 20, 20), &#39;max_features&#39;: [&#39;auto&#39;, &#39;log2&#39;], &#39;min_samples_split&#39;: range(2, 20), &#39;n_estimators&#39;: range(10, 20, 20)}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . Results . Now we see with only 8 features we still get pretty good results on on validation set. . Now the question is whether this small loss in accuracy outweighed by a simpler and more efficient model? That is a business question more than it is a data science question. . If you are detecting COVID-19, you probably want it to be as accurate as possible. If you are going to predict whether someone is a cat or a dog person based on a survey for marketing purposes, small changes in accuracy probably are not as critical. . confusion_matrix(y,clf.best_estimator_.predict(xs_imp)) . array([[422, 24], [ 62, 205]]) . confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp)) . array([[98, 5], [24, 51]]) . clf.best_estimator_ . RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=10, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=14, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . Redundant columns . Of the 6 remaining variables, we can see that some of them are very related. It makes sense to me that deck and fare are related. Nicer areas probably cost more. It makes sense to me that the person&#39;s sex has some redudancy with adult_male - the redundancy is even in the name. . #collapse def cluster_columns(df, figsize=(10,6), font_size=12): corr = np.round(scipy.stats.spearmanr(df).correlation, 4) corr_condensed = hc.distance.squareform(1-corr) z = hc.linkage(corr_condensed, method=&#39;average&#39;) fig = plt.figure(figsize=figsize) hc.dendrogram(z, labels=df.columns, orientation=&#39;left&#39;, leaf_font_size=font_size) plt.show() . . cluster_columns(xs_imp) . Now from the chart above, we can clearly see that class and pclass are completely redundant. We see the sex and adult_male has some redundancy as well. This makes sense as part of the adult_male column is the sex. Let&#39;s go ahead and drop one of the class or pclass variables (they are redundant so doesn&#39;t matter which). . xs_imp = xs_imp.drop(&#39;pclass&#39;, axis=1) valid_xs_imp = valid_xs_imp.drop(&#39;pclass&#39;, axis=1) . xs_imp.head() . fare age adult_male sex who deck class . 0 | 7.250000 | 22.0 | 2 | 2 | 2 | 0 | 3 | . 3 | 53.099998 | 35.0 | 1 | 1 | 3 | 3 | 1 | . 4 | 8.050000 | 35.0 | 2 | 2 | 2 | 0 | 3 | . 5 | 8.458300 | 28.0 | 2 | 2 | 2 | 0 | 3 | . 6 | 51.862499 | 54.0 | 2 | 2 | 2 | 5 | 1 | . clf.fit(xs_imp,y) . GridSearchCV(cv=&#39;warn&#39;, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=&#39;warn&#39;, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;max_depth&#39;: range(10, 20, 20), &#39;max_features&#39;: [&#39;auto&#39;, &#39;log2&#39;], &#39;min_samples_split&#39;: range(2, 20), &#39;n_estimators&#39;: range(10, 20, 20)}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . Ok, so now on to variables that are not completely redundant. Let&#39;s experiment with removing some columns and see what we get. We will use accuracy for our metric. . Here is out baseline: . #collapse print(&quot;accuracy: &quot;) (confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp))[0,0] + confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp))[1,1] )/ confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp)).sum() . . accuracy: . 0.8258426966292135 . #collapse def get_accuracy(x,y,valid_x,valid_y): m = RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=10, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=4, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) m.fit(xs_imp,y) print((confusion_matrix(valid_y,m.predict(valid_xs_imp))[0,0] + confusion_matrix(valid_y,m.predict(valid_xs_imp))[1,1] )/ confusion_matrix(valid_y,m.predict(valid_xs_imp)).sum()) . . We will now loop through each of the remaining variables and train a model and print out the accuracy score. . Judging by the scores below, removing any 1 variable does not significantly reduce the accuracy. This means that we have redundant columns that can likely be trimmed. Sex seems to be a column we would definitely keep as removing it have the most impact on accuracy. . From this we can remove variables and iterate through to continue simplifying as much as possible. . #collapse variables = list(xs_imp.columns) for variable in variables: print(&#39;drop &#39;+variable+&#39; accuracy:&#39;) get_accuracy(xs_imp.drop(variable, axis=1), y, valid_xs_imp.drop(variable, axis=1), valid_y) . . drop fare accuracy: 0.8539325842696629 drop age accuracy: 0.8258426966292135 drop adult_male accuracy: 0.8370786516853933 drop sex accuracy: 0.8258426966292135 drop who accuracy: 0.8314606741573034 drop deck accuracy: 0.8146067415730337 drop class accuracy: 0.8258426966292135 .",
            "url": "https://isaac-flath.github.io/blog/tree%20ensembles/2020/05/11/RandomForestClassifier.html",
            "relUrl": "/tree%20ensembles/2020/05/11/RandomForestClassifier.html",
            "date": " • May 11, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Multiplication and Inverse Matrices (18.06_L3)",
            "content": "Matrix Multiplication . Rules . A matrix is laid out by row and column. Menaing, a particular cell in matrix $C$ is $C_{ij}$. For example, $C_{34}$ is the 3rd row, 4th column in matrix $C$. . $ begin{bmatrix} 1,1&amp;1,2&amp;1,3&amp;1,4 2,1&amp;2,2&amp;2,3&amp;2,4 3,1&amp;3,2&amp;3,3&amp;3,4 4,1&amp;4,2&amp;4,3&amp;4,4 end{bmatrix}$ . Number of columns in matrix A must match number of rows in matrix B. | The output of matrix multiplication will be dimensions equal to the number of rows in matrix A by the number of columns in matrix B. | You can cut the matrix into blocks and do matrix multiplication in blocks. | $ begin{bmatrix}A_1&amp;A_2 A_3&amp;A_4 end{bmatrix} begin{bmatrix}B_1&amp;B_2 B_3&amp;B_4 end{bmatrix}= begin{bmatrix}A_1B_1+A_2B_3&amp;A_1B_2+A_2B_4 A_3B_1+A_4B_3&amp;A_3B_2+A_4B_4 end{bmatrix}$ . Note: While cutting it into blocks may not seem useful immediately, it is crucial for high speed computation. In Deep Learning where you are multiplying large matrices together can speed up computation speed immensely by breaking them into blocks so you can fit the blocks into CPU Memory. In fact, this is exactly what pytorch does. . 1st Way . Let&#39;s imagine we have a matrix multiplation . $$AB=C$$ . $C_{34}=$(row 3 of A)$ cdot$(column 4 of B) . $C_{34}=a_{31}b_{14}+a_{32}b_{24}+......$ . $C_{34}= sum limits_{k=1}^n a_{3k}b_{k4}$ . 2nd Way . $$AB=C$$ . The second way to think about it is that matrix $A$ times by the first column of matrix $B$ will give you the first column of $C$. . Matrix $A$ times by the second column of matrix $B$ will give you the second column of $C$. . Now really what we are doing is thinking of columns of $C$ as combinations of columns of $A$ . $A cdot B_{n1} =C_{m1}$ . 3rd Way . $$AB=C$$ . The third way to think about it is that a row of $A$ times matrix $B$ will give you a column of $C$. . Now really what we are doing is thinking of rows of $C$ as combinations of rows of $B$. . 4th Way . If we multiply a column by a row, we get a full sized matrix. We also see that the columns are multiples of the column, and the rows and multiples of the row. This is what we expect as we just discussed above with the combinations of each other. . $ begin{bmatrix}2 3 4 end{bmatrix} begin{bmatrix}1&amp;6 end{bmatrix}= begin{bmatrix}2&amp;12 3&amp;18 4&amp;24 end{bmatrix}$ . $AB=$Sum of (Cols of A) $ cdot$ (Rows of B) . $ begin{bmatrix}2&amp;7 3&amp;8 4&amp;9 end{bmatrix} begin{bmatrix}1&amp;6 0&amp;0 end{bmatrix}$ $=$ $ begin{bmatrix}2 3 4 end{bmatrix} begin{bmatrix}1&amp;6 end{bmatrix}+$ $ begin{bmatrix}7 8 9 end{bmatrix} begin{bmatrix}0&amp;0 end{bmatrix}$ $=$ $ begin{bmatrix}2&amp;12 3&amp;18 4&amp;24 end{bmatrix}$ . This matrix all sit on the same line because they are just multiples. . Inverses . Not all Matrices have inverses. Most important question is whether it&#39;s invertable. If $A^-1$ exists. . $A^{-1}A = I = AA^{-1}$ . Invertable, nonsingular are the good case. . Matrices with no inverse . In the singular case there is no inverse. 2x2 matrix that has no inverse. . $ begin{bmatrix}1&amp;3 2&amp;6 end{bmatrix}$ . Cannot get 1,0, because I can find a vector $x$ with $Ax=0$ . $ begin{bmatrix}1&amp;3 2&amp;6 end{bmatrix} begin{bmatrix}3 -1 end{bmatrix}= begin{bmatrix}0 0 end{bmatrix}$ . But why does this matter? Because if I use these values for X anything I multiply by cannot possibly give the identify matrix, because 0 times anything gives 0. . Matrices with an inverse . $AA^{-1}=I =&gt; begin{bmatrix}1&amp;3 2&amp;7 end{bmatrix} begin{bmatrix}a&amp;b c&amp;d end{bmatrix}= begin{bmatrix}1&amp;0 0&amp;1 end{bmatrix}$ . Now from our matrix multiplication work above we know that $A cdot$column $j$ of $A^{-1} = $column $j$ of $I$ . So we really have a system of 2 equations to solve. . $ begin{bmatrix}1&amp;3 2&amp;7 end{bmatrix} begin{bmatrix}a c end{bmatrix}= begin{bmatrix}1 0 end{bmatrix}$ . $ begin{bmatrix}1&amp;3 2&amp;7 end{bmatrix} begin{bmatrix}b d end{bmatrix}= begin{bmatrix}0 1 end{bmatrix}$ . So we are back to solving systems of equations. . Gauss-Jordan / Find $A^{-1}$ . Solve 2 equations at once . $ begin{bmatrix}1&amp;3 2&amp;7 end{bmatrix} begin{bmatrix}a b end{bmatrix}= begin{bmatrix}1 0 end{bmatrix}$ . $ begin{bmatrix}1&amp;3 2&amp;7 end{bmatrix} begin{bmatrix}c d end{bmatrix}= begin{bmatrix}0 1 end{bmatrix}$ . The gauss-Jordan Method solved both equations at once by created an augmented matrix. . We will now do elimination steps to get the identify on the left. This will convert A to A inverse. . Start with $AI$ | Elimination step subtracting 2 of the first row from the second row | Elinination step subtracting 3 of the second row from the first. | End with $IA^{-1}$ | $ left[ begin{array}{cc|cc} 1 &amp; 3 &amp; 1 &amp; 0 2 &amp; 7 &amp; 0 &amp; 1 end{array} right] =&gt; left[ begin{array}{cc|cc} 1 &amp; 3 &amp; 1 &amp; 0 0 &amp; 1 &amp; -2 &amp; 1 end{array} right] =&gt; left[ begin{array}{cc|cc} 1 &amp; 0 &amp; 7 &amp; -3 0 &amp; 1 &amp; -2 &amp; 1 end{array} right] $ . Naturally we get the inverse because we really reverse the sides of the equation. Just like in algebra if you have -5 on one side, you can move it by putting the inverse on the other side (+5). .",
            "url": "https://isaac-flath.github.io/blog/linear%20algebra/2020/05/11/18.06_3_Multiplication-and-Inverse-Matrices.html",
            "relUrl": "/linear%20algebra/2020/05/11/18.06_3_Multiplication-and-Inverse-Matrices.html",
            "date": " • May 11, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Descriptive Customer Analytics (Part 1)",
            "content": "Intro . Customer Analytics is a valuable tool, but often the basics are overlooked. It can have a huge impact, and it doesn’t always mean complex modeling. It always starts with the basics. Because customer analytics can make such a big impact, I am going to break this into several posts. . In this post (Part 1) I am going to explain a basic descriptive customer analytics framework and how use it to identify the current state.  This understanding allows us to conduct A/B testing so we can measure how our actions impact revenue and by how much. There are many ways to do this, this is just one of them. . In Part 2 I’ll talk about how to build on the descriptive framework to create a predictive model.  A predictive model allows companies to plan proactively instead of reactively. . Background: . Many companies rely on experience and basic summarizing of historical statistics to make decisions.  While both of these are useful tools, a powerful way to successfully sustain long term revenue growth is to add customer analytics to their tool belt.  In this post I am discussing the starting point for analytics, descriptive analytics. . An easy and solid starting point for descriptive analytics is a RFM (Recency, Frequency, Monetary Value) model.  A primary advantage to this is the only data you need is purchase history for each customer, which is information that almost every company tracks.  I’ve outlined the three main steps to creating this framework and using it to increase revenue. . Determine length of customer buying cycle: . The first step is to determine what time range is a good time range to use for a buying cycle.  For a grocery store, maybe we say typically people go grocery shopping once a week.  In that scenario we would use 1 week as our time range for our cycle.  However a company that prints business cards may find a much longer time frame as customers tend to buy 1,000 and repurchase when they get low.  For this example we could use a 6 month cycle.  I encourage people to explore their data and pick a time range based on intuition.  There are better and more complicated ways to figure out this time range, but I encourage people to demonstrate results and revenue growth before circling back with more resources and support from executive management. . Calculate descriptive customer metrics: . Let’s say we choose a 1 month cycle.  The next step is to create and fill in a table to get frequency and monetary Value.  In the table below I outline how we calculate both of those KPIs (Key Performance Indicators). .   January February March Frequency Monetary Value . Example | Rev | Rev | Rev | # of slots purchased in | Average Monetary Value | . Customer A | $102 | $60 | $75 | 3 | =($102+$60+$75) / 3 | . Customer B | $97 | $85 | $0 | 2 | =($85+97) / 2 | . Customer C | $0 | $9852 | $0 | 1 | =($9,852) / 1 | . Once this is done add another column for Recency and fill that in with how long since that last purchase.  This can be days, hours, or seconds since last purchase depending on how far apart customer purchases typically are.  For this simplified version we will use days.  We end up with a table like this: .   Recency Frequency Monetary Value . Customer A | 3 | 3 | $79 | . Customer B | 35 | 2 | $91 | . Customer C | 48 | 1 | $9852 | . Conduct A/B testing: . Now that we have three metrics (RFM) that describe customer buying patterns, we can start using them to run experiments to drive revenue growth.  A/B testing is a simple experiment that you can run to compare two things. You split your customers into two group and make no change to what you are doing to group A, but do something different in group B.  By doing this we can compare the revenue of the two groups to see what impact our efforts had.  If B is better, we will apply that action to the entire customer base.  If not, we scrap the idea and move on.  This is a never-ending process of  testing new ideas. . Here’s a few ideas for A/B tests using RFM to get you started: . Call every customer in group B to see how that impacts revenue.  Does this increase Frequency (F) or average Monetary Value (M) of orders?  Is it worth the additional resources? | Send a marketing email blast to everyone in group B.  How does this change group B’s buying patterns? | What impact does sending a one-time-only 5 % off coupon have? | If we raise the price by 5 % in group B, what is the effect on revenue?  This is especially useful because we can run limited experiments and gauge reaction of price hikes without angering every customer. | If we raise the price on a product that is nearing the end of its product life cycle, do sales increase on the newly released product?  Does a price hike on old products encourage customers to buy newer versions? | Send group A customers to the current website and group B customers to a new website layout.  Monitor sales, traffic, and clicks on each site.  This can help determine the better layout. | .",
            "url": "https://isaac-flath.github.io/blog/customer%20analytics/2020/05/10/Customer-Analytics-Part-1.html",
            "relUrl": "/customer%20analytics/2020/05/10/Customer-Analytics-Part-1.html",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Elimination with Matrices (18.06_L2)",
            "content": "Motivation . One of my goals is to understand more deeply what Neural Networks are doing. Another is to have an easier time understanding and implementing cutting edge academic papers. In order to work toward those goals, I am revisiting the Math behind Neural Networks. This time my goal is to understand intuitively every piece of the material forward and backward - rather than to pass a course on a deadline. . This blog post will be my notes about Lecture 2 from the following course: . Gilbert Strang. 18.06 Linear Algebra. Spring 2010. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. . Goal . The goal is to solve N equations with N unknowns, just like it was with lecture 1. In the first Lecture it was about understanding how to solve them intuitively with linear combinations and matrix multiplication. In reality, that is not a scalable choice when you get to higher and higher dimensions. We need to be able to express these concepts in matrices and be more comfortable with that language and operations.. . Matrix Form . As a recap, we can express a system of equations by listing the equations. x . $x+2y+z=2$ . $3x+8y+z=12$ . $4y+z=2$ . We can write the same thing in matrix form using the formula $Ax=b$. The matrix A is from the system of equation above. . $A = begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix}$ . Elimination . The general idea in elimination is to isolate variables so we can solve the equation. For example, if we have 0x + 0y + 1z = 10, it is very easy to solve for z. We can then plug it into another equation in the series that has z and 1 other unknown to get another value, and so on and so forth. . $E_{21}$ . We are going to start by eliminating the first variable in the second row. Row 2, column 1. This would leave the second row with only 2 unknowns. . The way we do this is we subtract row 1 from row 2. If we substract 3 * 1 from 3, we get 0, so 3 will be the multiplier. . $ begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 0&amp;0&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix} = begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;4&amp;1 end{bmatrix}$ . $E_{31}$ . We then move on to row 3, column 1. Lucky for us it&#39;s already 0 so we can skip this step . $E_{32}$ . We now move on to row 3, column 2. If we can get this to 0, then we have 1 unknown in that column. That&#39;s what we want as that&#39;s easily solvable. . $ begin{bmatrix}1&amp;0&amp;0 0&amp;1&amp;0 0&amp;-2&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;4&amp;1 end{bmatrix} = begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix}$ . How can this fail? . A 0 cannot be in one of the pivot spots. The pivot positions are the diagonals. If that happens, elimination will fail. If you think about it, a 0 being in the pivot spot means that the left side of the equation is 0. Either the right side is also 0 and it gives you no information, or the right side is not 0 and there is no solution (ie 2 = 0 is False). . Of course, there are some tricks that can be added to minimize failures. The most common of these are row exchanges. By swapping the order of the rows, you can potentially solve an equation that would have failed in the original order. . For example, if I were to exchange the 2nd and 3rd rows I would multiply by the following matrix. . $ begin{bmatrix}1&amp;0&amp;0 0&amp;0&amp;1 0&amp;1&amp;0 end{bmatrix}$ . Back Substitution . Let&#39;s recap the steps that were taken. We started with A and through elimination in 2 steps we ended with a new matrix. . $A =&gt; begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix} =&gt; begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;4&amp;1 end{bmatrix} =&gt; begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix} =&gt; U$ . Finish the Equation . You may have realized these matrices represented the left side of the equation. You may have wondered how we can modify the left side of the equation while leaving the right side alone? The answer is that we cannot. What is done on the left side must be applied to the right side. Let&#39;s do that now. . We have done 2 transformations. . The first transformation was subtracting 3 of the first row from the second. | The second transformation was subracting 2 of the second row from the third. | Let&#39;s do that to the the right side . b =&gt; $ begin{bmatrix}2 12 2 end{bmatrix} =&gt; begin{bmatrix}2 6 2 end{bmatrix} =&gt; begin{bmatrix}2 6 -10 end{bmatrix} =&gt; c$ . Let&#39;s put the left and right side of the equations together to see what it looks like. . $Ux = C$ . $ begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix} begin{bmatrix}x y z end{bmatrix}= begin{bmatrix}2 6 -10 end{bmatrix}$ . Final Solution . Intuitive Solution . Great! Let&#39;s translate the $Ux=C$ matrix above back to our systems of equations view just to see if we can see how this transformation helps us. . $x + 2y + z = 2$ . $2y - 2z = 6$ . $5z = -10$ . When we look at it here, we can solve them with some simple algebra starting from the bottom equation. . $5z = -10 ; ; ; mathbf{=&gt;} ; ; ; z = -10/5 ; ; ; mathbf{=&gt;} ; ; ; z = -2$ . $2y - 2z = 6 ; ; ; mathbf{=&gt;} ; ; ; 2y - 2(-2) = 6 ; ; ; mathbf{=&gt;} ; ; ; 2y + 4 = 6 ; ; ; mathbf{=&gt;} ; ; ; 2y = 2 ; ; ; mathbf{=&gt;} ; ; ; y = 1$ . $x+2y+z=2 ; ; ; mathbf{=&gt;} ; ; ;x+2-2=2$$x+2y+z=2 ; ; ; mathbf{=&gt;} ; ; ;x=2$ . Matrix Solution . So first we should ask if we have an intuitive solution, why bother with doing the whole thing in matrix format? Isn&#39;t it the same thing? . And yes, we will be doing the same thing. The reason is scalability and ability to transfer to N equation and N unknowns. There&#39;s a limit to what can be done by hand, and matrix form allows for easier scalability. . Recap . We did several steps intuitively. Let&#39;s recap our elimination steps from above . $E_{21}$ step $ begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 0&amp;0&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix} = begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;4&amp;1 end{bmatrix}$ . $E_{32}$ step $ begin{bmatrix}1&amp;0&amp;0 0&amp;1&amp;0 0&amp;-2&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;4&amp;1 end{bmatrix} = begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix}$ . Simplify into 1 step . Great! Now let&#39;s just combine these so we only have 1 equation. . $E_{21}E_{32}A=U$ $ begin{bmatrix}1&amp;0&amp;0 0&amp;1&amp;0 0&amp;-2&amp;1 end{bmatrix} begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 0&amp;0&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix}= begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix} $ . Now let&#39;s simplify this by multiplying and combining my $E_{21}$ and $E_{31}$ matrices together. We will call the result $E$ . $E_{21}E_{32}=E$ $ begin{bmatrix}1&amp;0&amp;0 0&amp;1&amp;0 0&amp;-2&amp;1 end{bmatrix} begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 0&amp;0&amp;1 end{bmatrix} = begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 6&amp;-2&amp;1 end{bmatrix}$ . Great! Now we can use this to simplify our formula. . $EA=U$ $ begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 6&amp;-2&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix}= begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix}$ .",
            "url": "https://isaac-flath.github.io/blog/linear%20algebra/2020/05/10/18.06_2_EliminationWithMatrices.html",
            "relUrl": "/linear%20algebra/2020/05/10/18.06_2_EliminationWithMatrices.html",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Gradient Descent for Linear Regression",
            "content": "The goal of linear regression is to find parameter values that fit a linear function to data points.  The goodness of fit is measured by a cost function.  The lower the cost, the better the model.  Gradient Descent is a method to minimize a cost function.  Gradient descent is a widely used tool and is used frequently in tuning predictive models.  It’s important to understand the method so you can apply it to various models and are not limited to using black box models. . As I just mentioned, gradient descent is a method to reduce a cost function.  To understand how to minimize a cost function, you need to understand how cost is calculated.  For this post I will be using a very simple example; linear regression with one feature, two data points, and two regression coefficients.  I will use the sum of squares cost function to take the predicted line and slowly change the regression coefficients until the line passes through both points. . . In this example we could easily draw a line through the points without using gradient descent, but if we had more data points this would get trickier.  In the table below we can see what the data looks like that we are working with. . . The tree below illustrates how to solve for cost as well as how to improve the values of $ theta$ to minimize cost.  In the illustration above, $J = a^1 + a^2$ is the cost function we want to minimize.  As we can see, if the regression coefficients ($ theta_0+ theta_1$) do not give a good fit, then the difference between our predicted values and observed values will be large and we will have a high cost ($J$).  For low values, we will have a low cost ($J$).  The figure below shows us how to calculate cost from the regression coefficients ($ theta_0$ and $ theta_1$). . . The second thing this chart shows you is how to improve values of theta.  We used the formulas in the boxes to evaluate $J$, so now we will use the values on the edges to improve the parameter values.  Each regression coefficient has a path up to the cost function.  You get a path value for each $ theta$ on the tree by multiplying the edge values along that path.  For example: . $ theta_1 ; path ;value = x^1 (y^1_{pred} - y^1_{obs}) (1) + x^2 (y^2_{pred} - y^2_{obs}) (1)$ . The last step is to improve the value of $ theta$.  In order to improve the value of $ theta$, we need to multiply the path value by $ alpha$, and subtract that from that $ theta$.  $ alpha$ is a value that determines how large the increments will be taken during optimization.  If you pick an $ alpha$ value that is is too large, you risk missing the local optima.  If you choose an $ alpha$ value that is too small you will be very accurate, but it will be more computationally expensive. With more data points there would be more edges originating at $J$, and with more features there would be more thetas originating from the predicted values, but the same concept can be applied to these more complicated examples. .",
            "url": "https://isaac-flath.github.io/blog/gradient%20descent/2020/05/09/GradientDescentforLinearRegression-P1.html",
            "relUrl": "/gradient%20descent/2020/05/09/GradientDescentforLinearRegression-P1.html",
            "date": " • May 9, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Geometry of Linear Equations (18.06_L1)",
            "content": "#collapse-hide import matplotlib.pyplot as plt from torch import tensor from torch import solve import numpy as np from mpl_toolkits import mplot3d from mpl_toolkits.mplot3d import Axes3D . . Motivation . One of my goals is to understand more deeply what Neural Networks are doing. Another is to have an easier time understanding and implementing cutting edge academic papers. In order to work toward those goals, I am revisiting the Math behind Neural Networks. This time my goal is to understand intuitively every piece of the material forward and backward - rather than to pass a course on a deadline. . This blog post will be my notes about Lecture 1 from the following course: . Gilbert Strang. 18.06 Linear Algebra. Spring 2010. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. . Goal . The goal is to solve N equations with N unknowns. We will start with 2 equations with 2 unknowns, then go to 3 equations with 3 unknowns. We will use an intuitive approach here to understand a bit about how linear equations work. . How do we multiply these together? . Matrix Multiplication (Ax=b) . $ begin{bmatrix} 2 &amp; 5 1 &amp; 3 end{bmatrix}$ $ begin{bmatrix} 1 2 end{bmatrix}$ $=1$ $ begin{bmatrix} 2 1 end{bmatrix}$ $+2$ $ begin{bmatrix} 5 3 end{bmatrix}$ $=$ $ begin{bmatrix} 12 7 end{bmatrix}$ . Ax is a linear combination of columns . 2 equations 2 unknowns . Ok, Let&#39;s look at a couple equations in a few different ways. The solution to these are any values of x and y that make both equations true. . $2x - y = 0$ . $-x + 2y = 3$ . Row Picture . We can take these 2 linear equations and plot them. . $2x - y = 0$ could be writen as $y = 2x$, which is trivial to graph. If we graph them both, we can see visually where they are both true (the intersection). . def plot_equations_2d(x_range,y_dict): for y in y_dict: plt.plot(x, y_dict[y], label=y) plt.xlabel(&#39;x&#39;, color=&#39;#1C2833&#39;) plt.ylabel(&#39;y&#39;, color=&#39;#1C2833&#39;) plt.legend(loc=&#39;upper left&#39;) plt.grid() plt.show() x = tensor(np.linspace(-4,4,100)) y_dict = {&#39;2x-y=0&#39;:2*x, &#39;-x+2y=3&#39;:(3 + x)/2} plot_equations_2d(x,y_dict) . Column Picture . We can rewrite out equations into a different notation, which gives us a more concise view of what is going on. You can see that the top row is the first equation, and the bottom row is the second. Same thing, written differently. . $x$ $ begin{bmatrix} 2 -1 end{bmatrix}$ $+y$ $ begin{bmatrix} -1 2 end{bmatrix}$ $=$ $ begin{bmatrix} 0 3 end{bmatrix}$ . Graphed as Vectors . Now that we see them it in column form, we can graph the vectors . #collapse-hide strt_pts = tensor([[0,0],[2,-1]]) end_pts = tensor([[2,-1],[1,2]]) diff = end_pts - strt_pts plt.ylim([-3, 3]) plt.xlim([-3, 3]) plt.quiver(strt_pts[:,0], strt_pts[:,1], diff[:,0], diff[:,1], angles=&#39;xy&#39;, scale_units=&#39;xy&#39;, scale=1.) plt.show() . . Summed as Vectors . Now that they are represented as vectors. Let&#39;s add 1X + 2Y vectors and see that we get (0,3). Simply make one vector start where the other ends . #collapse-hide strt_pts = tensor([[0,0],[2,-1],[1,1]]) end_pts = tensor([[2,-1],[1,1],[0,3]]) diff = end_pts - strt_pts plt.ylim([-3, 3]) plt.xlim([-3, 3]) plt.quiver(strt_pts[:,0], strt_pts[:,1], diff[:,0], diff[:,1], angles=&#39;xy&#39;, scale_units=&#39;xy&#39;, scale=1.) plt.show() . . Matrix Form AX = b . We can see the same view in matrix notation. Same as the row and column view, just in a nice compressed format. . $ begin{bmatrix} 2 &amp; -1 -1 &amp; 2 end{bmatrix}$ $ begin{bmatrix} x y end{bmatrix}$ $=$ $ begin{bmatrix} 0 3 end{bmatrix}$ . Equations, 3 equations 3 unknowns . $2x - y = 0$ . $-x + 2y -z = -1$ . $-3y + 4z = 4$ . Matrix Form . $A=$ $ begin{bmatrix} 2 &amp; -1 &amp; 0 -1 &amp; 2 &amp; -1 0 &amp; -3 &amp; 4 end{bmatrix}$ $b=$ $ begin{bmatrix} 0 -1 4 end{bmatrix}$ . Row Picture . #collapse-hide fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) x, z = tensor(np.linspace(-8,8,100)), tensor(np.linspace(-8,8,100)) X, Z = np.meshgrid(x,z) Y1 = 2*X Y2 = (-1 + X + Z) / 2 Y3 = (4*Z - 4)/3 ax.plot_surface(X,Y1,Z, alpha=0.5, rstride=100, cstride=100) ax.plot_surface(X,Y2,Z, alpha=0.5, rstride=100, cstride=100) ax.plot_surface(X,Y3,Z, alpha=0.5, rstride=100, cstride=100) plt.show() . . Column Picture . We can create the column picture and graph vectors, just like in 2D space. Graphing in 3D is harder to see, but it&#39;s the same concept. . In this example we can clearly see the solution is x = 0, y = 0, z = 1. . $x$ $ begin{bmatrix} 2 -1 0 end{bmatrix}$ $+y$ $ begin{bmatrix} -1 2 -3 end{bmatrix}$ $+z$ $ begin{bmatrix} 0 -1 4 end{bmatrix}$ $=$ $ begin{bmatrix} 0 -1 4 end{bmatrix}$ . #collapse-hide strt_pts = tensor([[0,0,0],[0,0,0],[0,0,0]]) end_pts = tensor([[2,-1,0],[-1,2,-1],[0,-3,4]]) diff = end_pts - strt_pts fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.set_xlim([-5, 5]) ax.set_ylim([-5, 5]) ax.set_zlim([-5, 5]) plt.quiver(strt_pts[0,0], strt_pts[0,1], strt_pts[0,2], end_pts[0,0], end_pts[0,1], end_pts[0,2]) plt.quiver(strt_pts[1,0], strt_pts[1,1], strt_pts[1,2], end_pts[1,0], end_pts[1,1], end_pts[1,2]) plt.quiver(strt_pts[2,0], strt_pts[2,1], strt_pts[2,2], end_pts[2,0], end_pts[2,1], end_pts[2,2]) plt.show() . . Numpy Solver . Here&#39;s how you can solve the equation using Numpy. . a = np.array([[2, -1, 0], [-1, 2, -1], [0, -3, 4]]) b = np.array([0, -1, 4]) x = np.linalg.solve(a, b) print(x) . [ 0. -0. 1.] . Can I solve Ax = b for every b? . Do the linear combinations of the columns fill 3 dimensional space. . If you have some dimensionality in each direction, then you can. 3 equations with 3 unknowns can fill the 3D space as long as they don&#39;t sit on the same line or plane. . # lets expirament length_b = 20 b = np.array([list(np.random.rand(length_b)*10), list(np.random.rand(length_b)*10), list(np.random.rand(length_b)*10)]) for x in range(0,length_b): x = np.linalg.solve(a, b[:,x]) print(x) . [12.34080396 17.25419494 14.45223124] [4.3199813 5.08051595 5.06761607] [11.26347828 12.97487851 12.22512736] [5.70944029 7.59603383 7.26994807] [ 8.14004194 10.62910222 9.74376037] [2.95913308 5.38708946 4.07602836] [ 5.71158581 10.15204127 9.91490065] [ 9.64601458 14.15432693 11.82836017] [14.30704025 20.64995811 17.69860633] [ 7.83852946 14.51438959 12.77361829] [8.10022402 8.2034441 6.89683312] [4.83294145 4.45944243 3.902063 ] [4.97509585 9.38184883 9.47499918] [4.21358292 5.53636436 5.35998439] [ 7.49769118 11.83894164 10.43770683] [11.07197937 17.24535878 15.22695312] [6.41919127 6.39617205 5.86620567] [10.07150973 11.6354783 11.01565376] [10.97611756 12.95884772 10.99723627] [4.40229263 4.48480965 3.50238492] .",
            "url": "https://isaac-flath.github.io/blog/linear%20algebra/2020/05/09/18.06_1_GeometryOfLinearEquations.html",
            "relUrl": "/linear%20algebra/2020/05/09/18.06_1_GeometryOfLinearEquations.html",
            "date": " • May 9, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Jupyter Notebook Tutorial",
            "content": "Top 3 uses: . Exploratory analysis, model creation, data science, any kind of coding that require lots of rapid expiramentation and iteration. | Tutorials, guides, and blogs (like this one). Because you have a great mix of text functionality with code, they work really well for tutorials and guides. Rather than having static images, or code snippits that have to get updated each iteration, the code is part of the guide and it really simplifies the process. Notebooks can be exported directly to html and be opened in any browser to give to people. With the easy conversion to html, naturally it&#39;s easy to post them on a web page. | Technical presentations of results. You can have the actual code analysis done, with text explanations. Excess code can be collapsed so that if someone asks really detailed questions you can expand and have every piece of detail. Changes to the analysis are in the presentation so no need to save and put static images in other documents | Cell Types . A cell can be 3 different types. The most useful are code cells and markdown cells. . Code Cells . - Code cells run code The next few cells are examples of code cells - While the most common application is Python, you can set up environments easily to use R, swift, and other languages within jupyter notebooks . Markdown Cells . - This cell is a markdown cell. It is really nice for adding details and text explanations in where a code comment is not enough - They have all the normal markdown functionality, plus more. For example, I can write any technical or mathy stuff using latex, or create html tables in markdown or html. - I can also make markdown tables. . Latex Formulas . $$ begin{bmatrix}w_1&amp;w_2&amp;w_3&amp;w_4&amp;w_5 x_1&amp;x_2&amp;x_3&amp;x_4&amp;x_5 y_1&amp;y_2&amp;y_3&amp;y_4&amp;y_5 z_1&amp;z_2&amp;z_3&amp;z_4&amp;z_5 end{bmatrix}$$ . $ begin{align} frac{dy}{du} &amp;= f&#39;(u) = e^u = e^{ sin(x^2)}, frac{du}{dv} &amp;= g&#39;(v) = cos v = cos(x^2), frac{dv}{dx} &amp;= h&#39;(x) = 2x. end{align}$ . Markdown Table . This is a table for demos . perc | 55% | 22% | 23% | 12% | 53% | . qty | 23 | 19 | 150 | 9 | 92 | . #collapse-hide import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns pd.options.display.max_columns = None pd.options.display.max_rows = None %matplotlib inline . . Running Code . Naturally you can run code cells and print to the Jupyter Notebook . for x in range(0,5): print(x*10) . 0 10 20 30 40 . DataFrames . iris = sns.load_dataset(&#39;iris&#39;) iris[iris.petal_length &gt; 6] . sepal_length sepal_width petal_length petal_width species . 105 | 7.6 | 3.0 | 6.6 | 2.1 | virginica | . 107 | 7.3 | 2.9 | 6.3 | 1.8 | virginica | . 109 | 7.2 | 3.6 | 6.1 | 2.5 | virginica | . 117 | 7.7 | 3.8 | 6.7 | 2.2 | virginica | . 118 | 7.7 | 2.6 | 6.9 | 2.3 | virginica | . 122 | 7.7 | 2.8 | 6.7 | 2.0 | virginica | . 130 | 7.4 | 2.8 | 6.1 | 1.9 | virginica | . 131 | 7.9 | 3.8 | 6.4 | 2.0 | virginica | . 135 | 7.7 | 3.0 | 6.1 | 2.3 | virginica | . Plotting . Below we are going to make a few graphs to get the point accross. Naturally, each graph can be accompanied with a markdown cell that gives context and explains the value of that graph. . Line Chart . # evenly sampled time at 200ms intervals t = np.arange(0., 5., 0.2) # red dashes, blue squares and green triangles plt.plot(t, t, &#39;r--&#39;, t, t**2, &#39;bs&#39;, t, t**3, &#39;g^&#39;) plt.show() . Scatter Plot . Sometimes we will want to display a graph, but may not want all the code and details to be immediately visable. In these examples we can create a scatter plot like below, but collapse the code cell. . This is great when you want to show a graph and explain it, but the details of how the graph was created aren&#39;t that important. . #collapse-hide data = {&#39;a&#39;: np.arange(50), &#39;c&#39;: np.random.randint(0, 50, 50), &#39;d&#39;: np.random.randn(50)} data[&#39;b&#39;] = data[&#39;a&#39;] + 10 * np.random.randn(50) data[&#39;d&#39;] = np.abs(data[&#39;d&#39;]) * 100 plt.scatter(&#39;a&#39;, &#39;b&#39;, c=&#39;c&#39;, s=&#39;d&#39;, data=data) plt.xlabel(&#39;entry a&#39;) plt.ylabel(&#39;entry b&#39;) plt.show() . . Categorical Plot . We can create subplots to have multiple plots show up. This can be especially helpful when showing lots of the same information, or showing how 2 different metrics are related or need to be analyzed together . #collapse-hide names = [&#39;group_a&#39;, &#39;group_b&#39;, &#39;group_c&#39;] values = [1, 10, 100] plt.figure(figsize=(9, 3)) plt.subplot(131) plt.bar(names, values) plt.subplot(132) plt.scatter(names, values) plt.subplot(133) plt.plot(names, values) plt.suptitle(&#39;Categorical Plotting&#39;) plt.show() . . Stack Traces . When you run into an error, by default jupyter notebooks give you whatever the error message is, but also the entire stack trace. . There is a debug functionality, but I find that these stack traces and jupyter cells work even better than a debugger. I can break my code into as many cells as I want and run things interactively. Here&#39;s a few examples of stack traces. . Matrix Multiplication Good . Now we are going to show an example of errors where the stack trace isn&#39;t as simple. Suppose we are trying to multiply 2 arrays together (matrix multiplication). . a = np.array([ [1,2,4], [3,4,5], [5,6,7] ]) b = np.array([ [11,12,14], [31,14,15], [23,32,23] ]) a@b . array([[165, 168, 136], [272, 252, 217], [402, 368, 321]]) . Matrix Multiplication Bad . Now if it errors because the columns from matrix a don&#39;t match the rows from matrix b, we will get an error as matrix multiplication is impossible with those matrices. We see the same idea s the above for loop, stack trace with error and arrow pointing at the line that failed . # here&#39;s another a = np.array([ [1,2,4], [3,4,5], [5,6,7] ]) b = np.array([ [11,12,14], [31,14,15] ]) a@b . ValueError Traceback (most recent call last) &lt;ipython-input-8-52272ca444fc&gt; in &lt;module&gt; 9 [31,14,15] 10 ]) &gt; 11 a@b ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 3) . Second Layer of Bad . But what if the line we call isn&#39;t what fails? What if what I run works, but the function underneath fails? . In these example, you see the entire trace. It starts with are arrow at what you ran that errored. It then shows an arrow that your code called that caused the error, so you can track all the way back to the source. Here&#39;s how it shows a two step stack trace, but it can be as long as needed. . def matmul(a,b): c = a@b return c . matmul(a,b) . ValueError Traceback (most recent call last) &lt;ipython-input-10-7853c1c27063&gt; in &lt;module&gt; -&gt; 1 matmul(a,b) &lt;ipython-input-9-1c8b6b954779&gt; in matmul(a, b) 1 def matmul(a,b): -&gt; 2 c = a@b 3 return c ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 3) . Magic Commands . Magic commands are special commands for Juptyer Notebooks. They give you incredible functionality and you will likley find the experience very frustrating without them. A few that I use often are: . ? | put a question mark or 2 after a function or method to get the documentation. ?? gives more detail than ?. I can also use it to wild card search modules for functions. | shift tab | when you are writing something holding shift + tab will open a mini popup with the documentation for that thing. It may be a funciton, method, or module. | %who or %whos or %who_ls | These are all variants that list the objects and variables. I prefer %whos most of the time | %history | This allows you to look at the last pieces of code that you ran | $$ | wrapping latex code in dollar signs in a markdown cell renders latex code in markdown cells | ! | putting ! at the beginning of a line makes it run that in terminal. For example !ls | grep .csv | %time | I can use this to time the execution of things | . np.*array*? . np.array np.array2string np.array_equal np.array_equiv np.array_repr np.array_split np.array_str np.asanyarray np.asarray np.asarray_chkfinite np.ascontiguousarray np.asfarray np.asfortranarray np.broadcast_arrays np.chararray np.compare_chararrays np.get_array_wrap np.ndarray np.numarray np.recarray . np.array_equal?? . Signature: np.array_equal(a1, a2) Source: @array_function_dispatch(_array_equal_dispatcher) def array_equal(a1, a2): &#34;&#34;&#34; True if two arrays have the same shape and elements, False otherwise. Parameters - a1, a2 : array_like Input arrays. Returns - b : bool Returns True if the arrays are equal. See Also -- allclose: Returns True if two arrays are element-wise equal within a tolerance. array_equiv: Returns True if input arrays are shape consistent and all elements equal. Examples -- &gt;&gt;&gt; np.array_equal([1, 2], [1, 2]) True &gt;&gt;&gt; np.array_equal(np.array([1, 2]), np.array([1, 2])) True &gt;&gt;&gt; np.array_equal([1, 2], [1, 2, 3]) False &gt;&gt;&gt; np.array_equal([1, 2], [1, 4]) False &#34;&#34;&#34; try: a1, a2 = asarray(a1), asarray(a2) except Exception: return False if a1.shape != a2.shape: return False return bool(asarray(a1 == a2).all()) File: ~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py Type: function . a = np.array(np.random.rand(512,512)) b = np.array(np.random.rand(512,512)) %time for i in range(0,20): a@b . CPU times: user 198 ms, sys: 2.68 ms, total: 200 ms Wall time: 34.8 ms . %whos . Variable Type Data/Info a ndarray 512x512: 262144 elems, type `float64`, 2097152 bytes (2.0 Mb) b ndarray 512x512: 262144 elems, type `float64`, 2097152 bytes (2.0 Mb) data dict n=4 i int 19 iris DataFrame sepal_length sepal_&lt;...&gt; 1.8 virginica matmul function &lt;function matmul at 0x1a169cc680&gt; names list n=3 np module &lt;module &#39;numpy&#39; from &#39;/Us&lt;...&gt;kages/numpy/__init__.py&#39;&gt; pd module &lt;module &#39;pandas&#39; from &#39;/U&lt;...&gt;ages/pandas/__init__.py&#39;&gt; plt module &lt;module &#39;matplotlib.pyplo&lt;...&gt;es/matplotlib/pyplot.py&#39;&gt; sns module &lt;module &#39;seaborn&#39; from &#39;/&lt;...&gt;ges/seaborn/__init__.py&#39;&gt; t ndarray 25: 25 elems, type `float64`, 200 bytes values list n=3 x int 4 . %history -l 5 . matmul(a,b) np.*array*? np.array_equal?? a = np.array(np.random.rand(512,512)) b = np.array(np.random.rand(512,512)) %time for i in range(0,20): a@b %whos . Jupyter Extensions . There are many extensions to Jupyter Notebooks. After all a jupyter notebook is just a JSON file, so you can read the JSON in and manipulate and transform things however you want! There are many features, such as variable explorers, auto code timers, and more - but I find I that most are unneccesary. About half the people I talk to don&#39;t use any, and the other half use several. . NBDEV . NBdev is a jupyter extension/python library that allows you to do full development projects in Jupyter Notebooks. There have been books and libraries written entirely in Jupyter notebooks, including testing frameworks and unit testing that goes with them. A common misconception is that Jupyter notebooks cannot be used for that, though many people already have. . . There are many features NBdev adds. Here&#39;s a few. . Using notebooks written like this, nbdev can create and run any of the following with a single command: . Searchable, hyperlinked documentation; any word you surround in backticks will by automatically hyperlinked to the appropriate documentation | Cells in jupyter notebook marked with #export will be exported automatically to a python module | Python modules, following best practices such as automatically defining all (more details) with your exported functions, classes, and variables | Pip installers (uploaded to pypi for you) | Tests (defined directly in your notebooks, and run in parallel). | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks | . I reccomend checking them out for more detail https://github.com/fastai/nbdev .",
            "url": "https://isaac-flath.github.io/blog/jupyter/2020/05/07/JupyterNotebooks.html",
            "relUrl": "/jupyter/2020/05/07/JupyterNotebooks.html",
            "date": " • May 7, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Heirarchical Loss Function",
            "content": "Intro . For this post, I am going to make a modification to the loss function. The goal will be to weight losses differently. This post is a work in progress as I try to figure stuff out. . In this blog we are going to do an image classification to take pet pictures and predict the breed. Normally, each class is treated the same when calculating the loss function. In this post I will explore a change to take into account weighting classes differently. For example, If a picture is a maincoon, predicting a sphynx (wrong type of cat) is less egregious of an error than predicting pitbull. I want my loss function to reflect this. . I am going to skim over loading the data and training the model, feel free to see my Fine Grain Image Classifier post from Jun-27-2020 if you want more detail on those aspect. . Setup . Library Import and Dataset Download . from fastai2.vision.all import * seed = 42 # Download and get path for dataseet path = untar_data(URLs.PETS) #Sample dataset from fastai2 img = (path/&#39;images&#39;).ls()[0] . Data Prep . Load the Data in a data block, take a look at the classes and scale the images down a bit so the problem isn&#39;t too easy. . pets = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, splitter= RandomSplitter(valid_pct = 0.2, seed=seed), get_y= using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;),&#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(min_scale = 0.9,size=56) ) . dls = pets.dataloaders(path/&quot;images&quot;,bs=64) dls.show_batch() . /home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . # first 12 classes are casts, next 25 are dogs pet_type = [&#39;cat&#39;]*12+[&#39;dog&#39;]*25 . Custom Stuff . Metrics . I need to measure the accuracy at the 2 levels in the heirarchy. The goal is that I can impact them differently by modifying the weights in my loss function. This will be the metrics to determine is this approach is working or not. . def accuracy_breed(inp, targ, axis=-1): &quot;Compute accuracy with `targ` when `pred` is bs * n_classes&quot; pred,targ = flatten_check(inp.argmax(dim=axis), targ) return (pred == targ).float().mean() def accuracy_species(inp, targ, axis=-1): temp = [torch.argmax(x) for x in inp] new_inp = tensor([(x &gt; 11).int() for x in temp]) new_targ = tensor([(x &gt; 11).int() for x in targ]) return (new_inp == new_targ).float().mean() . Loss Function . The loss functions are what will actually be optimized. I am summing a cross entropy loss as the 2 levels of heirarchy. I then use a weight to change the proportion of which level I use. . from torch.nn.functional import nll_loss,log_softmax def cross_entropy_species(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;): input_p = torch.softmax(input,dim=-1) return nll_loss(torch.log(input_p), target, None, None, ignore_index, None, reduction) def cross_entropy_breed(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;): # softmax to convert scores to probabilities input_p = torch.softmax(input,dim=1) # changes from many classes down to 2 classes for both input and target # Sum the probabilities for all the cat breeds to get probability it&#39;s a cat. Same for dog cats = torch.sum(input_p[:,0:12],dim=1).view(input_p.shape[0],1) dogs = torch.sum(input_p[:,12:37],dim=1).view(input_p.shape[0],1) # format new inputs and new targets for 2 classes new_input = torch.cat([cats,dogs],-1) new_target = TensorCategory((target &gt; 11).long(),device=&#39;cuda:0&#39;) return nll_loss(torch.log(new_input), new_target, None, None, ignore_index, None, reduction) def final_loss(input, target, w=1, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;): ce_species = cross_entropy_species(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;) ce_breed = cross_entropy_breed(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;) # weighted proportion of these, w should be a number between 0 and 1 ideally. return w*ce_species+(1-w)*ce_breed . Expiraments . Run tests for different weights. Run 5 at each weight so we can see how it performs with random initializations (ie getting stuck in local minimums). . from functools import partial weights = np.array(range(0,21))/20 results = pd.DataFrame() id_num = 1 for w in weights: for i in range(0,5): print(w) tmp_loss = partial(final_loss,w=w) learn = cnn_learner(dls, resnet18, metrics=[accuracy_breed,accuracy_species], pretrained=True, loss_func = tmp_loss) learn.fine_tune(3) df = pd.DataFrame(learn.recorder.values) df.columns = learn.recorder.metric_names[1:-1] df[&#39;w&#39;] = w df[&#39;id&#39;] = id_num results = results.append(df) id_num = id_num + 1 learn = None torch.cuda.empty_cache() . 0.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.608369 | 0.485310 | 0.032476 | 0.743572 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.477966 | 0.370505 | 0.043978 | 0.818674 | 00:36 | . 1 | 0.386495 | 0.313353 | 0.041272 | 0.850474 | 00:36 | . 2 | 0.322463 | 0.299790 | 0.043302 | 0.863329 | 00:37 | . 0.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.624959 | 0.496182 | 0.043978 | 0.734100 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.483409 | 0.356743 | 0.043978 | 0.826793 | 00:38 | . 1 | 0.387003 | 0.318895 | 0.051421 | 0.847767 | 00:36 | . 2 | 0.319544 | 0.302874 | 0.058187 | 0.856563 | 00:37 | . 0.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.634777 | 0.483592 | 0.032476 | 0.739513 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.475056 | 0.379465 | 0.033829 | 0.812585 | 00:36 | . 1 | 0.385135 | 0.310869 | 0.047361 | 0.859269 | 00:37 | . 2 | 0.329891 | 0.302679 | 0.046685 | 0.857916 | 00:36 | . 0.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.621033 | 0.484860 | 0.044655 | 0.738160 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.463823 | 0.384365 | 0.052097 | 0.798376 | 00:37 | . 1 | 0.377218 | 0.310583 | 0.050068 | 0.851827 | 00:37 | . 2 | 0.326142 | 0.299156 | 0.056157 | 0.859946 | 00:37 | . 0.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.631593 | 0.478884 | 0.039242 | 0.729364 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.483503 | 0.371758 | 0.039919 | 0.820027 | 00:37 | . 1 | 0.391198 | 0.322902 | 0.038566 | 0.844384 | 00:37 | . 2 | 0.320347 | 0.306178 | 0.037889 | 0.858593 | 00:37 | . 0.05 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.823117 | 0.662975 | 0.075101 | 0.744249 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.670503 | 0.523395 | 0.131935 | 0.822057 | 00:37 | . 1 | 0.544042 | 0.431288 | 0.200271 | 0.857239 | 00:37 | . 2 | 0.473669 | 0.423128 | 0.221245 | 0.863329 | 00:36 | . 0.05 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.843841 | 0.665078 | 0.066306 | 0.730041 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.666394 | 0.526876 | 0.106225 | 0.821380 | 00:37 | . 1 | 0.543386 | 0.442043 | 0.186739 | 0.859946 | 00:37 | . 2 | 0.470841 | 0.433636 | 0.215156 | 0.859269 | 00:37 | . 0.05 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.843284 | 0.667888 | 0.066982 | 0.713802 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.675478 | 0.507734 | 0.134641 | 0.821380 | 00:37 | . 1 | 0.553605 | 0.435599 | 0.197564 | 0.866712 | 00:36 | . 2 | 0.474729 | 0.417160 | 0.221922 | 0.870095 | 00:37 | . 0.05 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.814559 | 0.634952 | 0.084574 | 0.753045 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.673761 | 0.505092 | 0.154939 | 0.820027 | 00:37 | . 1 | 0.547948 | 0.436672 | 0.205683 | 0.864005 | 00:37 | . 2 | 0.472081 | 0.419146 | 0.212449 | 0.872801 | 00:36 | . 0.05 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.832394 | 0.647693 | 0.085927 | 0.740866 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.674024 | 0.513490 | 0.139378 | 0.828823 | 00:37 | . 1 | 0.546988 | 0.431611 | 0.217185 | 0.853857 | 00:37 | . 2 | 0.463304 | 0.417156 | 0.230717 | 0.861976 | 00:38 | . 0.1 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.028380 | 0.794604 | 0.125169 | 0.738160 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.808646 | 0.597925 | 0.228011 | 0.839648 | 00:37 | . 1 | 0.654278 | 0.513786 | 0.300406 | 0.868065 | 00:37 | . 2 | 0.568267 | 0.495825 | 0.314614 | 0.874831 | 00:37 | . 0.1 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.012370 | 0.784275 | 0.118403 | 0.729364 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.804162 | 0.609322 | 0.228687 | 0.825440 | 00:36 | . 1 | 0.641239 | 0.508337 | 0.317321 | 0.873478 | 00:37 | . 2 | 0.558782 | 0.494046 | 0.328146 | 0.880920 | 00:36 | . 0.1 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.010802 | 0.774311 | 0.131258 | 0.751691 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.800170 | 0.600636 | 0.211096 | 0.841001 | 00:37 | . 1 | 0.647214 | 0.526570 | 0.299729 | 0.859269 | 00:37 | . 2 | 0.546728 | 0.514732 | 0.311908 | 0.861976 | 00:36 | . 0.1 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.016842 | 0.774543 | 0.119080 | 0.747632 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.799683 | 0.607935 | 0.236806 | 0.823410 | 00:37 | . 1 | 0.666906 | 0.536426 | 0.296346 | 0.847767 | 00:37 | . 2 | 0.563323 | 0.515809 | 0.303112 | 0.861299 | 00:37 | . 0.1 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.030700 | 0.788971 | 0.116373 | 0.732747 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.800689 | 0.596558 | 0.220568 | 0.845061 | 00:36 | . 1 | 0.652220 | 0.508233 | 0.292963 | 0.869418 | 00:37 | . 2 | 0.556286 | 0.485374 | 0.324087 | 0.875507 | 00:36 | . 0.15 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.204711 | 0.898807 | 0.156969 | 0.731394 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.940931 | 0.727985 | 0.254398 | 0.828146 | 00:36 | . 1 | 0.776746 | 0.622834 | 0.322733 | 0.859269 | 00:37 | . 2 | 0.660291 | 0.598933 | 0.338295 | 0.877537 | 00:37 | . 0.15 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.195785 | 0.901621 | 0.156292 | 0.742896 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.948710 | 0.721946 | 0.262517 | 0.828823 | 00:36 | . 1 | 0.772252 | 0.611095 | 0.338295 | 0.859269 | 00:36 | . 2 | 0.649322 | 0.588588 | 0.351827 | 0.865359 | 00:36 | . 0.15 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.201084 | 0.887853 | 0.173207 | 0.748309 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.940567 | 0.709384 | 0.257104 | 0.821380 | 00:37 | . 1 | 0.768750 | 0.603334 | 0.338295 | 0.859946 | 00:37 | . 2 | 0.654803 | 0.578260 | 0.355886 | 0.867388 | 00:37 | . 0.15 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.208346 | 0.880094 | 0.180650 | 0.755074 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.917859 | 0.682157 | 0.295670 | 0.859269 | 00:37 | . 1 | 0.761364 | 0.603745 | 0.332882 | 0.867388 | 00:37 | . 2 | 0.639739 | 0.581207 | 0.351150 | 0.872124 | 00:37 | . 0.15 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.209788 | 0.892329 | 0.149526 | 0.742896 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.934193 | 0.704706 | 0.271313 | 0.834912 | 00:37 | . 1 | 0.763844 | 0.601783 | 0.343031 | 0.867388 | 00:37 | . 2 | 0.652547 | 0.576504 | 0.360622 | 0.872124 | 00:37 | . 0.2 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.372305 | 0.998853 | 0.197564 | 0.767930 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.042876 | 0.802067 | 0.280785 | 0.828146 | 00:37 | . 1 | 0.870905 | 0.691230 | 0.365359 | 0.864682 | 00:37 | . 2 | 0.752739 | 0.662253 | 0.379567 | 0.877537 | 00:37 | . 0.2 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.372441 | 1.016332 | 0.177943 | 0.747632 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.056653 | 0.818558 | 0.268606 | 0.834235 | 00:37 | . 1 | 0.856927 | 0.683783 | 0.366035 | 0.873478 | 00:37 | . 2 | 0.737668 | 0.661584 | 0.370095 | 0.879567 | 00:37 | . 0.2 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.388116 | 1.001921 | 0.182003 | 0.755074 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.068509 | 0.803566 | 0.285521 | 0.834235 | 00:37 | . 1 | 0.868781 | 0.684169 | 0.352503 | 0.858593 | 00:36 | . 2 | 0.743331 | 0.664095 | 0.366712 | 0.876861 | 00:37 | . 0.2 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.387226 | 1.009751 | 0.182679 | 0.773342 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.051160 | 0.775238 | 0.298376 | 0.850474 | 00:37 | . 1 | 0.859415 | 0.666100 | 0.366712 | 0.880244 | 00:37 | . 2 | 0.740217 | 0.643835 | 0.387686 | 0.880920 | 00:36 | . 0.2 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.383159 | 1.017055 | 0.179296 | 0.763870 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.050643 | 0.799929 | 0.286874 | 0.843031 | 00:37 | . 1 | 0.851690 | 0.692005 | 0.353857 | 0.863329 | 00:37 | . 2 | 0.727401 | 0.663250 | 0.378890 | 0.872801 | 00:36 | . 0.25 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.542990 | 1.127748 | 0.207037 | 0.750338 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.168357 | 0.909275 | 0.307848 | 0.834235 | 00:37 | . 1 | 0.953239 | 0.777908 | 0.389039 | 0.873478 | 00:37 | . 2 | 0.815091 | 0.748984 | 0.401894 | 0.876184 | 00:37 | . 0.25 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.571853 | 1.114848 | 0.194858 | 0.757781 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.182769 | 0.896103 | 0.303112 | 0.832882 | 00:37 | . 1 | 0.967641 | 0.772979 | 0.377537 | 0.872124 | 00:37 | . 2 | 0.826857 | 0.746550 | 0.381597 | 0.874154 | 00:36 | . 0.25 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.568540 | 1.123630 | 0.200271 | 0.744926 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.186586 | 0.893305 | 0.306495 | 0.838972 | 00:37 | . 1 | 0.976547 | 0.772123 | 0.375507 | 0.877537 | 00:36 | . 2 | 0.820513 | 0.745009 | 0.389716 | 0.882273 | 00:37 | . 0.25 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.561743 | 1.153941 | 0.196211 | 0.724628 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.185727 | 0.922488 | 0.285521 | 0.839648 | 00:37 | . 1 | 0.974151 | 0.779154 | 0.355210 | 0.854533 | 00:37 | . 2 | 0.837362 | 0.747603 | 0.382273 | 0.872124 | 00:37 | . 0.25 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.560485 | 1.138875 | 0.198917 | 0.742219 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.204598 | 0.911750 | 0.293640 | 0.829499 | 00:37 | . 1 | 0.977880 | 0.782452 | 0.369418 | 0.863329 | 00:36 | . 2 | 0.836801 | 0.750208 | 0.384980 | 0.875507 | 00:36 | . 0.3 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.707528 | 1.248888 | 0.207037 | 0.732070 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.309080 | 0.995432 | 0.301083 | 0.835589 | 00:37 | . 1 | 1.057639 | 0.862132 | 0.363329 | 0.872124 | 00:36 | . 2 | 0.914748 | 0.829543 | 0.372801 | 0.879567 | 00:36 | . 0.3 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.736965 | 1.240410 | 0.207037 | 0.761840 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.312297 | 0.994160 | 0.332882 | 0.841678 | 00:37 | . 1 | 1.081087 | 0.849271 | 0.393099 | 0.874831 | 00:37 | . 2 | 0.922378 | 0.823370 | 0.414750 | 0.878890 | 00:37 | . 0.3 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.752068 | 1.250638 | 0.194181 | 0.763193 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.305790 | 1.013971 | 0.304466 | 0.841678 | 00:37 | . 1 | 1.064948 | 0.857283 | 0.371448 | 0.871448 | 00:37 | . 2 | 0.915530 | 0.829952 | 0.383627 | 0.876184 | 00:37 | . 0.3 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.710413 | 1.239413 | 0.217862 | 0.771989 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.322684 | 1.015339 | 0.299053 | 0.843708 | 00:37 | . 1 | 1.070804 | 0.874287 | 0.357240 | 0.859946 | 00:36 | . 2 | 0.923676 | 0.836354 | 0.386333 | 0.876184 | 00:37 | . 0.3 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.734363 | 1.277092 | 0.194181 | 0.746279 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.322425 | 1.003118 | 0.295670 | 0.828823 | 00:36 | . 1 | 1.084552 | 0.876272 | 0.372801 | 0.873478 | 00:36 | . 2 | 0.917708 | 0.841883 | 0.388363 | 0.882273 | 00:37 | . 0.35 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.929933 | 1.355661 | 0.210419 | 0.748985 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.418954 | 1.092692 | 0.306495 | 0.829499 | 00:37 | . 1 | 1.176513 | 0.936602 | 0.396482 | 0.863329 | 00:37 | . 2 | 1.015720 | 0.897315 | 0.413396 | 0.872124 | 00:36 | . 0.35 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.916997 | 1.356515 | 0.209743 | 0.742896 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.437846 | 1.101117 | 0.317997 | 0.844384 | 00:37 | . 1 | 1.181034 | 0.942642 | 0.399865 | 0.866712 | 00:37 | . 2 | 1.011137 | 0.900310 | 0.403248 | 0.876861 | 00:36 | . 0.35 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.949436 | 1.356328 | 0.216509 | 0.763870 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.461935 | 1.120919 | 0.310555 | 0.837618 | 00:37 | . 1 | 1.193816 | 0.942734 | 0.384980 | 0.863329 | 00:38 | . 2 | 0.998023 | 0.906454 | 0.399188 | 0.864005 | 00:37 | . 0.35 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.921862 | 1.374400 | 0.213802 | 0.748985 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.443368 | 1.101771 | 0.315291 | 0.815968 | 00:37 | . 1 | 1.187037 | 0.946179 | 0.390392 | 0.873478 | 00:37 | . 2 | 1.012338 | 0.899724 | 0.406631 | 0.872801 | 00:37 | . 0.35 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.896226 | 1.388788 | 0.207713 | 0.750338 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.419015 | 1.098286 | 0.314614 | 0.843708 | 00:37 | . 1 | 1.178550 | 0.917286 | 0.410014 | 0.875507 | 00:37 | . 2 | 1.002859 | 0.891514 | 0.410014 | 0.882273 | 00:37 | . 0.4 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.100758 | 1.496793 | 0.201624 | 0.734777 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.554024 | 1.194201 | 0.320027 | 0.831529 | 00:37 | . 1 | 1.288655 | 1.023650 | 0.413396 | 0.873478 | 00:37 | . 2 | 1.090596 | 0.992833 | 0.416779 | 0.878890 | 00:37 | . 0.4 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.092079 | 1.472220 | 0.224628 | 0.757104 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.547134 | 1.180309 | 0.328823 | 0.844384 | 00:37 | . 1 | 1.282384 | 1.008919 | 0.390392 | 0.872801 | 00:37 | . 2 | 1.092271 | 0.975074 | 0.412043 | 0.878214 | 00:37 | . 0.4 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.078680 | 1.483194 | 0.217185 | 0.739513 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.566993 | 1.200589 | 0.336265 | 0.834235 | 00:37 | . 1 | 1.307330 | 1.043954 | 0.407307 | 0.859946 | 00:37 | . 2 | 1.088244 | 1.000872 | 0.420162 | 0.870095 | 00:37 | . 0.4 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.085802 | 1.479658 | 0.215832 | 0.744926 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.564929 | 1.222082 | 0.320027 | 0.817321 | 00:36 | . 1 | 1.296267 | 1.039866 | 0.388363 | 0.873478 | 00:38 | . 2 | 1.113197 | 0.998180 | 0.412720 | 0.868742 | 00:36 | . 0.4 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.107252 | 1.506865 | 0.196211 | 0.744249 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.567821 | 1.191365 | 0.326793 | 0.832882 | 00:36 | . 1 | 1.290905 | 1.035527 | 0.384980 | 0.862652 | 00:37 | . 2 | 1.116125 | 0.999690 | 0.401218 | 0.874154 | 00:37 | . 0.45 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.273743 | 1.616927 | 0.223951 | 0.753045 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.695131 | 1.288442 | 0.351150 | 0.829499 | 00:37 | . 1 | 1.383734 | 1.111375 | 0.411367 | 0.864005 | 00:36 | . 2 | 1.198852 | 1.084545 | 0.424222 | 0.873478 | 00:37 | . 0.45 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.275370 | 1.594491 | 0.223275 | 0.747632 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.689565 | 1.287953 | 0.338295 | 0.839648 | 00:37 | . 1 | 1.420274 | 1.110631 | 0.401894 | 0.856563 | 00:37 | . 2 | 1.204680 | 1.067348 | 0.418133 | 0.874154 | 00:37 | . 0.45 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.257897 | 1.625519 | 0.212449 | 0.751691 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.662211 | 1.292337 | 0.340325 | 0.838295 | 00:36 | . 1 | 1.389873 | 1.110648 | 0.418809 | 0.865359 | 00:37 | . 2 | 1.172078 | 1.072133 | 0.426252 | 0.870095 | 00:37 | . 0.45 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.269518 | 1.593601 | 0.221922 | 0.752368 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.690343 | 1.303310 | 0.317997 | 0.824763 | 00:37 | . 1 | 1.404830 | 1.097163 | 0.423545 | 0.872801 | 00:37 | . 2 | 1.209087 | 1.057741 | 0.422192 | 0.868742 | 00:37 | . 0.45 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.289545 | 1.615383 | 0.226658 | 0.744249 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.695525 | 1.314701 | 0.338972 | 0.829499 | 00:37 | . 1 | 1.402953 | 1.118837 | 0.396482 | 0.866035 | 00:37 | . 2 | 1.184489 | 1.067961 | 0.425575 | 0.872801 | 00:37 | . 0.5 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.452419 | 1.755479 | 0.205007 | 0.746279 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.814753 | 1.414594 | 0.330176 | 0.829499 | 00:37 | . 1 | 1.496663 | 1.223859 | 0.397835 | 0.868065 | 00:37 | . 2 | 1.290893 | 1.186765 | 0.420839 | 0.870771 | 00:37 | . 0.5 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.436152 | 1.733347 | 0.227334 | 0.763870 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.835567 | 1.390087 | 0.338295 | 0.826116 | 00:37 | . 1 | 1.503260 | 1.199302 | 0.418133 | 0.863329 | 00:37 | . 2 | 1.278899 | 1.150436 | 0.434371 | 0.876184 | 00:37 | . 0.5 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.458572 | 1.732656 | 0.221922 | 0.735453 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.813781 | 1.426824 | 0.319350 | 0.816644 | 00:37 | . 1 | 1.503715 | 1.205033 | 0.402571 | 0.865359 | 00:36 | . 2 | 1.292959 | 1.146333 | 0.423545 | 0.868065 | 00:37 | . 0.5 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.461262 | 1.764038 | 0.212449 | 0.740866 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.837844 | 1.410658 | 0.343031 | 0.833559 | 00:37 | . 1 | 1.499308 | 1.200701 | 0.400541 | 0.865359 | 00:37 | . 2 | 1.291889 | 1.165429 | 0.420839 | 0.872801 | 00:37 | . 0.5 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.428308 | 1.740628 | 0.213126 | 0.755074 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.824184 | 1.428657 | 0.328146 | 0.836265 | 00:37 | . 1 | 1.512689 | 1.212721 | 0.397158 | 0.851827 | 00:37 | . 2 | 1.302866 | 1.169763 | 0.410014 | 0.860622 | 00:37 | . 0.55 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.638014 | 1.845488 | 0.222598 | 0.747632 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.925791 | 1.482326 | 0.333559 | 0.832882 | 00:37 | . 1 | 1.598462 | 1.298628 | 0.412720 | 0.858593 | 00:37 | . 2 | 1.357530 | 1.238791 | 0.433018 | 0.861299 | 00:37 | . 0.55 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.615300 | 1.854350 | 0.221245 | 0.729364 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.940952 | 1.501719 | 0.328146 | 0.824087 | 00:37 | . 1 | 1.605274 | 1.317969 | 0.391069 | 0.855210 | 00:36 | . 2 | 1.376344 | 1.254735 | 0.412720 | 0.871448 | 00:37 | . 0.55 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.631370 | 1.876606 | 0.217185 | 0.738160 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.960124 | 1.525003 | 0.334235 | 0.832882 | 00:36 | . 1 | 1.631765 | 1.291659 | 0.409337 | 0.865359 | 00:37 | . 2 | 1.400282 | 1.254287 | 0.422869 | 0.869418 | 00:36 | . 0.55 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.633411 | 1.870220 | 0.218539 | 0.761164 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.937681 | 1.510999 | 0.337618 | 0.828823 | 00:37 | . 1 | 1.613708 | 1.292224 | 0.399188 | 0.858593 | 00:36 | . 2 | 1.385506 | 1.239504 | 0.416779 | 0.874831 | 00:37 | . 0.55 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.614647 | 1.858063 | 0.205007 | 0.753045 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.959156 | 1.518517 | 0.334235 | 0.837618 | 00:37 | . 1 | 1.592576 | 1.313447 | 0.400541 | 0.875507 | 00:37 | . 2 | 1.364758 | 1.264032 | 0.417456 | 0.872124 | 00:37 | . 0.6 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.817973 | 1.972988 | 0.234100 | 0.746955 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.104136 | 1.597962 | 0.326116 | 0.817321 | 00:37 | . 1 | 1.738460 | 1.402159 | 0.394452 | 0.857916 | 00:37 | . 2 | 1.483484 | 1.348877 | 0.413396 | 0.866712 | 00:37 | . 0.6 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.820508 | 1.987026 | 0.215156 | 0.753721 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.130444 | 1.609915 | 0.324087 | 0.814614 | 00:37 | . 1 | 1.723010 | 1.379772 | 0.405954 | 0.866035 | 00:37 | . 2 | 1.454965 | 1.335233 | 0.414073 | 0.874831 | 00:37 | . 0.6 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.809963 | 2.036355 | 0.216509 | 0.746279 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.097596 | 1.621040 | 0.343708 | 0.822733 | 00:37 | . 1 | 1.717408 | 1.396926 | 0.400541 | 0.871448 | 00:36 | . 2 | 1.497107 | 1.331714 | 0.413396 | 0.874154 | 00:37 | . 0.6 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.822942 | 1.991981 | 0.219892 | 0.749662 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.128933 | 1.612198 | 0.328823 | 0.822733 | 00:37 | . 1 | 1.730440 | 1.381875 | 0.405954 | 0.868065 | 00:36 | . 2 | 1.485145 | 1.337474 | 0.427605 | 0.874831 | 00:37 | . 0.6 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.789948 | 1.987623 | 0.228687 | 0.750338 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.084112 | 1.585744 | 0.342355 | 0.841001 | 00:36 | . 1 | 1.727367 | 1.380116 | 0.416103 | 0.862652 | 00:37 | . 2 | 1.459438 | 1.327770 | 0.437077 | 0.872124 | 00:37 | . 0.65 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.009011 | 2.161180 | 0.213802 | 0.758457 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.235606 | 1.718681 | 0.328823 | 0.809202 | 00:38 | . 1 | 1.848825 | 1.455383 | 0.414073 | 0.876861 | 00:37 | . 2 | 1.580189 | 1.407809 | 0.421516 | 0.873478 | 00:37 | . 0.65 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.955913 | 2.096981 | 0.246955 | 0.756428 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.223003 | 1.707097 | 0.327470 | 0.822733 | 00:36 | . 1 | 1.827725 | 1.448493 | 0.408660 | 0.870095 | 00:38 | . 2 | 1.563311 | 1.399002 | 0.422192 | 0.869418 | 00:37 | . 0.65 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.018540 | 2.081591 | 0.226658 | 0.753045 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.230608 | 1.754743 | 0.329499 | 0.829499 | 00:37 | . 1 | 1.830380 | 1.480901 | 0.410690 | 0.861299 | 00:37 | . 2 | 1.550415 | 1.434487 | 0.415426 | 0.862652 | 00:37 | . 0.65 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.962046 | 2.118025 | 0.230041 | 0.745602 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.214669 | 1.706971 | 0.334235 | 0.826116 | 00:37 | . 1 | 1.855873 | 1.451348 | 0.405277 | 0.859946 | 00:37 | . 2 | 1.581562 | 1.417826 | 0.423545 | 0.868065 | 00:37 | . 0.65 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.951794 | 2.098015 | 0.234100 | 0.738836 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.171657 | 1.739040 | 0.335589 | 0.823410 | 00:37 | . 1 | 1.811115 | 1.488068 | 0.403924 | 0.868742 | 00:37 | . 2 | 1.594258 | 1.425765 | 0.419486 | 0.864682 | 00:36 | . 0.7 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.198257 | 2.204554 | 0.244926 | 0.745602 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.359416 | 1.820511 | 0.326116 | 0.820027 | 00:36 | . 1 | 1.941639 | 1.554378 | 0.408660 | 0.852503 | 00:37 | . 2 | 1.664369 | 1.512919 | 0.419486 | 0.858593 | 00:37 | . 0.7 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.155220 | 2.207247 | 0.227334 | 0.745602 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.326856 | 1.790417 | 0.349120 | 0.822057 | 00:37 | . 1 | 1.946431 | 1.538183 | 0.403924 | 0.866035 | 00:37 | . 2 | 1.675359 | 1.499935 | 0.416779 | 0.876184 | 00:37 | . 0.7 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.163814 | 2.214384 | 0.234777 | 0.759134 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.357758 | 1.773194 | 0.345061 | 0.816644 | 00:36 | . 1 | 1.948731 | 1.535761 | 0.415426 | 0.855210 | 00:37 | . 2 | 1.634738 | 1.486537 | 0.433018 | 0.860622 | 00:36 | . 0.7 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.192344 | 2.230326 | 0.221922 | 0.761164 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.358797 | 1.829024 | 0.327470 | 0.820704 | 00:37 | . 1 | 1.958898 | 1.590302 | 0.401218 | 0.867388 | 00:36 | . 2 | 1.644176 | 1.532671 | 0.422869 | 0.877537 | 00:37 | . 0.7 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.181948 | 2.217805 | 0.229364 | 0.771989 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.358733 | 1.863542 | 0.326793 | 0.824763 | 00:37 | . 1 | 1.954758 | 1.579601 | 0.387686 | 0.854533 | 00:36 | . 2 | 1.686130 | 1.504631 | 0.420839 | 0.870771 | 00:37 | . 0.75 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.330437 | 2.333937 | 0.236130 | 0.735453 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.464814 | 1.903766 | 0.350474 | 0.811908 | 00:37 | . 1 | 2.065806 | 1.603928 | 0.415426 | 0.851150 | 00:36 | . 2 | 1.748820 | 1.582519 | 0.419486 | 0.854533 | 00:37 | . 0.75 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.366714 | 2.374402 | 0.225304 | 0.735453 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.469616 | 1.925668 | 0.335589 | 0.814614 | 00:37 | . 1 | 2.051031 | 1.646415 | 0.420162 | 0.861299 | 00:37 | . 2 | 1.785221 | 1.601296 | 0.439107 | 0.871448 | 00:37 | . 0.75 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.393744 | 2.347261 | 0.221245 | 0.742219 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.506415 | 1.907930 | 0.338295 | 0.838295 | 00:37 | . 1 | 2.063862 | 1.658614 | 0.408660 | 0.862652 | 00:37 | . 2 | 1.753641 | 1.602062 | 0.428281 | 0.861299 | 00:37 | . 0.75 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.342486 | 2.357576 | 0.218539 | 0.733424 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.458844 | 1.908887 | 0.326116 | 0.813261 | 00:37 | . 1 | 2.061423 | 1.633882 | 0.413396 | 0.864005 | 00:38 | . 2 | 1.761941 | 1.596569 | 0.425575 | 0.863329 | 00:37 | . 0.75 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.333535 | 2.393155 | 0.222598 | 0.757104 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.455312 | 1.903025 | 0.341001 | 0.832206 | 00:37 | . 1 | 2.013276 | 1.640475 | 0.402571 | 0.866035 | 00:37 | . 2 | 1.734778 | 1.603247 | 0.420162 | 0.868065 | 00:37 | . 0.8 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.517603 | 2.470913 | 0.228687 | 0.733424 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.584037 | 1.991639 | 0.343031 | 0.817997 | 00:37 | . 1 | 2.155891 | 1.740584 | 0.405277 | 0.848444 | 00:37 | . 2 | 1.881066 | 1.692623 | 0.418809 | 0.854533 | 00:36 | . 0.8 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.531799 | 2.473167 | 0.212449 | 0.728011 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.586812 | 2.013087 | 0.339648 | 0.813938 | 00:37 | . 1 | 2.147250 | 1.731336 | 0.415426 | 0.865359 | 00:37 | . 2 | 1.809500 | 1.678230 | 0.412720 | 0.866035 | 00:37 | . 0.8 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.496719 | 2.481428 | 0.231394 | 0.756428 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.588414 | 2.020012 | 0.347091 | 0.808525 | 00:36 | . 1 | 2.147182 | 1.729274 | 0.408660 | 0.859946 | 00:36 | . 2 | 1.830954 | 1.673658 | 0.424899 | 0.868742 | 00:37 | . 0.8 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.561256 | 2.464022 | 0.219215 | 0.761840 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.615277 | 2.064457 | 0.320704 | 0.822057 | 00:37 | . 1 | 2.163239 | 1.750760 | 0.395129 | 0.859269 | 00:37 | . 2 | 1.842750 | 1.683698 | 0.419486 | 0.865359 | 00:37 | . 0.8 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.516812 | 2.515579 | 0.220568 | 0.746955 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.663132 | 2.040370 | 0.324763 | 0.827470 | 00:37 | . 1 | 2.186795 | 1.747166 | 0.409337 | 0.858593 | 00:37 | . 2 | 1.862716 | 1.682933 | 0.421516 | 0.863329 | 00:37 | . 0.85 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.736004 | 2.610614 | 0.225981 | 0.743572 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.772554 | 2.123350 | 0.342355 | 0.821380 | 00:36 | . 1 | 2.309196 | 1.815178 | 0.418809 | 0.844384 | 00:36 | . 2 | 1.941451 | 1.770293 | 0.435047 | 0.849797 | 00:37 | . 0.85 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.755409 | 2.652715 | 0.215156 | 0.737483 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.753325 | 2.170297 | 0.319350 | 0.805142 | 00:37 | . 1 | 2.272888 | 1.856154 | 0.403248 | 0.851827 | 00:37 | . 2 | 1.942390 | 1.770742 | 0.425575 | 0.853180 | 00:37 | . 0.85 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.754647 | 2.639532 | 0.232747 | 0.744249 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.746267 | 2.144594 | 0.336265 | 0.811231 | 00:37 | . 1 | 2.291466 | 1.836326 | 0.416103 | 0.853180 | 00:36 | . 2 | 1.951140 | 1.789369 | 0.428958 | 0.864005 | 00:37 | . 0.85 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.781451 | 2.594095 | 0.226658 | 0.731394 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.735040 | 2.121984 | 0.349120 | 0.817321 | 00:37 | . 1 | 2.279536 | 1.837771 | 0.405277 | 0.861299 | 00:37 | . 2 | 1.950481 | 1.769247 | 0.415426 | 0.861299 | 00:37 | . 0.85 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.733494 | 2.624789 | 0.217862 | 0.732747 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.737912 | 2.116808 | 0.332206 | 0.826793 | 00:38 | . 1 | 2.258536 | 1.797428 | 0.432341 | 0.861299 | 00:37 | . 2 | 1.940394 | 1.753681 | 0.439107 | 0.861299 | 00:37 | . 0.9 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.886960 | 2.733624 | 0.217862 | 0.721245 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.863107 | 2.238920 | 0.336265 | 0.808525 | 00:37 | . 1 | 2.360180 | 1.894698 | 0.413396 | 0.852503 | 00:37 | . 2 | 2.025337 | 1.835800 | 0.433694 | 0.855210 | 00:37 | . 0.9 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.849422 | 2.745886 | 0.221245 | 0.759134 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.856699 | 2.247028 | 0.336942 | 0.826793 | 00:37 | . 1 | 2.387679 | 1.962858 | 0.398511 | 0.851827 | 00:37 | . 2 | 2.072373 | 1.887339 | 0.418809 | 0.861976 | 00:38 | . 0.9 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.840360 | 2.741130 | 0.215832 | 0.735453 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.882113 | 2.204600 | 0.336942 | 0.809202 | 00:37 | . 1 | 2.357663 | 1.902547 | 0.402571 | 0.847767 | 00:37 | . 2 | 2.024590 | 1.843035 | 0.421516 | 0.853857 | 00:37 | . 0.9 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.905269 | 2.730477 | 0.234100 | 0.740189 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.877471 | 2.243390 | 0.332206 | 0.820027 | 00:38 | . 1 | 2.385533 | 1.956185 | 0.399188 | 0.860622 | 00:37 | . 2 | 2.035516 | 1.866663 | 0.416103 | 0.859946 | 00:37 | . 0.9 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.893532 | 2.768904 | 0.217862 | 0.738836 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.893402 | 2.293969 | 0.326793 | 0.809878 | 00:38 | . 1 | 2.374747 | 1.977614 | 0.390392 | 0.844384 | 00:37 | . 2 | 2.044080 | 1.895727 | 0.420162 | 0.851150 | 00:37 | . 0.95 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 4.149364 | 2.917283 | 0.212449 | 0.720568 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.034748 | 2.387491 | 0.330853 | 0.801759 | 00:38 | . 1 | 2.500775 | 2.035289 | 0.400541 | 0.843708 | 00:37 | . 2 | 2.182788 | 1.967989 | 0.418809 | 0.853180 | 00:37 | . 0.95 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 4.088542 | 2.851956 | 0.234100 | 0.750338 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.008730 | 2.364009 | 0.319350 | 0.803789 | 00:37 | . 1 | 2.514319 | 2.006232 | 0.397835 | 0.858593 | 00:38 | . 2 | 2.182567 | 1.951065 | 0.424899 | 0.868742 | 00:37 | . 0.95 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 4.045652 | 2.849393 | 0.230717 | 0.732070 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.007210 | 2.375066 | 0.330176 | 0.815968 | 00:37 | . 1 | 2.492804 | 1.999894 | 0.407307 | 0.857239 | 00:36 | . 2 | 2.124202 | 1.936110 | 0.427605 | 0.868742 | 00:37 | . 0.95 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 4.006185 | 2.852786 | 0.230717 | 0.720568 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.030243 | 2.398566 | 0.321380 | 0.796346 | 00:37 | . 1 | 2.511322 | 2.031214 | 0.410690 | 0.850474 | 00:37 | . 2 | 2.163672 | 1.964476 | 0.423545 | 0.854533 | 00:36 | . 0.95 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 4.082810 | 2.887275 | 0.223951 | 0.750338 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.026063 | 2.399104 | 0.332206 | 0.793640 | 00:37 | . 1 | 2.472524 | 2.068276 | 0.397158 | 0.844384 | 00:37 | . 2 | 2.164427 | 1.981487 | 0.423545 | 0.847767 | 00:37 | . 1.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 4.299767 | 3.003678 | 0.231394 | 0.737483 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.199222 | 2.478894 | 0.336265 | 0.812585 | 00:37 | . 1 | 2.598992 | 2.129506 | 0.410690 | 0.842355 | 00:38 | . 2 | 2.228962 | 2.058195 | 0.415426 | 0.844384 | 00:38 | . 1.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 4.259891 | 3.005287 | 0.238160 | 0.716509 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.157617 | 2.463936 | 0.330853 | 0.806495 | 00:39 | . 1 | 2.630949 | 2.125719 | 0.410014 | 0.847767 | 00:38 | . 2 | 2.232567 | 2.053470 | 0.424222 | 0.852503 | 00:37 | . 1.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 4.254341 | 3.027207 | 0.225304 | 0.733424 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.131791 | 2.483761 | 0.323410 | 0.795670 | 00:38 | . 1 | 2.607056 | 2.157785 | 0.396482 | 0.850474 | 00:38 | . 2 | 2.246079 | 2.051265 | 0.423545 | 0.855210 | 00:38 | . 1.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 4.266289 | 2.933907 | 0.223275 | 0.739513 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.178112 | 2.446662 | 0.310555 | 0.807172 | 00:38 | . 1 | 2.636582 | 2.114400 | 0.391069 | 0.853857 | 00:38 | . 2 | 2.228099 | 2.031883 | 0.421516 | 0.860622 | 00:38 | . 1.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 4.296805 | 2.970987 | 0.231394 | 0.707037 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.168451 | 2.439990 | 0.332882 | 0.806495 | 00:37 | . 1 | 2.610174 | 2.150257 | 0.387686 | 0.845061 | 00:37 | . 2 | 2.226271 | 2.044568 | 0.415426 | 0.859269 | 00:37 | . # Save results for analysis later results.to_csv(&#39;results.csv&#39;) . from functools import partial weights = np.array(range(0,11))/10 results = pd.DataFrame() id_num = 1 for w in weights: for i in range(0,5): print(w) tmp_loss = partial(final_loss,w=w) learn = cnn_learner(dls, resnet18, metrics=[accuracy_breed,accuracy_species], pretrained=True, loss_func = tmp_loss) learn.fine_tune(10) df = pd.DataFrame(learn.recorder.values) df.columns = learn.recorder.metric_names[1:-1] df[&#39;w&#39;] = w df[&#39;id&#39;] = id_num results = results.append(df) id_num = id_num + 1 learn = None torch.cuda.empty_cache() . 0.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.625016 | 0.479923 | 0.036536 | 0.725981 | 00:36 | . /home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.507059 | 0.401960 | 0.037212 | 0.791610 | 00:37 | . 1 | 0.433211 | 0.345630 | 0.046685 | 0.833559 | 00:37 | . 2 | 0.359912 | 0.294489 | 0.042625 | 0.862652 | 00:36 | . 3 | 0.299221 | 0.287968 | 0.039919 | 0.877537 | 00:37 | . 4 | 0.257674 | 0.263851 | 0.037212 | 0.878890 | 00:37 | . 5 | 0.220921 | 0.244987 | 0.039242 | 0.898512 | 00:37 | . 6 | 0.186900 | 0.235392 | 0.037212 | 0.896482 | 00:36 | . 7 | 0.159165 | 0.234218 | 0.046008 | 0.903248 | 00:37 | . 8 | 0.138894 | 0.231472 | 0.036536 | 0.906631 | 00:38 | . 9 | 0.135808 | 0.229498 | 0.041272 | 0.912043 | 00:37 | . 0.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.622322 | 0.474331 | 0.041949 | 0.746955 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.510941 | 0.410511 | 0.051421 | 0.784844 | 00:36 | . 1 | 0.441043 | 0.349347 | 0.049391 | 0.824763 | 00:37 | . 2 | 0.363789 | 0.301225 | 0.051421 | 0.855886 | 00:38 | . 3 | 0.300586 | 0.275724 | 0.046685 | 0.870771 | 00:37 | . 4 | 0.256025 | 0.265866 | 0.054804 | 0.878890 | 00:37 | . 5 | 0.222844 | 0.244691 | 0.064276 | 0.897158 | 00:37 | . 6 | 0.178984 | 0.243718 | 0.060217 | 0.896482 | 00:37 | . 7 | 0.157990 | 0.253866 | 0.054804 | 0.901894 | 00:38 | . 8 | 0.150251 | 0.238918 | 0.050744 | 0.901894 | 00:37 | . 9 | 0.135434 | 0.242577 | 0.051421 | 0.904601 | 00:37 | . 0.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.633932 | 0.480479 | 0.036536 | 0.734100 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.508789 | 0.409770 | 0.047361 | 0.781461 | 00:37 | . 1 | 0.429826 | 0.352348 | 0.056834 | 0.835589 | 00:37 | . 2 | 0.361513 | 0.306510 | 0.058863 | 0.854533 | 00:36 | . 3 | 0.309663 | 0.278162 | 0.042625 | 0.863329 | 00:37 | . 4 | 0.250305 | 0.282136 | 0.047361 | 0.875507 | 00:37 | . 5 | 0.211476 | 0.242131 | 0.055480 | 0.903248 | 00:38 | . 6 | 0.183167 | 0.240755 | 0.057510 | 0.899865 | 00:36 | . 7 | 0.158439 | 0.237997 | 0.052097 | 0.903924 | 00:36 | . 8 | 0.145325 | 0.244260 | 0.050744 | 0.896482 | 00:38 | . 9 | 0.136802 | 0.240418 | 0.052774 | 0.901894 | 00:37 | . 0.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.626234 | 0.495495 | 0.037212 | 0.713126 | 00:38 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.497866 | 0.421819 | 0.041272 | 0.777402 | 00:38 | . 1 | 0.440182 | 0.351191 | 0.053451 | 0.823410 | 00:38 | . 2 | 0.361280 | 0.311216 | 0.050068 | 0.855210 | 00:38 | . 3 | 0.305357 | 0.285613 | 0.043302 | 0.883627 | 00:38 | . 4 | 0.249462 | 0.265825 | 0.038566 | 0.883627 | 00:38 | . 5 | 0.222984 | 0.249104 | 0.048714 | 0.889039 | 00:37 | . 6 | 0.187364 | 0.247087 | 0.046008 | 0.895805 | 00:37 | . 7 | 0.166315 | 0.253369 | 0.050744 | 0.888363 | 00:37 | . 8 | 0.147665 | 0.244913 | 0.050068 | 0.897158 | 00:37 | . 9 | 0.135472 | 0.245122 | 0.046685 | 0.893775 | 00:37 | . 0.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.612926 | 0.498674 | 0.042625 | 0.738836 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.493829 | 0.421074 | 0.035859 | 0.788904 | 00:37 | . 1 | 0.417949 | 0.341242 | 0.037212 | 0.838972 | 00:37 | . 2 | 0.370740 | 0.299446 | 0.046685 | 0.863329 | 00:38 | . 3 | 0.304158 | 0.263265 | 0.047361 | 0.889039 | 00:38 | . 4 | 0.256633 | 0.256540 | 0.045332 | 0.887686 | 00:38 | . 5 | 0.220906 | 0.226854 | 0.047361 | 0.903248 | 00:38 | . 6 | 0.189633 | 0.235439 | 0.042625 | 0.898512 | 00:38 | . 7 | 0.160309 | 0.218298 | 0.045332 | 0.907307 | 00:38 | . 8 | 0.149116 | 0.214529 | 0.048038 | 0.910014 | 00:37 | . 9 | 0.145614 | 0.215297 | 0.046685 | 0.907307 | 00:37 | . 0.1 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.015695 | 0.790338 | 0.124493 | 0.730717 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.846479 | 0.684321 | 0.160352 | 0.784168 | 00:38 | . 1 | 0.727632 | 0.569384 | 0.266576 | 0.837618 | 00:39 | . 2 | 0.615839 | 0.495469 | 0.320704 | 0.880920 | 00:37 | . 3 | 0.540326 | 0.455996 | 0.377537 | 0.886333 | 00:37 | . 4 | 0.458295 | 0.419409 | 0.425575 | 0.905954 | 00:37 | . 5 | 0.402397 | 0.393873 | 0.439107 | 0.912043 | 00:37 | . 6 | 0.355740 | 0.380987 | 0.470907 | 0.913396 | 00:37 | . 7 | 0.326227 | 0.370533 | 0.476319 | 0.913396 | 00:37 | . 8 | 0.298857 | 0.364401 | 0.486468 | 0.914750 | 00:37 | . 9 | 0.288544 | 0.365401 | 0.483085 | 0.914073 | 00:36 | . 0.1 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.034890 | 0.778333 | 0.135318 | 0.731394 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.848742 | 0.670756 | 0.188092 | 0.786198 | 00:37 | . 1 | 0.720974 | 0.570555 | 0.254398 | 0.839648 | 00:37 | . 2 | 0.615514 | 0.497560 | 0.330176 | 0.864005 | 00:36 | . 3 | 0.528419 | 0.449524 | 0.365359 | 0.885656 | 00:36 | . 4 | 0.461734 | 0.421773 | 0.424899 | 0.888363 | 00:37 | . 5 | 0.394547 | 0.408474 | 0.428958 | 0.909337 | 00:37 | . 6 | 0.363974 | 0.393286 | 0.451962 | 0.908660 | 00:37 | . 7 | 0.325545 | 0.389925 | 0.452639 | 0.899188 | 00:36 | . 8 | 0.295776 | 0.384255 | 0.465494 | 0.902571 | 00:37 | . 9 | 0.295245 | 0.379077 | 0.474290 | 0.905954 | 00:37 | . 0.1 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.034582 | 0.774873 | 0.116373 | 0.749662 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.838457 | 0.672623 | 0.177943 | 0.802436 | 00:36 | . 1 | 0.725706 | 0.557961 | 0.254398 | 0.849797 | 00:37 | . 2 | 0.614908 | 0.483182 | 0.328823 | 0.878890 | 00:37 | . 3 | 0.521889 | 0.434875 | 0.376861 | 0.897158 | 00:37 | . 4 | 0.467466 | 0.411293 | 0.418133 | 0.903924 | 00:36 | . 5 | 0.402984 | 0.402105 | 0.426252 | 0.901894 | 00:37 | . 6 | 0.349201 | 0.370461 | 0.461434 | 0.914750 | 00:37 | . 7 | 0.313538 | 0.361878 | 0.462788 | 0.914750 | 00:37 | . 8 | 0.298288 | 0.357511 | 0.479026 | 0.916779 | 00:37 | . 9 | 0.285764 | 0.361997 | 0.468200 | 0.912720 | 00:37 | . 0.1 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.023727 | 0.768461 | 0.142084 | 0.740866 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.844927 | 0.655789 | 0.183356 | 0.811231 | 00:36 | . 1 | 0.724034 | 0.566467 | 0.264547 | 0.846414 | 00:38 | . 2 | 0.621008 | 0.488818 | 0.324087 | 0.870095 | 00:37 | . 3 | 0.524997 | 0.443974 | 0.373478 | 0.893775 | 00:37 | . 4 | 0.467627 | 0.437977 | 0.395805 | 0.894452 | 00:37 | . 5 | 0.416274 | 0.405570 | 0.426928 | 0.897835 | 00:37 | . 6 | 0.356366 | 0.387477 | 0.452639 | 0.906631 | 00:38 | . 7 | 0.316069 | 0.377638 | 0.462788 | 0.915426 | 00:37 | . 8 | 0.299604 | 0.371586 | 0.472260 | 0.914073 | 00:37 | . 9 | 0.290746 | 0.377479 | 0.466847 | 0.916103 | 00:37 | . 0.1 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.008276 | 0.781661 | 0.128552 | 0.750338 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 0.833022 | 0.652778 | 0.187415 | 0.814614 | 00:38 | . 1 | 0.730837 | 0.558292 | 0.259134 | 0.853857 | 00:37 | . 2 | 0.618586 | 0.500116 | 0.305819 | 0.873478 | 00:37 | . 3 | 0.527472 | 0.455808 | 0.360622 | 0.889716 | 00:38 | . 4 | 0.450522 | 0.403438 | 0.420839 | 0.909337 | 00:37 | . 5 | 0.392979 | 0.397645 | 0.446549 | 0.916779 | 00:38 | . 6 | 0.358926 | 0.379845 | 0.453315 | 0.917456 | 00:37 | . 7 | 0.317811 | 0.363151 | 0.462111 | 0.918133 | 00:37 | . 8 | 0.305132 | 0.359969 | 0.480379 | 0.918133 | 00:37 | . 9 | 0.285435 | 0.355290 | 0.482409 | 0.916103 | 00:38 | . 0.2 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.365327 | 1.025878 | 0.184032 | 0.722598 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.129920 | 0.885158 | 0.244926 | 0.788904 | 00:37 | . 1 | 0.970001 | 0.744516 | 0.318674 | 0.855210 | 00:37 | . 2 | 0.815925 | 0.662721 | 0.380920 | 0.880244 | 00:37 | . 3 | 0.700131 | 0.591389 | 0.426252 | 0.896482 | 00:37 | . 4 | 0.593274 | 0.542593 | 0.468200 | 0.905954 | 00:37 | . 5 | 0.528986 | 0.520209 | 0.486468 | 0.912043 | 00:37 | . 6 | 0.476613 | 0.500101 | 0.504060 | 0.922869 | 00:37 | . 7 | 0.432674 | 0.489240 | 0.526387 | 0.920839 | 00:37 | . 8 | 0.401982 | 0.473175 | 0.524357 | 0.921516 | 00:37 | . 9 | 0.379050 | 0.475408 | 0.519621 | 0.915426 | 00:36 | . 0.2 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.389214 | 1.009220 | 0.175913 | 0.748309 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.092932 | 0.872213 | 0.255074 | 0.804465 | 00:37 | . 1 | 0.948197 | 0.743900 | 0.332206 | 0.848444 | 00:37 | . 2 | 0.796688 | 0.648667 | 0.393099 | 0.889039 | 00:37 | . 3 | 0.717130 | 0.594006 | 0.425575 | 0.884980 | 00:37 | . 4 | 0.606179 | 0.566392 | 0.453992 | 0.906631 | 00:37 | . 5 | 0.516399 | 0.521967 | 0.473613 | 0.907984 | 00:37 | . 6 | 0.473719 | 0.492788 | 0.500677 | 0.916779 | 00:37 | . 7 | 0.429246 | 0.479788 | 0.508796 | 0.922192 | 00:37 | . 8 | 0.401799 | 0.478379 | 0.507442 | 0.921516 | 00:36 | . 9 | 0.385225 | 0.475498 | 0.518945 | 0.921516 | 00:37 | . 0.2 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.387291 | 1.014596 | 0.190122 | 0.759811 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.111250 | 0.865412 | 0.259134 | 0.817997 | 00:37 | . 1 | 0.962793 | 0.748841 | 0.329499 | 0.850474 | 00:38 | . 2 | 0.816421 | 0.651094 | 0.399865 | 0.872801 | 00:37 | . 3 | 0.688583 | 0.610195 | 0.445873 | 0.892422 | 00:37 | . 4 | 0.604417 | 0.552991 | 0.454668 | 0.903248 | 00:37 | . 5 | 0.540546 | 0.534100 | 0.488498 | 0.910014 | 00:38 | . 6 | 0.462065 | 0.525367 | 0.496617 | 0.903248 | 00:37 | . 7 | 0.429902 | 0.491264 | 0.511502 | 0.920162 | 00:36 | . 8 | 0.402967 | 0.490337 | 0.516915 | 0.916779 | 00:37 | . 9 | 0.394441 | 0.492252 | 0.520974 | 0.916779 | 00:36 | . 0.2 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.374817 | 0.998662 | 0.190122 | 0.771313 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.094308 | 0.867839 | 0.253721 | 0.799729 | 00:36 | . 1 | 0.950839 | 0.749833 | 0.333559 | 0.845061 | 00:37 | . 2 | 0.805073 | 0.639634 | 0.393099 | 0.887010 | 00:37 | . 3 | 0.700090 | 0.593801 | 0.418133 | 0.889039 | 00:37 | . 4 | 0.606435 | 0.553453 | 0.455345 | 0.902571 | 00:37 | . 5 | 0.513009 | 0.516373 | 0.504060 | 0.915426 | 00:37 | . 6 | 0.470729 | 0.482671 | 0.521651 | 0.923545 | 00:37 | . 7 | 0.430210 | 0.482066 | 0.523004 | 0.922192 | 00:37 | . 8 | 0.400643 | 0.472134 | 0.529770 | 0.924899 | 00:37 | . 9 | 0.387562 | 0.468577 | 0.530447 | 0.926928 | 00:37 | . 0.2 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.393870 | 1.006662 | 0.169147 | 0.759811 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.108243 | 0.858972 | 0.242896 | 0.827470 | 00:38 | . 1 | 0.952699 | 0.735171 | 0.322733 | 0.859946 | 00:36 | . 2 | 0.813064 | 0.667579 | 0.367388 | 0.883627 | 00:36 | . 3 | 0.692973 | 0.583742 | 0.424899 | 0.886333 | 00:38 | . 4 | 0.585241 | 0.572835 | 0.447226 | 0.901218 | 00:37 | . 5 | 0.513562 | 0.527759 | 0.479026 | 0.902571 | 00:37 | . 6 | 0.470761 | 0.508769 | 0.513532 | 0.911367 | 00:37 | . 7 | 0.427946 | 0.485042 | 0.523681 | 0.916103 | 00:39 | . 8 | 0.398634 | 0.479724 | 0.535183 | 0.916779 | 00:39 | . 9 | 0.384465 | 0.481936 | 0.534506 | 0.912720 | 00:39 | . 0.3 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.738919 | 1.297133 | 0.188769 | 0.744926 | 00:39 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.361800 | 1.091435 | 0.271313 | 0.805142 | 00:38 | . 1 | 1.184849 | 0.939257 | 0.334235 | 0.854533 | 00:39 | . 2 | 1.017697 | 0.817283 | 0.408660 | 0.880244 | 00:38 | . 3 | 0.851813 | 0.734951 | 0.453992 | 0.888363 | 00:38 | . 4 | 0.743630 | 0.684212 | 0.508796 | 0.896482 | 00:38 | . 5 | 0.660696 | 0.666156 | 0.515562 | 0.903924 | 00:38 | . 6 | 0.591511 | 0.638917 | 0.535859 | 0.916103 | 00:39 | . 7 | 0.533230 | 0.615589 | 0.554804 | 0.920162 | 00:38 | . 8 | 0.483556 | 0.617888 | 0.552097 | 0.921516 | 00:38 | . 9 | 0.479016 | 0.613786 | 0.549391 | 0.921516 | 00:39 | . 0.3 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.722157 | 1.245633 | 0.211096 | 0.759134 | 00:39 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.341543 | 1.070682 | 0.259134 | 0.797700 | 00:38 | . 1 | 1.171057 | 0.938443 | 0.343708 | 0.859269 | 00:39 | . 2 | 1.019498 | 0.826886 | 0.407307 | 0.878890 | 00:38 | . 3 | 0.872075 | 0.740434 | 0.457375 | 0.893775 | 00:38 | . 4 | 0.749391 | 0.696174 | 0.483085 | 0.899188 | 00:39 | . 5 | 0.654669 | 0.654207 | 0.496617 | 0.907307 | 00:38 | . 6 | 0.599969 | 0.639355 | 0.507442 | 0.897835 | 00:38 | . 7 | 0.524947 | 0.618700 | 0.527740 | 0.916103 | 00:38 | . 8 | 0.492097 | 0.616302 | 0.533830 | 0.918809 | 00:40 | . 9 | 0.488781 | 0.616275 | 0.532476 | 0.918133 | 00:38 | . 0.3 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.756495 | 1.247279 | 0.211773 | 0.747632 | 00:38 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.371660 | 1.064888 | 0.272666 | 0.805142 | 00:38 | . 1 | 1.192697 | 0.917089 | 0.348444 | 0.853180 | 00:38 | . 2 | 1.015479 | 0.807710 | 0.412720 | 0.889039 | 00:40 | . 3 | 0.868745 | 0.742092 | 0.451286 | 0.899865 | 00:39 | . 4 | 0.737173 | 0.688626 | 0.489851 | 0.910690 | 00:39 | . 5 | 0.658555 | 0.654598 | 0.508796 | 0.918133 | 00:39 | . 6 | 0.584645 | 0.626055 | 0.531123 | 0.921516 | 00:38 | . 7 | 0.526449 | 0.614886 | 0.543978 | 0.922192 | 00:39 | . 8 | 0.491468 | 0.608906 | 0.547361 | 0.922192 | 00:38 | . 9 | 0.469669 | 0.611100 | 0.545332 | 0.920839 | 00:39 | . 0.3 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.741386 | 1.283731 | 0.192828 | 0.748985 | 00:38 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.366029 | 1.100150 | 0.247632 | 0.792287 | 00:40 | . 1 | 1.190240 | 0.948467 | 0.338295 | 0.847091 | 00:38 | . 2 | 1.012447 | 0.838680 | 0.397835 | 0.872124 | 00:38 | . 3 | 0.867978 | 0.729815 | 0.430311 | 0.899188 | 00:38 | . 4 | 0.758979 | 0.694144 | 0.478349 | 0.909337 | 00:38 | . 5 | 0.652083 | 0.657052 | 0.500000 | 0.913396 | 00:39 | . 6 | 0.581800 | 0.630727 | 0.523681 | 0.913396 | 00:38 | . 7 | 0.523255 | 0.620222 | 0.528417 | 0.910690 | 00:38 | . 8 | 0.494075 | 0.608656 | 0.529770 | 0.912043 | 00:38 | . 9 | 0.481408 | 0.606972 | 0.533153 | 0.910014 | 00:39 | . 0.3 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.749489 | 1.228595 | 0.220568 | 0.742219 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.367404 | 1.059520 | 0.291610 | 0.815968 | 00:38 | . 1 | 1.186649 | 0.910826 | 0.366712 | 0.857239 | 00:38 | . 2 | 1.015825 | 0.821598 | 0.400541 | 0.878890 | 00:39 | . 3 | 0.868677 | 0.742662 | 0.451286 | 0.882273 | 00:39 | . 4 | 0.753911 | 0.674934 | 0.476319 | 0.913396 | 00:38 | . 5 | 0.651777 | 0.653150 | 0.495940 | 0.912043 | 00:38 | . 6 | 0.577334 | 0.632651 | 0.514885 | 0.917456 | 00:38 | . 7 | 0.522653 | 0.612776 | 0.538566 | 0.924222 | 00:38 | . 8 | 0.496123 | 0.604917 | 0.538566 | 0.924222 | 00:39 | . 9 | 0.477039 | 0.608792 | 0.540595 | 0.924222 | 00:38 | . 0.4 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.092387 | 1.500538 | 0.211773 | 0.782138 | 00:38 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.607424 | 1.306018 | 0.276725 | 0.809878 | 00:39 | . 1 | 1.422672 | 1.147301 | 0.339648 | 0.851150 | 00:39 | . 2 | 1.194223 | 0.989294 | 0.403248 | 0.880920 | 00:38 | . 3 | 1.015840 | 0.917401 | 0.464817 | 0.897158 | 00:39 | . 4 | 0.920966 | 0.826603 | 0.484438 | 0.918133 | 00:38 | . 5 | 0.773403 | 0.784496 | 0.516915 | 0.904601 | 00:38 | . 6 | 0.686298 | 0.764131 | 0.523004 | 0.914073 | 00:39 | . 7 | 0.616138 | 0.750087 | 0.538566 | 0.913396 | 00:39 | . 8 | 0.594364 | 0.745064 | 0.539919 | 0.913396 | 00:38 | . 9 | 0.569814 | 0.746348 | 0.539919 | 0.913396 | 00:39 | . 0.4 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.111873 | 1.496513 | 0.222598 | 0.757104 | 00:39 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.645708 | 1.296128 | 0.304466 | 0.809202 | 00:39 | . 1 | 1.421104 | 1.115199 | 0.359946 | 0.851150 | 00:39 | . 2 | 1.209268 | 0.996226 | 0.407984 | 0.875507 | 00:38 | . 3 | 1.056087 | 0.892814 | 0.474290 | 0.902571 | 00:38 | . 4 | 0.893261 | 0.853814 | 0.476996 | 0.905954 | 00:39 | . 5 | 0.799684 | 0.803617 | 0.507442 | 0.910014 | 00:39 | . 6 | 0.699611 | 0.769485 | 0.527064 | 0.910690 | 00:38 | . 7 | 0.636087 | 0.755648 | 0.541272 | 0.916779 | 00:39 | . 8 | 0.613674 | 0.751083 | 0.551421 | 0.912720 | 00:38 | . 9 | 0.593500 | 0.743683 | 0.554127 | 0.917456 | 00:39 | . 0.4 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.102594 | 1.472984 | 0.228011 | 0.749662 | 00:38 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.618601 | 1.285751 | 0.294317 | 0.800406 | 00:38 | . 1 | 1.412088 | 1.132139 | 0.349120 | 0.857239 | 00:38 | . 2 | 1.202745 | 0.997658 | 0.412043 | 0.880920 | 00:38 | . 3 | 1.051524 | 0.906678 | 0.451286 | 0.889716 | 00:37 | . 4 | 0.888721 | 0.830003 | 0.494587 | 0.905277 | 00:37 | . 5 | 0.787210 | 0.785158 | 0.511502 | 0.912720 | 00:37 | . 6 | 0.702175 | 0.760165 | 0.529770 | 0.914073 | 00:37 | . 7 | 0.638614 | 0.748983 | 0.541949 | 0.922192 | 00:38 | . 8 | 0.595424 | 0.742924 | 0.551421 | 0.913396 | 00:37 | . 9 | 0.576719 | 0.739741 | 0.550744 | 0.912720 | 00:37 | . 0.4 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.088583 | 1.476634 | 0.211773 | 0.746279 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.613358 | 1.281217 | 0.289581 | 0.804465 | 00:37 | . 1 | 1.405729 | 1.113642 | 0.356563 | 0.851150 | 00:37 | . 2 | 1.194845 | 0.984453 | 0.404601 | 0.878214 | 00:37 | . 3 | 1.041767 | 0.890357 | 0.459405 | 0.897158 | 00:37 | . 4 | 0.890710 | 0.824064 | 0.510825 | 0.917456 | 00:37 | . 5 | 0.794209 | 0.796116 | 0.516915 | 0.914073 | 00:37 | . 6 | 0.678106 | 0.769165 | 0.529093 | 0.918809 | 00:37 | . 7 | 0.621794 | 0.750070 | 0.543978 | 0.916779 | 00:37 | . 8 | 0.583664 | 0.737505 | 0.543302 | 0.921516 | 00:37 | . 9 | 0.559424 | 0.736777 | 0.550068 | 0.920162 | 00:37 | . 0.4 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.080439 | 1.482592 | 0.214479 | 0.752368 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.628090 | 1.271325 | 0.293640 | 0.808525 | 00:37 | . 1 | 1.419389 | 1.114662 | 0.359269 | 0.841678 | 00:36 | . 2 | 1.209594 | 0.988580 | 0.411367 | 0.887686 | 00:36 | . 3 | 1.039253 | 0.901245 | 0.472936 | 0.895805 | 00:37 | . 4 | 0.903212 | 0.832708 | 0.491881 | 0.906631 | 00:37 | . 5 | 0.783649 | 0.774291 | 0.526387 | 0.915426 | 00:37 | . 6 | 0.698352 | 0.763328 | 0.537212 | 0.916779 | 00:37 | . 7 | 0.638497 | 0.741665 | 0.547361 | 0.916779 | 00:37 | . 8 | 0.593709 | 0.735375 | 0.549391 | 0.914750 | 00:37 | . 9 | 0.580151 | 0.733593 | 0.552097 | 0.919486 | 00:37 | . 0.5 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.470473 | 1.737991 | 0.209066 | 0.748985 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.906550 | 1.512185 | 0.281461 | 0.816644 | 00:37 | . 1 | 1.667706 | 1.331141 | 0.344384 | 0.847091 | 00:37 | . 2 | 1.426077 | 1.164751 | 0.418133 | 0.876861 | 00:37 | . 3 | 1.214114 | 1.040492 | 0.456698 | 0.890392 | 00:37 | . 4 | 1.050850 | 0.978984 | 0.498647 | 0.897835 | 00:37 | . 5 | 0.917660 | 0.914083 | 0.525034 | 0.907984 | 00:37 | . 6 | 0.804004 | 0.876696 | 0.532476 | 0.911367 | 00:37 | . 7 | 0.740519 | 0.859325 | 0.541272 | 0.918133 | 00:37 | . 8 | 0.673837 | 0.853517 | 0.541272 | 0.920162 | 00:37 | . 9 | 0.671926 | 0.850789 | 0.546008 | 0.921516 | 00:37 | . 0.5 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.477305 | 1.760878 | 0.211773 | 0.747632 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.920752 | 1.522343 | 0.294317 | 0.813261 | 00:36 | . 1 | 1.669307 | 1.333238 | 0.353857 | 0.846414 | 00:36 | . 2 | 1.400953 | 1.155387 | 0.414750 | 0.882273 | 00:37 | . 3 | 1.213571 | 1.039228 | 0.470907 | 0.888363 | 00:37 | . 4 | 1.059989 | 0.992996 | 0.487145 | 0.891746 | 00:37 | . 5 | 0.920640 | 0.945389 | 0.508119 | 0.898512 | 00:37 | . 6 | 0.817530 | 0.905937 | 0.533153 | 0.914073 | 00:37 | . 7 | 0.750498 | 0.884740 | 0.545332 | 0.920839 | 00:37 | . 8 | 0.681675 | 0.876558 | 0.546008 | 0.917456 | 00:36 | . 9 | 0.669070 | 0.879721 | 0.544655 | 0.918133 | 00:37 | . 0.5 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.449495 | 1.732052 | 0.225981 | 0.747632 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.897457 | 1.517567 | 0.302436 | 0.810555 | 00:38 | . 1 | 1.639247 | 1.298905 | 0.377537 | 0.853857 | 00:37 | . 2 | 1.415111 | 1.152529 | 0.427605 | 0.878214 | 00:37 | . 3 | 1.195785 | 1.049602 | 0.468200 | 0.893775 | 00:37 | . 4 | 1.059030 | 1.005059 | 0.481055 | 0.901894 | 00:37 | . 5 | 0.918150 | 0.935395 | 0.527740 | 0.917456 | 00:37 | . 6 | 0.813309 | 0.893357 | 0.541949 | 0.920162 | 00:37 | . 7 | 0.734996 | 0.877878 | 0.548714 | 0.916779 | 00:37 | . 8 | 0.693819 | 0.869940 | 0.545332 | 0.913396 | 00:36 | . 9 | 0.647826 | 0.871225 | 0.548714 | 0.914750 | 00:37 | . 0.5 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.459829 | 1.742656 | 0.215832 | 0.742896 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.901809 | 1.501850 | 0.293640 | 0.801083 | 00:37 | . 1 | 1.668814 | 1.288649 | 0.372124 | 0.849120 | 00:37 | . 2 | 1.421782 | 1.127819 | 0.442490 | 0.883627 | 00:36 | . 3 | 1.210835 | 1.047999 | 0.459405 | 0.898512 | 00:37 | . 4 | 1.054801 | 0.972207 | 0.497970 | 0.910014 | 00:38 | . 5 | 0.934249 | 0.911774 | 0.528417 | 0.916779 | 00:37 | . 6 | 0.813281 | 0.878551 | 0.553451 | 0.924899 | 00:37 | . 7 | 0.731343 | 0.861506 | 0.558863 | 0.924222 | 00:37 | . 8 | 0.678463 | 0.853170 | 0.564276 | 0.927605 | 00:37 | . 9 | 0.662475 | 0.852433 | 0.560216 | 0.926252 | 00:37 | . 0.5 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.468423 | 1.747778 | 0.213126 | 0.753721 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 1.879629 | 1.525970 | 0.283491 | 0.795670 | 00:37 | . 1 | 1.656154 | 1.328376 | 0.352503 | 0.843708 | 00:37 | . 2 | 1.413864 | 1.151351 | 0.420162 | 0.881597 | 00:37 | . 3 | 1.211042 | 1.053085 | 0.471583 | 0.891746 | 00:37 | . 4 | 1.050736 | 0.971986 | 0.489175 | 0.905277 | 00:36 | . 5 | 0.917589 | 0.915066 | 0.531123 | 0.915426 | 00:37 | . 6 | 0.821204 | 0.889223 | 0.540595 | 0.922192 | 00:37 | . 7 | 0.725125 | 0.870516 | 0.552774 | 0.924222 | 00:37 | . 8 | 0.676369 | 0.861526 | 0.556157 | 0.921516 | 00:36 | . 9 | 0.629235 | 0.861338 | 0.559540 | 0.925575 | 00:37 | . 0.6 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.811696 | 2.003752 | 0.217862 | 0.751015 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.175946 | 1.725664 | 0.294317 | 0.808525 | 00:37 | . 1 | 1.874148 | 1.509262 | 0.375507 | 0.847767 | 00:37 | . 2 | 1.581947 | 1.323296 | 0.427605 | 0.878214 | 00:37 | . 3 | 1.402788 | 1.239938 | 0.447903 | 0.899188 | 00:36 | . 4 | 1.182036 | 1.109013 | 0.498647 | 0.905954 | 00:37 | . 5 | 1.038673 | 1.079195 | 0.518268 | 0.907984 | 00:37 | . 6 | 0.935243 | 1.032340 | 0.539242 | 0.910690 | 00:37 | . 7 | 0.843290 | 1.006846 | 0.540595 | 0.914073 | 00:37 | . 8 | 0.790311 | 1.004138 | 0.558863 | 0.918809 | 00:37 | . 9 | 0.751649 | 0.999126 | 0.552097 | 0.920162 | 00:36 | . 0.6 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.786541 | 1.971358 | 0.230041 | 0.741543 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.189273 | 1.726684 | 0.303112 | 0.798376 | 00:37 | . 1 | 1.899409 | 1.500890 | 0.381597 | 0.846414 | 00:37 | . 2 | 1.620356 | 1.315545 | 0.427605 | 0.870771 | 00:37 | . 3 | 1.385875 | 1.197554 | 0.479702 | 0.895129 | 00:37 | . 4 | 1.207470 | 1.124653 | 0.504736 | 0.907984 | 00:37 | . 5 | 1.048009 | 1.059193 | 0.527740 | 0.916779 | 00:38 | . 6 | 0.937516 | 1.021763 | 0.529770 | 0.907984 | 00:37 | . 7 | 0.846192 | 0.996942 | 0.549391 | 0.918809 | 00:37 | . 8 | 0.780519 | 0.996726 | 0.552097 | 0.921516 | 00:36 | . 9 | 0.758739 | 0.994845 | 0.557510 | 0.914073 | 00:37 | . 0.6 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.801334 | 1.987771 | 0.223275 | 0.757104 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.182163 | 1.722899 | 0.301083 | 0.809878 | 00:37 | . 1 | 1.907701 | 1.512146 | 0.371448 | 0.851150 | 00:37 | . 2 | 1.638567 | 1.320926 | 0.433694 | 0.872801 | 00:37 | . 3 | 1.377851 | 1.198415 | 0.474966 | 0.891746 | 00:37 | . 4 | 1.181832 | 1.123111 | 0.499323 | 0.903248 | 00:36 | . 5 | 1.055885 | 1.088091 | 0.523681 | 0.907307 | 00:37 | . 6 | 0.919165 | 1.050352 | 0.527064 | 0.907307 | 00:37 | . 7 | 0.854994 | 1.025359 | 0.543978 | 0.912043 | 00:36 | . 8 | 0.786608 | 1.019802 | 0.548038 | 0.913396 | 00:37 | . 9 | 0.735101 | 1.015926 | 0.552774 | 0.910014 | 00:37 | . 0.6 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.793879 | 1.962432 | 0.224628 | 0.740866 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.161317 | 1.716679 | 0.295670 | 0.786874 | 00:37 | . 1 | 1.900744 | 1.508022 | 0.351827 | 0.841001 | 00:37 | . 2 | 1.614968 | 1.329168 | 0.422869 | 0.868065 | 00:38 | . 3 | 1.400536 | 1.177783 | 0.484438 | 0.881597 | 00:37 | . 4 | 1.205177 | 1.123343 | 0.504060 | 0.891069 | 00:37 | . 5 | 1.046032 | 1.081031 | 0.526387 | 0.897158 | 00:37 | . 6 | 0.957948 | 1.042185 | 0.529770 | 0.905954 | 00:37 | . 7 | 0.849409 | 1.020082 | 0.542625 | 0.907984 | 00:37 | . 8 | 0.780837 | 1.012410 | 0.548038 | 0.904601 | 00:36 | . 9 | 0.759504 | 1.008992 | 0.545332 | 0.905954 | 00:37 | . 0.6 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.810615 | 1.992500 | 0.221245 | 0.751015 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.166698 | 1.709916 | 0.292287 | 0.788227 | 00:37 | . 1 | 1.914357 | 1.511588 | 0.356563 | 0.824763 | 00:37 | . 2 | 1.630239 | 1.349062 | 0.406631 | 0.868065 | 00:36 | . 3 | 1.365341 | 1.213932 | 0.467524 | 0.895129 | 00:37 | . 4 | 1.201496 | 1.137011 | 0.492557 | 0.897835 | 00:36 | . 5 | 1.038821 | 1.074184 | 0.515562 | 0.909337 | 00:37 | . 6 | 0.938698 | 1.051192 | 0.531800 | 0.903248 | 00:37 | . 7 | 0.820270 | 1.012659 | 0.546008 | 0.913396 | 00:37 | . 8 | 0.769944 | 0.998651 | 0.553451 | 0.918133 | 00:37 | . 9 | 0.747062 | 0.994771 | 0.547361 | 0.916779 | 00:37 | . 0.7 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.178746 | 2.200556 | 0.239513 | 0.746279 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.447702 | 1.940305 | 0.293640 | 0.787551 | 00:37 | . 1 | 2.128111 | 1.668682 | 0.394452 | 0.840325 | 00:36 | . 2 | 1.800813 | 1.445110 | 0.436401 | 0.868742 | 00:36 | . 3 | 1.555890 | 1.341513 | 0.471583 | 0.883627 | 00:37 | . 4 | 1.304863 | 1.247415 | 0.502706 | 0.898512 | 00:38 | . 5 | 1.193316 | 1.221006 | 0.520974 | 0.897835 | 00:36 | . 6 | 1.045637 | 1.153683 | 0.538566 | 0.907307 | 00:37 | . 7 | 0.941176 | 1.131230 | 0.546008 | 0.913396 | 00:36 | . 8 | 0.916726 | 1.119985 | 0.546008 | 0.916103 | 00:36 | . 9 | 0.866210 | 1.118154 | 0.551421 | 0.912720 | 00:38 | . 0.7 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.183741 | 2.199328 | 0.225981 | 0.744926 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.464386 | 1.957088 | 0.282138 | 0.774696 | 00:37 | . 1 | 2.158280 | 1.686249 | 0.372124 | 0.828146 | 00:37 | . 2 | 1.826308 | 1.500789 | 0.435724 | 0.871448 | 00:37 | . 3 | 1.596723 | 1.361097 | 0.460758 | 0.884980 | 00:38 | . 4 | 1.370997 | 1.287873 | 0.487821 | 0.897835 | 00:37 | . 5 | 1.179129 | 1.197191 | 0.526387 | 0.911367 | 00:37 | . 6 | 1.045935 | 1.150186 | 0.554127 | 0.906631 | 00:36 | . 7 | 0.939134 | 1.129232 | 0.558187 | 0.912720 | 00:37 | . 8 | 0.875608 | 1.117012 | 0.558187 | 0.916779 | 00:37 | . 9 | 0.856248 | 1.116807 | 0.564276 | 0.912720 | 00:37 | . 0.7 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.143158 | 2.233288 | 0.239513 | 0.736130 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.486842 | 1.940419 | 0.306495 | 0.794317 | 00:37 | . 1 | 2.150010 | 1.692106 | 0.382950 | 0.834235 | 00:38 | . 2 | 1.824808 | 1.516433 | 0.424222 | 0.867388 | 00:37 | . 3 | 1.574653 | 1.396957 | 0.470907 | 0.882273 | 00:37 | . 4 | 1.356175 | 1.317680 | 0.491204 | 0.890392 | 00:37 | . 5 | 1.193615 | 1.199905 | 0.531800 | 0.912720 | 00:37 | . 6 | 1.028708 | 1.155948 | 0.543302 | 0.918133 | 00:37 | . 7 | 0.969768 | 1.140278 | 0.551421 | 0.920839 | 00:37 | . 8 | 0.887686 | 1.120537 | 0.559540 | 0.924899 | 00:37 | . 9 | 0.835205 | 1.126454 | 0.556834 | 0.922869 | 00:37 | . 0.7 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.208053 | 2.240466 | 0.210419 | 0.744926 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.480655 | 1.948372 | 0.298376 | 0.788227 | 00:37 | . 1 | 2.153230 | 1.709706 | 0.358593 | 0.833559 | 00:37 | . 2 | 1.814844 | 1.466356 | 0.443843 | 0.870095 | 00:36 | . 3 | 1.570745 | 1.366551 | 0.464141 | 0.882950 | 00:37 | . 4 | 1.325701 | 1.251873 | 0.494587 | 0.899188 | 00:36 | . 5 | 1.175096 | 1.205427 | 0.500677 | 0.895805 | 00:37 | . 6 | 1.077067 | 1.155915 | 0.546685 | 0.910014 | 00:37 | . 7 | 0.966489 | 1.125599 | 0.554804 | 0.913396 | 00:37 | . 8 | 0.898975 | 1.118698 | 0.545332 | 0.918809 | 00:37 | . 9 | 0.869859 | 1.117751 | 0.547361 | 0.915426 | 00:37 | . 0.7 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.214741 | 2.280385 | 0.210419 | 0.740866 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.445000 | 2.000435 | 0.280108 | 0.800406 | 00:37 | . 1 | 2.160559 | 1.688362 | 0.371448 | 0.835589 | 00:36 | . 2 | 1.861311 | 1.534233 | 0.426928 | 0.856563 | 00:37 | . 3 | 1.558548 | 1.388075 | 0.457375 | 0.871448 | 00:36 | . 4 | 1.373012 | 1.265771 | 0.509472 | 0.897158 | 00:37 | . 5 | 1.178989 | 1.219092 | 0.522327 | 0.899188 | 00:37 | . 6 | 1.074393 | 1.191718 | 0.535859 | 0.904601 | 00:37 | . 7 | 0.981246 | 1.161868 | 0.542625 | 0.908660 | 00:37 | . 8 | 0.912559 | 1.146814 | 0.541272 | 0.902571 | 00:37 | . 9 | 0.857626 | 1.146125 | 0.546685 | 0.905954 | 00:37 | . 0.8 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.508439 | 2.477670 | 0.234100 | 0.765223 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.720196 | 2.173434 | 0.293640 | 0.788904 | 00:37 | . 1 | 2.366083 | 1.894034 | 0.378214 | 0.836265 | 00:37 | . 2 | 2.027270 | 1.667744 | 0.433018 | 0.875507 | 00:37 | . 3 | 1.726501 | 1.551184 | 0.469553 | 0.881597 | 00:37 | . 4 | 1.521299 | 1.455209 | 0.474290 | 0.877537 | 00:37 | . 5 | 1.328394 | 1.350719 | 0.514885 | 0.893775 | 00:37 | . 6 | 1.188029 | 1.323084 | 0.523681 | 0.893775 | 00:37 | . 7 | 1.058619 | 1.289467 | 0.542625 | 0.901218 | 00:38 | . 8 | 0.986556 | 1.277326 | 0.562246 | 0.905954 | 00:37 | . 9 | 0.947034 | 1.284123 | 0.548038 | 0.900541 | 00:37 | . 0.8 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.524315 | 2.474948 | 0.227334 | 0.753721 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.741448 | 2.165591 | 0.301083 | 0.786198 | 00:37 | . 1 | 2.399327 | 1.868722 | 0.366712 | 0.844384 | 00:38 | . 2 | 2.028822 | 1.687996 | 0.420839 | 0.866035 | 00:37 | . 3 | 1.720565 | 1.532189 | 0.457375 | 0.884303 | 00:37 | . 4 | 1.512225 | 1.411126 | 0.496617 | 0.902571 | 00:37 | . 5 | 1.339291 | 1.365101 | 0.507442 | 0.907307 | 00:37 | . 6 | 1.165017 | 1.319320 | 0.546008 | 0.913396 | 00:37 | . 7 | 1.050022 | 1.285914 | 0.552774 | 0.915426 | 00:37 | . 8 | 0.984937 | 1.274907 | 0.563599 | 0.915426 | 00:37 | . 9 | 0.944949 | 1.277266 | 0.560216 | 0.914750 | 00:37 | . 0.8 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.562545 | 2.516571 | 0.230041 | 0.736807 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.717265 | 2.183523 | 0.294317 | 0.805142 | 00:37 | . 1 | 2.425969 | 1.884403 | 0.357916 | 0.830853 | 00:37 | . 2 | 2.040721 | 1.684892 | 0.416779 | 0.864682 | 00:37 | . 3 | 1.741810 | 1.523723 | 0.460081 | 0.884980 | 00:37 | . 4 | 1.517373 | 1.421285 | 0.502030 | 0.896482 | 00:37 | . 5 | 1.320617 | 1.366544 | 0.515562 | 0.902571 | 00:37 | . 6 | 1.169965 | 1.302253 | 0.546685 | 0.903924 | 00:36 | . 7 | 1.061111 | 1.281986 | 0.558187 | 0.905954 | 00:37 | . 8 | 0.971401 | 1.267084 | 0.558863 | 0.910690 | 00:37 | . 9 | 0.926024 | 1.263578 | 0.558863 | 0.908660 | 00:37 | . 0.8 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.529577 | 2.495765 | 0.220568 | 0.731394 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.746557 | 2.164502 | 0.297023 | 0.803112 | 00:36 | . 1 | 2.398521 | 1.885085 | 0.376861 | 0.824763 | 00:36 | . 2 | 2.040808 | 1.682530 | 0.426252 | 0.855886 | 00:37 | . 3 | 1.763800 | 1.523788 | 0.475643 | 0.888363 | 00:37 | . 4 | 1.519384 | 1.401421 | 0.518268 | 0.901894 | 00:37 | . 5 | 1.322864 | 1.358130 | 0.526387 | 0.907307 | 00:37 | . 6 | 1.175016 | 1.314740 | 0.542625 | 0.917456 | 00:37 | . 7 | 1.066695 | 1.288273 | 0.547361 | 0.909337 | 00:37 | . 8 | 0.999892 | 1.274948 | 0.557510 | 0.912720 | 00:37 | . 9 | 0.958234 | 1.280777 | 0.555480 | 0.911367 | 00:37 | . 0.8 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.586078 | 2.519877 | 0.225981 | 0.728011 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.760551 | 2.179957 | 0.305819 | 0.790934 | 00:37 | . 1 | 2.375387 | 1.932233 | 0.368742 | 0.832882 | 00:37 | . 2 | 2.034920 | 1.697310 | 0.414750 | 0.860622 | 00:37 | . 3 | 1.753487 | 1.529908 | 0.457375 | 0.882273 | 00:37 | . 4 | 1.516834 | 1.425024 | 0.506766 | 0.891069 | 00:37 | . 5 | 1.318555 | 1.388853 | 0.514885 | 0.895805 | 00:37 | . 6 | 1.175357 | 1.329039 | 0.534506 | 0.906631 | 00:38 | . 7 | 1.089122 | 1.311369 | 0.539242 | 0.901894 | 00:37 | . 8 | 0.991363 | 1.300776 | 0.538566 | 0.904601 | 00:37 | . 9 | 0.954550 | 1.297550 | 0.546008 | 0.907984 | 00:37 | . 0.9 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.929849 | 2.769847 | 0.227334 | 0.746955 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.985343 | 2.421226 | 0.287551 | 0.778078 | 00:38 | . 1 | 2.605740 | 2.101244 | 0.361299 | 0.816644 | 00:37 | . 2 | 2.203190 | 1.844675 | 0.428958 | 0.871448 | 00:36 | . 3 | 1.926163 | 1.724730 | 0.456698 | 0.876184 | 00:37 | . 4 | 1.677004 | 1.587203 | 0.488498 | 0.892422 | 00:37 | . 5 | 1.456832 | 1.531526 | 0.508796 | 0.899188 | 00:38 | . 6 | 1.295032 | 1.468722 | 0.533153 | 0.901894 | 00:37 | . 7 | 1.164772 | 1.441536 | 0.529093 | 0.903924 | 00:37 | . 8 | 1.091720 | 1.430688 | 0.540595 | 0.909337 | 00:37 | . 9 | 1.030846 | 1.421544 | 0.533830 | 0.908660 | 00:37 | . 0.9 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.882299 | 2.757034 | 0.217862 | 0.762517 | 00:38 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.960778 | 2.380797 | 0.309878 | 0.799729 | 00:37 | . 1 | 2.640439 | 2.051850 | 0.380244 | 0.836942 | 00:37 | . 2 | 2.207140 | 1.838879 | 0.426252 | 0.864005 | 00:37 | . 3 | 1.919783 | 1.685615 | 0.471583 | 0.874154 | 00:38 | . 4 | 1.678249 | 1.573868 | 0.500000 | 0.892422 | 00:38 | . 5 | 1.443337 | 1.509748 | 0.523004 | 0.894452 | 00:38 | . 6 | 1.288939 | 1.442213 | 0.539242 | 0.903248 | 00:37 | . 7 | 1.191164 | 1.429835 | 0.541272 | 0.899188 | 00:37 | . 8 | 1.082962 | 1.407424 | 0.547361 | 0.902571 | 00:38 | . 9 | 1.031873 | 1.402644 | 0.560216 | 0.910690 | 00:37 | . 0.9 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.880489 | 2.722723 | 0.223951 | 0.738160 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.001838 | 2.404881 | 0.294993 | 0.776049 | 00:37 | . 1 | 2.650714 | 2.057544 | 0.368742 | 0.836265 | 00:37 | . 2 | 2.269529 | 1.818037 | 0.441137 | 0.863329 | 00:38 | . 3 | 1.940286 | 1.667893 | 0.469553 | 0.875507 | 00:37 | . 4 | 1.671974 | 1.548611 | 0.506089 | 0.891746 | 00:37 | . 5 | 1.483565 | 1.457307 | 0.525710 | 0.903924 | 00:37 | . 6 | 1.314587 | 1.415945 | 0.545332 | 0.912043 | 00:38 | . 7 | 1.177440 | 1.392353 | 0.552097 | 0.908660 | 00:37 | . 8 | 1.080910 | 1.386723 | 0.548714 | 0.910690 | 00:37 | . 9 | 1.036743 | 1.371493 | 0.552774 | 0.911367 | 00:37 | . 0.9 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.900769 | 2.769724 | 0.223275 | 0.737483 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.049546 | 2.407842 | 0.285521 | 0.787551 | 00:37 | . 1 | 2.662035 | 2.101704 | 0.356563 | 0.820704 | 00:37 | . 2 | 2.269236 | 1.866396 | 0.424899 | 0.861299 | 00:37 | . 3 | 1.927717 | 1.713596 | 0.462788 | 0.878890 | 00:37 | . 4 | 1.663729 | 1.586510 | 0.493911 | 0.899865 | 00:37 | . 5 | 1.450642 | 1.502067 | 0.522327 | 0.901218 | 00:37 | . 6 | 1.290199 | 1.448592 | 0.548714 | 0.905954 | 00:37 | . 7 | 1.185959 | 1.436772 | 0.552097 | 0.901894 | 00:37 | . 8 | 1.110296 | 1.429173 | 0.564276 | 0.907307 | 00:37 | . 9 | 1.055753 | 1.429661 | 0.555480 | 0.908660 | 00:37 | . 0.9 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.884483 | 2.693047 | 0.230041 | 0.756428 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 2.969113 | 2.395159 | 0.306495 | 0.805142 | 00:37 | . 1 | 2.621535 | 2.095836 | 0.359269 | 0.838972 | 00:37 | . 2 | 2.248646 | 1.835639 | 0.432341 | 0.872801 | 00:37 | . 3 | 1.939157 | 1.713292 | 0.466847 | 0.890392 | 00:37 | . 4 | 1.658232 | 1.554274 | 0.506766 | 0.899865 | 00:37 | . 5 | 1.462044 | 1.506509 | 0.527740 | 0.907984 | 00:36 | . 6 | 1.298722 | 1.453311 | 0.551421 | 0.912720 | 00:37 | . 7 | 1.188548 | 1.427383 | 0.550068 | 0.913396 | 00:37 | . 8 | 1.102674 | 1.416120 | 0.558187 | 0.911367 | 00:37 | . 9 | 1.031420 | 1.420349 | 0.559540 | 0.914750 | 00:38 | . 1.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 4.344813 | 2.975266 | 0.229364 | 0.749662 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.317024 | 2.614984 | 0.303789 | 0.775372 | 00:37 | . 1 | 2.881727 | 2.269658 | 0.386333 | 0.824087 | 00:37 | . 2 | 2.480301 | 2.033594 | 0.433018 | 0.856563 | 00:37 | . 3 | 2.087299 | 1.864601 | 0.467524 | 0.873478 | 00:37 | . 4 | 1.826170 | 1.707703 | 0.509472 | 0.893775 | 00:37 | . 5 | 1.598705 | 1.637676 | 0.523681 | 0.893099 | 00:37 | . 6 | 1.416358 | 1.574962 | 0.539919 | 0.906631 | 00:37 | . 7 | 1.256919 | 1.535741 | 0.547361 | 0.908660 | 00:38 | . 8 | 1.210650 | 1.530421 | 0.551421 | 0.907984 | 00:37 | . 9 | 1.168600 | 1.516965 | 0.560216 | 0.910690 | 00:37 | . 1.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 4.272217 | 2.985735 | 0.227334 | 0.736807 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.269060 | 2.604709 | 0.307172 | 0.785521 | 00:37 | . 1 | 2.878568 | 2.269967 | 0.370771 | 0.818674 | 00:38 | . 2 | 2.438551 | 2.007175 | 0.439107 | 0.866712 | 00:37 | . 3 | 2.101155 | 1.847181 | 0.476319 | 0.889039 | 00:37 | . 4 | 1.807126 | 1.739108 | 0.499323 | 0.889039 | 00:37 | . 5 | 1.609773 | 1.679631 | 0.518945 | 0.907307 | 00:37 | . 6 | 1.414744 | 1.625525 | 0.543302 | 0.898512 | 00:37 | . 7 | 1.294823 | 1.591010 | 0.556157 | 0.906631 | 00:37 | . 8 | 1.170158 | 1.564463 | 0.552774 | 0.908660 | 00:37 | . 9 | 1.142079 | 1.571044 | 0.549391 | 0.903924 | 00:37 | . 1.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 4.259790 | 2.961363 | 0.230717 | 0.736807 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.341803 | 2.610825 | 0.299053 | 0.786874 | 00:38 | . 1 | 2.866162 | 2.247749 | 0.390392 | 0.838972 | 00:37 | . 2 | 2.461322 | 2.097981 | 0.419486 | 0.863329 | 00:37 | . 3 | 2.102239 | 1.895470 | 0.457375 | 0.888363 | 00:37 | . 4 | 1.788782 | 1.738981 | 0.499323 | 0.897158 | 00:37 | . 5 | 1.567412 | 1.667408 | 0.521651 | 0.903924 | 00:38 | . 6 | 1.409781 | 1.601621 | 0.548714 | 0.906631 | 00:37 | . 7 | 1.282712 | 1.580474 | 0.552774 | 0.907307 | 00:37 | . 8 | 1.171459 | 1.568391 | 0.563599 | 0.914750 | 00:36 | . 9 | 1.161886 | 1.562464 | 0.556834 | 0.910014 | 00:38 | . 1.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 4.278201 | 2.984197 | 0.233424 | 0.760487 | 00:37 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.285433 | 2.621417 | 0.301759 | 0.794317 | 00:37 | . 1 | 2.870027 | 2.286499 | 0.380244 | 0.823410 | 00:37 | . 2 | 2.451526 | 2.053459 | 0.443843 | 0.853857 | 00:37 | . 3 | 2.076892 | 1.880450 | 0.465494 | 0.883627 | 00:37 | . 4 | 1.826856 | 1.755741 | 0.495940 | 0.886333 | 00:37 | . 5 | 1.576267 | 1.623619 | 0.531800 | 0.899865 | 00:37 | . 6 | 1.401009 | 1.599875 | 0.538566 | 0.901894 | 00:37 | . 7 | 1.266603 | 1.576163 | 0.542625 | 0.897835 | 00:37 | . 8 | 1.182635 | 1.560193 | 0.550068 | 0.901218 | 00:37 | . 9 | 1.122207 | 1.551291 | 0.554804 | 0.903248 | 00:37 | . 1.0 . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 4.250400 | 2.962082 | 0.228011 | 0.746279 | 00:36 | . epoch train_loss valid_loss accuracy_breed accuracy_species time . 0 | 3.314677 | 2.615248 | 0.291610 | 0.783491 | 00:37 | . 1 | 2.876733 | 2.251967 | 0.354533 | 0.829499 | 00:37 | . 2 | 2.468880 | 1.980189 | 0.441137 | 0.859946 | 00:37 | . 3 | 2.095378 | 1.846294 | 0.479702 | 0.881597 | 00:37 | . 4 | 1.823901 | 1.737270 | 0.504060 | 0.899188 | 00:37 | . 5 | 1.593422 | 1.630034 | 0.531123 | 0.897158 | 00:37 | . 6 | 1.431555 | 1.601677 | 0.541272 | 0.907984 | 00:36 | . 7 | 1.291068 | 1.570975 | 0.540595 | 0.903924 | 00:38 | . 8 | 1.178105 | 1.547459 | 0.552774 | 0.903248 | 00:37 | . 9 | 1.153928 | 1.552425 | 0.544655 | 0.909337 | 00:37 | . # Save results for analysis later results.to_csv(&#39;results2.csv&#39;) .",
            "url": "https://isaac-flath.github.io/blog/neural%20networks/image%20classification/1900/01/01/Heirarchical-Loss.html",
            "relUrl": "/neural%20networks/image%20classification/1900/01/01/Heirarchical-Loss.html",
            "date": " • Jan 1, 1900"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a Data Scientist with a passion for teaching. . I really enjoy playing with machine learning algorithms so much of my content will probably relate to that in some way :grinning: . Basically anything I find interesting and/or helpful will be included. .",
          "url": "https://isaac-flath.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://isaac-flath.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}