{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Progress - NLP - Language Model Basics\n",
    "> Building a Language Model\n",
    "\n",
    "- toc: true \n",
    "- hide: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Isaac Flath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "In this post we are going to dive into NLP, specifically a Language Model.  Language models are the foundation of all NLP.  You will always want to start with a language model then use transfer learning to tune that model to your particular goal (ie Classification).  \n",
    "\n",
    "So what is a language model?  In short, it is a model that uses the preceding words to predict the next word.  We do not need seperate labels, because they are in the text.  This is training the model on the nuances of the language you will be working on.  If you want to know if a tweet is toxic or not, you will need to be able to read and understand the tweet in order to do that.  The language model helps with understanding the tweet - then you can use that model with those weights to tune it for the final task (determining whether the tweet is toxic or not).\n",
    "\n",
    "For this post, I will be using news articles to show how to create a language model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using the \"All-the-news\" dataset from this site.  https://components.one/datasets/all-the-news-2-news-articles-dataset/\n",
    "\n",
    "I downloaded then put the csv into a sqlite database for conveniece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication</th>\n",
       "      <th>min(date)</th>\n",
       "      <th>max(date)</th>\n",
       "      <th>count(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Buzzfeed News</td>\n",
       "      <td>2016-02-19 00:00:00</td>\n",
       "      <td>2020-04-02 00:00:00</td>\n",
       "      <td>32819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2020-04-01 13:42:08</td>\n",
       "      <td>252259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Insider</td>\n",
       "      <td>2016-01-01 03:08:00</td>\n",
       "      <td>2020-04-01 01:48:46</td>\n",
       "      <td>57953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Washington Post</td>\n",
       "      <td>2016-06-10 00:00:00</td>\n",
       "      <td>2020-04-01 00:00:00</td>\n",
       "      <td>40882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TMZ</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2020-04-01 00:00:00</td>\n",
       "      <td>49595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Refinery 29</td>\n",
       "      <td>2016-01-01 07:00:00</td>\n",
       "      <td>2020-04-01 00:00:00</td>\n",
       "      <td>111433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vox</td>\n",
       "      <td>2016-01-01 01:41:26</td>\n",
       "      <td>2020-03-31 23:50:00</td>\n",
       "      <td>47272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Verge</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2020-03-31 00:00:00</td>\n",
       "      <td>52424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hyperallergic</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2020-03-31 00:00:00</td>\n",
       "      <td>13551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CNN</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2020-03-31 00:00:00</td>\n",
       "      <td>127602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CNBC</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2020-03-31 00:00:00</td>\n",
       "      <td>238096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>2016-01-01 08:00:19</td>\n",
       "      <td>2020-03-30 10:57:55</td>\n",
       "      <td>52095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wired</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2020-03-30 00:00:00</td>\n",
       "      <td>20243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Reuters</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2020-03-30 00:00:00</td>\n",
       "      <td>840094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>New Republic</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2020-03-30 00:00:00</td>\n",
       "      <td>11809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Mashable</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2020-03-30 00:00:00</td>\n",
       "      <td>94107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Economist</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2020-03-30 00:00:00</td>\n",
       "      <td>26227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Vice</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2020-03-29 15:17:00</td>\n",
       "      <td>101137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Hill</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2020-03-26 00:00:00</td>\n",
       "      <td>208411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Politico</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2020-03-26 00:00:00</td>\n",
       "      <td>46377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>New Yorker</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>2020-02-17 00:00:00</td>\n",
       "      <td>4701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Axios</td>\n",
       "      <td>2016-12-23 00:00:00</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>47815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>People</td>\n",
       "      <td>2016-01-01 00:05:00</td>\n",
       "      <td>2019-12-15 22:40:00</td>\n",
       "      <td>136488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Gizmodo</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2019-10-15 00:00:00</td>\n",
       "      <td>27228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Vice News</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2019-08-07 00:00:00</td>\n",
       "      <td>15539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2019-03-08 00:00:00</td>\n",
       "      <td>12578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Fox News</td>\n",
       "      <td>2016-04-09</td>\n",
       "      <td>2018-08-14</td>\n",
       "      <td>20144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           publication            min(date)            max(date)  count(*)\n",
       "0        Buzzfeed News  2016-02-19 00:00:00  2020-04-02 00:00:00     32819\n",
       "1   The New York Times  2016-01-01 00:00:00  2020-04-01 13:42:08    252259\n",
       "2     Business Insider  2016-01-01 03:08:00  2020-04-01 01:48:46     57953\n",
       "3      Washington Post  2016-06-10 00:00:00  2020-04-01 00:00:00     40882\n",
       "4                  TMZ  2016-01-01 00:00:00  2020-04-01 00:00:00     49595\n",
       "5          Refinery 29  2016-01-01 07:00:00  2020-04-01 00:00:00    111433\n",
       "6                  Vox  2016-01-01 01:41:26  2020-03-31 23:50:00     47272\n",
       "7            The Verge  2016-01-01 00:00:00  2020-03-31 00:00:00     52424\n",
       "8        Hyperallergic  2016-01-01 00:00:00  2020-03-31 00:00:00     13551\n",
       "9                  CNN           2016-01-01  2020-03-31 00:00:00    127602\n",
       "10                CNBC           2016-01-01  2020-03-31 00:00:00    238096\n",
       "11          TechCrunch  2016-01-01 08:00:19  2020-03-30 10:57:55     52095\n",
       "12               Wired           2016-01-01  2020-03-30 00:00:00     20243\n",
       "13             Reuters           2016-01-01  2020-03-30 00:00:00    840094\n",
       "14        New Republic           2016-01-01  2020-03-30 00:00:00     11809\n",
       "15            Mashable           2016-01-01  2020-03-30 00:00:00     94107\n",
       "16           Economist  2016-01-01 00:00:00  2020-03-30 00:00:00     26227\n",
       "17                Vice  2016-01-01 00:00:00  2020-03-29 15:17:00    101137\n",
       "18            The Hill           2016-01-01  2020-03-26 00:00:00    208411\n",
       "19            Politico           2016-01-01  2020-03-26 00:00:00     46377\n",
       "20          New Yorker           2016-01-04  2020-02-17 00:00:00      4701\n",
       "21               Axios  2016-12-23 00:00:00  2020-01-01 00:00:00     47815\n",
       "22              People  2016-01-01 00:05:00  2019-12-15 22:40:00    136488\n",
       "23             Gizmodo           2016-01-01  2019-10-15 00:00:00     27228\n",
       "24           Vice News  2016-01-01 00:00:00  2019-08-07 00:00:00     15539\n",
       "25                                           2019-03-08 00:00:00     12578\n",
       "26            Fox News           2016-04-09           2018-08-14     20144"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from fastai.text.all import *\n",
    "\n",
    "path = Path('../../../data/all-the-news')\n",
    "con = sqlite3.connect(path/'all_the_news.db')\n",
    "\n",
    "pd.read_sql_query('SELECT publication, min(date),max(date), count(*) from news group by publication order by max(date) desc', con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to pick the 5 most recent New York times Articles.  For the final model I will use all of the data, but for simplicity of demonstrating tokenization we will use just 5 articles.  Here is an example of the start of one of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"SELECT article from news where publication = 'CNBC' and length(article) > 10 order by random() limit 500000\",con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastai Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>3.649046</td>\n",
       "      <td>0.342520</td>\n",
       "      <td>22:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>3.519593</td>\n",
       "      <td>0.358146</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>3.412565</td>\n",
       "      <td>0.369808</td>\n",
       "      <td>23:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>nan</td>\n",
       "      <td>3.356618</td>\n",
       "      <td>0.376045</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>nan</td>\n",
       "      <td>3.332409</td>\n",
       "      <td>0.378788</td>\n",
       "      <td>24:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>nan</td>\n",
       "      <td>3.327978</td>\n",
       "      <td>0.379292</td>\n",
       "      <td>24:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs=128\n",
    "dls = TextDataLoaders.from_df(df, text_col='article', is_lm=True,bs = bs)\n",
    "learn = language_model_learner(dls, AWD_LSTM, metrics=accuracy)\n",
    "learn.fine_tune(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/lmlearner_n50000128.pth')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('lmlearner_n50000'+str(bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Trump cares about immigration is Wednesday 's resignation of former National Security Advisor Paul Langer , who is led by the Whites National Turkish President Jack Dorsey , according to a report from the White House . The White\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Trump cares about immigration is his official slogan solanezumab undersea exchange that wonders the only things that happened in 1994 , as the Great Wall taner amazon underground and has a top 5 , 000 fallerfallerfaller ... He consider saying there are issues of values and threat the economy will\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Trump cares about immigration is why the economic growth has slowed . Taxes are potentially free from tax increases that could have profound consequences . Tax reform — which requires management — powers the tax code to inflate earnings ratios , estimates being a signiﬁcant scenario of 19.4 percent and 47 percent of\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Trump cares about immigration is because at least some claims that made direct trade deals optimistically are just necessary . For example , the firm plans to hire people so they can shift their spending , but that has not allowed individual companies to take benefits ; they are doing initiatives like business technology\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Trump cares about immigration is not the titanic market emergency , but since Britain paid for Canada the rights to funding certain undocumented rights , Trump said that there was how too many people could using immigration duty . Gspc has largely invested in the local investment machinery carrier Dunkin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Trump cares about immigration is that Trump has a much more aggressive sleepy lunch harder when he comes to the house for it . He 's become the top Republican in the presidency . He 's driving on from a growing base . He has already started such a drive\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Trump cares about immigration is his weekend photo assistant . He said \" Since December the Axios issue does n't send you in a living room . You 'll do something about yup . I would well see that what happens in next year after you die . \"\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Trump cares about immigration is the 50 percent tax policy on both sides of the aisle and tragic implications for his Steve Bannon campaign . Trump gave his name on Monday as his sniffer timer . The ties between Sweep and Goldman accounts can be evident in their\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Trump cares about immigration is as Federal Reserve officials and private equity executives , including Wilbur Ross , Transportation Secretary Wilbur Ross , and Southwest Airlines , have experienced less oversight and employee spending and screws said . \" these shorts were ineffective in recent\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Trump cares about immigration is , he says , that retiring , and working on a solid choice of white house with Republicans around China — in powerful U.S . view is the sort of way Trump obstructed a Democratic presidential nominee by attacking voters . When Trump\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,10):\n",
    "    print(learn.predict('The reason Trump cares about immigration is',n_words=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Biden cares about immigration is . He 's drove the airline away from a sweeping public boundaries car sale , which helps shareholders . Bombast , or policing , has caused parallels for Tesla 's history , immigration and our trade relationship . Denmark 's Bounty Force said it\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Biden cares about immigration is because he explains not to say why he should be doing everything that could be done and he contains even the most important things . At the beginning of August , the Federal Reserve quickly stopped speed of on ronna and trips to Chile .\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Biden cares about immigration is no one plan to completely support refugees . But President Barack Obama on Tuesday laid out Obama 's potential base for immigration , saying that \" who i am not going to make to wear with a lawyer like every other 34-year - old\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Biden cares about immigration is because there are closed corporate districts , and candidates paying attention to lost faith did n't run a good plan as what was to be done . And Democrats kept there for a third of the time he took office , parish council s - u - lago\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Biden cares about immigration is \" to book immigration . \" In the fourth quarter , however , Biden needed to take off a brief tie for the election . \" i know that , I 've got , i really ca n't even think esiner the special counsel nancy Sanders\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Biden cares about immigration is , he says , . The Trump administration sat down with the U.S . Defense Department not only in Washington , but also in Washington . Lawmakers also contended with his polling , election ambitions and the ongoing discussions by financial experts\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Biden cares about immigration is with regulations that ask for race and bowe — one of the biggest exceptions he has ever found — on his section of the vehicle to ladder across bridges and bridges in the country . But diversity among medals was not nominated by most American organizations , but\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Biden cares about immigration is a background band that includes rights experts including Fedexcup Michelle Trump and Jay Inslee . He said Tuesday that an illegal incursion into the Northeast could put Virginia in the middle in a turn to enter a slowdown . Trump\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Biden cares about immigration is about getting back to Europe at international trade . \" that 's because of our trade war with the European Union and so it 's a political measure , not a scenario that is eating in the U.S . \" Peter Navarro , director\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason Biden cares about immigration is because people could move forward , known as Advanced America Mrs . Trump , to address Qatar 's warming to history for thousands of decades . On travel from safe - haven China to France and the United States ,\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,10):\n",
    "    print(learn.predict('The reason Biden cares about immigration is',n_words=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I need to tokenize my data.  Let's do that first.  The fastai library adds some extra tokens.  Tokens such as xxbos which indicates that it's the beginning of a sentance, or xxup that indicates that the word is in capital letters.\n",
    "\n",
    ">Note:  In a previous post I showed how you can do a basic tokenization from scratch.  Please check out that post for a foundation on tokenization and numericalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "txts = L(o for o in df.article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy = WordTokenizer()\n",
    "tkn = Tokenizer(spacy)\n",
    "\n",
    "toks = txts.map(tkn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,len(toks)):\n",
    "#     toks[i] = L(filter(lambda a: a != 'xxmaj', toks[i]))\n",
    "# toks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to numericalize our data.  By that, I mean assign numbers to each unique token and replace the tokens with those numbers.  We can do that very easily using Numericalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks)\n",
    "coll_repr(num.vocab,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below that we can look at our numericalized tokens, and convert those back to tokens if we need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = toks.map(num); \n",
    "nums[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(toks[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(num.vocab[o] for o in nums[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model\n",
    "\n",
    "A Language model is a semi-surpervised learning.  It is different from classification or regression because the labels are not seperate from the training data.  We will use previous words (or tokens more specifically) to predict the next word.  For this post, I will be creating this from scratch to demonstrate exactly how it works.\n",
    "\n",
    "Let's start by creating our training set.  We will create tuples where the first element is a series of tokens, and the second element is the following word.  Let's see what that looks like for 1 article in both tokens and numbers.  We will start with using the 3 tokens to predict the next token in 1 article.  We will almost certainly need to use more articles as well as more tokens for the prediction, but we can increase those numbers later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packaging the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L((toks[0][i:i+n_words], toks[0][i+n_words]) for i in range(0,len(toks[0])-(n_words+1),n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = L((nums[0][i:i+n_words], nums[0][i+n_words]) for i in range(0,len(nums[0])-(n_words+1),n_words))\n",
    "\n",
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = L()\n",
    "for article_num in range(0,len(nums)):\n",
    "    seq = L((nums[article_num][i:i+n_words], nums[article_num][i+n_words]) for i in range(0,len(nums[article_num])-(n_words+1),n_words))\n",
    "    seqs.append(seq)\n",
    "    \n",
    "seqs = L(item for sublist in seqs for item in sublist)\n",
    "\n",
    "seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily package this into a dataloader so that we can feed this into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "cut = int(len(seqs) * 0.9)\n",
    "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "So now we need to create a RNN.  Let's start with a fastai model, then go back and create some basic models to illustrate how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n,counts = 0,torch.zeros(len(num.vocab))\n",
    "# for x,y in dls.valid:\n",
    "#     n += y.shape[0]\n",
    "#     for i in range_of(num.vocab): counts[i] += (y==i).long().sum()\n",
    "# idx = torch.argmax(counts)\n",
    "\n",
    "# top10 = torch.topk(counts,15)\n",
    "# for idx in top10[1]:\n",
    "#     print(idx, num.vocab[idx.item()], round(counts[idx].item()/n*100,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready for an RNN.  WE will start with an RNN that is as simple as it gets.\n",
    "\n",
    "```for i in range(3):```\n",
    "Because we are feeding in 3 tokens to predict the fourth, we will have 3 hidden layers, 1 per token.\n",
    "\n",
    "```h = h + self.i_h(x[:,i])```\n",
    "For each input token we will run our input to hidden function.  We are indexing to grab the column in our embedding matrix that corresponds with the token, and adding that. All this is doing is adding the embedding for the particular token. \n",
    "    \n",
    "```h = F.relu(self.h_h(h))```\n",
    "We then run our hidden to hidden function (h_h), which is a linear layer (y = wx + b).  We do a ReLu of that, which is just replacing any negative values with 0.\n",
    "    \n",
    "```return self.h_o(h)```\n",
    "We then run our hidden to output function (h_o), which is another linear layer, but it is outputing the prediction of which word is next.  Naturally, this is the size of our vocabulary.\n",
    "\n",
    "Wrap all that in a class and it looks like the below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel1(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = 0\n",
    "        for i in range(3):\n",
    "            h = h + self.i_h(x[:,i])\n",
    "            h = F.relu(self.h_h(h))\n",
    "        return self.h_o(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then threw it in a learner for 3 epochs and we see about an 16% accuracy.  Much better than just predicting the most common words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, LanguageModel1(len(num.vocab), 64), loss_func=F.cross_entropy, \n",
    "                metrics=accuracy)\n",
    "learn.fit_one_cycle(5, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with the previous model is it is only using the previous 3 words to predict the next one.  In reality, words are in a logical order that is longer than 3 words - so we really don't want to just reset it every time by setting h to 0.  So instead we set it to 0 when we first initialize it, but not later.\n",
    "\n",
    "Unfortunately what this means is we end up with more and more weights as we train, which means more and more gradients to calculate.  The model would explode, so instead we just deal with the recent gradients by using \"detach\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LanguageModel2(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
    "        self.h = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(3):\n",
    "            self.h = self.h + self.i_h(x[:,i])\n",
    "            self.h = F.relu(self.h_h(self.h))\n",
    "        out = self.h_o(self.h)\n",
    "        self.h = self.h.detach()\n",
    "        return out\n",
    "    \n",
    "    def reset(self): self.h = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to work, our data needs to be in a logical order.  So let's put our data in our dataloader in the order it was in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_chunks(ds, bs):\n",
    "    m = len(ds) // bs\n",
    "    new_ds = L()\n",
    "    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n",
    "    return new_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = int(len(seqs) * 0.9)\n",
    "dls = DataLoaders.from_dsets(\n",
    "    group_chunks(seqs[:cut], bs), \n",
    "    group_chunks(seqs[cut:], bs), \n",
    "    bs=bs, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And throw it in a learner for 3 epochs and we see our accuracy is much better.  It can predict the next word correctly almost 1 out of every 5 times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, LanguageModel2(len(num.vocab), 64), loss_func=F.cross_entropy,\n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(5, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more steps to this iterative process to get to a really cutting edge model, and future posts will cover those steps.  But for now, we have a great start and a good foundation in what an RNN is in it's simplest form.  Future blog posts that continue to expand and pick up where this one left off.\n",
    "\n",
    "Other areas that more cutting edge architectures improve upon:\n",
    "+ Rather than predicting 1 token for each group of 4 tokens (3 inputs -> 1 output), predict every word.\n",
    "+ Stack the RNNs together for more layers\n",
    "+ Use LSTMs\n",
    "+ Regularization (ie dropout, AR, TAR) \n",
    "\n",
    "We will continue to build on this language model until we reach close to the performance we would get using the fastai library.  See below for the out of the box language model using fastai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastai Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dls = TextDataLoaders.from_df(df, text_col='article', is_lm=True,bs = 256)\n",
    "learn = language_model_learner(dls, AWD_LSTM, metrics=accuracy)\n",
    "learn.fit_one_cycle(5, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
