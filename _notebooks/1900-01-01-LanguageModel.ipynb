{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Progress - NLP - Language Model Basics\n",
    "> Building a Language Model\n",
    "\n",
    "- toc: true \n",
    "- hide: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Isaac Flath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "In this post we are going to dive into NLP, specifically a Language Model.  Language models are the foundation of all NLP.  You will always want to start with a language model then use transfer learning to tune that model to your particular goal (ie Classification).  \n",
    "\n",
    "So what is a language model?  In short, it is a model that uses the preceding words to predict the next word.  We do not need seperate labels, because they are in the text.  This is training the model on the nuances of the language you will be working on.  If you want to know if a tweet is toxic or not, you will need to be able to read and understand the tweet in order to do that.  The language model helps with understanding the tweet - then you can use that model with those weights to tune it for the final task (determining whether the tweet is toxic or not).\n",
    "\n",
    "For this post, I will be using news articles to show how to create a language model using fastai's high level interface.  In other posts, I am diving into the details of how NLP models work.  This is just focused on the high level API fastai offers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using the \"All-the-news\" dataset from this site.  https://components.one/datasets/all-the-news-2-news-articles-dataset/\n",
    "\n",
    "I downloaded then put the csv into a sqlite database for conveniece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication</th>\n",
       "      <th>min(date)</th>\n",
       "      <th>max(date)</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2020-04-01 13:42:08</td>\n",
       "      <td>252259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2020-03-31 00:00:00</td>\n",
       "      <td>127602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNBC</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2020-03-31 00:00:00</td>\n",
       "      <td>238096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reuters</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2020-03-30 00:00:00</td>\n",
       "      <td>840094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Hill</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2020-03-26 00:00:00</td>\n",
       "      <td>208411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>People</td>\n",
       "      <td>2016-01-01 00:05:00</td>\n",
       "      <td>2019-12-15 22:40:00</td>\n",
       "      <td>136488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          publication            min(date)            max(date)     cnt\n",
       "0  The New York Times  2016-01-01 00:00:00  2020-04-01 13:42:08  252259\n",
       "1                 CNN           2016-01-01  2020-03-31 00:00:00  127602\n",
       "2                CNBC           2016-01-01  2020-03-31 00:00:00  238096\n",
       "3             Reuters           2016-01-01  2020-03-30 00:00:00  840094\n",
       "4            The Hill           2016-01-01  2020-03-26 00:00:00  208411\n",
       "5              People  2016-01-01 00:05:00  2019-12-15 22:40:00  136488"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from fastai.text.all import *\n",
    "\n",
    "path = Path('../../../data/all-the-news')\n",
    "con = sqlite3.connect(path/'all_the_news.db')\n",
    "\n",
    "pub_stats = pd.read_sql_query('SELECT publication, min(date),max(date), count(*) as cnt from news group by publication having count(*) > 125000 order by max(date) desc', con)\n",
    "pub_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I need to tokenize my data.  Let's do that first.  The fastai library adds some extra tokens.  Tokens such as xxbos which indicates that it's the beginning of a sentance, or xxup that indicates that the word is in capital letters.\n",
    "\n",
    ">Note:  In a previous post I showed how you can do basic tokenization from scratch.  Please check out that post for a foundation on tokenization and numericalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will get a bunch of articles to tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher = 'The New York Times'\n",
    "records = '1000'\n",
    "query = f\"SELECT article FROM news WHERE publication = '%s' and length(article) > 500 order by date desc limit %s\"%(publisher,records)\n",
    "df = pd.read_sql_query(query,con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see what tokenization looks like.  Fastai will do this for us, but it's good to know what's going into our model.  Most data scientists make models without really understanding them well, which leads to lots of problems when trying to get actual value out of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = L(o for o in df.article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy = WordTokenizer()\n",
    "tkn = Tokenizer(spacy)\n",
    "\n",
    "toks = txts.map(tkn);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to numericalize our data.  By that, I mean assign numbers to each unique token and replace the tokens with those numbers.  We can do that very easily using Numericalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#20568) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj',',','.','the','to','and','of','a','in','that','“','”'...]\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks)\n",
    "coll_repr(num.vocab,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below that we can look at our numericalized tokens, and convert those back to tokens if we need to.  We see tokens such as xxmaj, which indicates the next word starts with a capital letter.  Or xxup, wchich indicates the next work is in all caps.  These are not words, but they hold meaning in the english value and so we want a way to feed it into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,     8,   140,     8,   110,   225,    14,    18,   360,   162,\n",
       "           17,  4237,   890,    19,    32,    46,   391,  1966,  2661,   705,\n",
       "         2694,  6129,    17,    11,     7,   302,    10,   327,   828,    66,\n",
       "           39,   650,     9,     5,   109,   129,    12,  5068,     9,     5,\n",
       "          109,   129,    10,     8,   963,  4354,    55,  2796,    16, 10492,\n",
       "          709,   465,    10,     8,   169,     8,    87,     8,    11,   242,\n",
       "           14,   456,   234,     8,    50,     8,    95,     9,     8,    50,\n",
       "            8,   810,    13,     8,  1712,    74,  6425,   342,     9,     5,\n",
       "          109,   129,   382,     9,    26,    57,    81,   650,     9,     5,\n",
       "          109,   129,  6715,   695,    10,     0,     8,    11,     8,   149,\n",
       "            8,  1455,  1313,    23,     8,   567,    17,    11,  5655,  1847,\n",
       "          260,    11,    59,    66,   977,    12,    18,  6710, 14416,     9,\n",
       "         6710,  8960,     9,    13,  6710,  3518,    10,    19,     8,    32,\n",
       "            8,   278,     0,   483,    20,    69,     8,   140,     8,   110,\n",
       "           27,    66,    39,    15,    18,   193,     9,   193,  2657,   104,\n",
       "          179,     9,    19,    11,  1120,    14,    11,   253,     9,   455,\n",
       "           13,  9659,  3614,   180,    11,   151,   330,   108,   534,  1557,\n",
       "         1512,    10,    18,    38,    31,  1007,    15,   429,    97,   173,\n",
       "         2083,   157,    16,    11,     0,   501,    14,    11,     8,   149,\n",
       "            8,  1455,    40,    62,    17,    22,  2218,    53,     9,  1030])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = toks.map(num); \n",
    "nums[1][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['xxbos', 'xxmaj', 'president', 'xxmaj', 'trump', 'told', 'of', '“',\n",
       "       'hard', 'days', 'that', 'lie', 'ahead', '”', 'as', 'his', 'top',\n",
       "       'scientific', 'advisers', 'released', 'models', 'predicting',\n",
       "       'that', 'the', 'xxup', 'u.s', '.', 'death', 'toll', 'would', 'be',\n",
       "       '100', ',', 'xxrep', '3', '0', 'to', '240', ',', 'xxrep', '3', '0',\n",
       "       '.', 'xxmaj', 'governors', 'complained', 'about', 'chaos', 'in',\n",
       "       'obtaining', 'critical', 'supplies', '.', 'xxmaj', 'right',\n",
       "       'xxmaj', 'now', 'xxmaj', 'the', 'number', 'of', 'deaths', 'across',\n",
       "       'xxmaj', 'new', 'xxmaj', 'york', ',', 'xxmaj', 'new', 'xxmaj',\n",
       "       'jersey', 'and', 'xxmaj', 'connecticut', 'will', 'exceed', '2',\n",
       "       ',', 'xxrep', '3', '0', 'today', ',', 'with', 'more', 'than',\n",
       "       '100', ',', 'xxrep', '3', '0', 'detected', 'infections', '.',\n",
       "       '新冠病毒疫情最新消息', 'xxmaj', 'the', 'xxmaj', 'united', 'xxmaj',\n",
       "       'nations', 'warned', 'on', 'xxmaj', 'wednesday', 'that', 'the',\n",
       "       'unfolding', 'battle', 'against', 'the', 'coronavirus', 'would',\n",
       "       'lead', 'to', '“', 'enhanced', 'instability', ',', 'enhanced',\n",
       "       'unrest', ',', 'and', 'enhanced', 'conflict', '.', '”', 'xxmaj',\n",
       "       'as', 'xxmaj', 'americans', 'steeled', 'themselves', 'for', 'what',\n",
       "       'xxmaj', 'president', 'xxmaj', 'trump', 'said', 'would', 'be', 'a',\n",
       "       '“', 'very', ',', 'very', 'painful', 'two', 'weeks', ',', '”',\n",
       "       'the', 'scale', 'of', 'the', 'economic', ',', 'political', 'and',\n",
       "       'societal', 'fallout', 'around', 'the', 'world', 'came', 'into',\n",
       "       'ever', 'greater', 'focus', '.', '“', 'we', 'are', 'facing', 'a',\n",
       "       'global', 'health', 'crisis', 'unlike', 'any', 'in', 'the',\n",
       "       '75-year', 'history', 'of', 'the', 'xxmaj', 'united', 'xxmaj',\n",
       "       'nations', '—', 'one', 'that', 'is', 'killing', 'people', ',',\n",
       "       'spreading'], dtype='<U11')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(toks[1][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj president xxmaj trump told of “ hard days that lie ahead ” as his top scientific advisers released models predicting that the xxup u.s . death toll would be 100 , xxrep 3 0 to 240 , xxrep 3 0 . xxmaj governors complained about chaos in obtaining critical supplies . xxmaj right xxmaj now xxmaj the number of deaths across xxmaj new xxmaj york , xxmaj new xxmaj jersey and xxmaj connecticut will exceed 2 , xxrep 3 0 today , with more than 100 , xxrep 3 0 detected infections . xxunk xxmaj the xxmaj united xxmaj nations warned on xxmaj wednesday that the unfolding battle against the coronavirus would lead to “ enhanced instability , enhanced unrest , and enhanced conflict . ” xxmaj as xxmaj americans xxunk themselves for what xxmaj president xxmaj trump said would be a “ very , very painful two weeks , ” the scale of the economic , political and societal fallout around the world came into ever greater focus . “ we are facing a global health crisis unlike any in the xxunk history of the xxmaj united xxmaj nations — one that is killing people , spreading'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in nums[1][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Fastai Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "publisher = 'The New York Times'\n",
    "records = '10000'\n",
    "query = f\"SELECT article from news where publication = '%s' and length(article) > 500 order by date desc limit %s\"%(publisher,records)\n",
    "df = pd.read_sql_query(query,con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/fastai/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "bs=128\n",
    "dls = TextDataLoaders.from_df(df, text_col='article', is_lm=True,bs = bs)\n",
    "learn = language_model_learner(dls, AWD_LSTM, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.570260</td>\n",
       "      <td>4.004526</td>\n",
       "      <td>0.312939</td>\n",
       "      <td>5:06:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.266904</td>\n",
       "      <td>3.756997</td>\n",
       "      <td>0.334835</td>\n",
       "      <td>5:07:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.076308</td>\n",
       "      <td>3.660905</td>\n",
       "      <td>0.343656</td>\n",
       "      <td>5:07:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='8' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      80.00% [8/10 46:20:41<11:35:10]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.976979</td>\n",
       "      <td>3.619511</td>\n",
       "      <td>0.349825</td>\n",
       "      <td>5:50:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.910682</td>\n",
       "      <td>3.576260</td>\n",
       "      <td>0.355482</td>\n",
       "      <td>5:49:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.870065</td>\n",
       "      <td>3.532961</td>\n",
       "      <td>0.360485</td>\n",
       "      <td>5:48:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.801512</td>\n",
       "      <td>3.495675</td>\n",
       "      <td>0.365133</td>\n",
       "      <td>5:45:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.783073</td>\n",
       "      <td>3.466334</td>\n",
       "      <td>0.368626</td>\n",
       "      <td>5:46:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.721595</td>\n",
       "      <td>3.445397</td>\n",
       "      <td>0.371505</td>\n",
       "      <td>5:46:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.698827</td>\n",
       "      <td>3.431377</td>\n",
       "      <td>0.373355</td>\n",
       "      <td>5:47:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.657815</td>\n",
       "      <td>3.422835</td>\n",
       "      <td>0.374445</td>\n",
       "      <td>5:46:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='652' class='' max='1327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      49.13% [652/1327 2:36:20<2:41:51 3.6772]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fine_tune(10,freeze_epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Let's take a look at a few prompts.  Let's see what it spits out about a few controversial topics.  What did it learn from reading 125000 news articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'lm_all_%s_%sArticles_%s'%(publisher,records,str(iteration))\n",
    "learn.save(fname)\n",
    "iteration = iteration+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,2):\n",
    "    print(learn.predict('Immigration is',n_words = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,2):\n",
    "    print(learn.predict('Immigration is',n_words = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,2):\n",
    "    print(learn.predict('Minorities are',n_words = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,2):\n",
    "    print(learn.predict('Minorities are',n_words = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,2):\n",
    "    print(learn.predict('Minorities are',n_words = 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
